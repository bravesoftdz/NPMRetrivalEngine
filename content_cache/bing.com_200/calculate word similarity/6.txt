<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head> 
  <title>Similarity Metrics: Cosine Similarity</title> 
  <meta name="keywords" content=""> 
  <meta name="description" content=""> 
  <meta name="author" content=""> 
  <meta http-equiv="content-type" content="text/html;charset=utf-8"> 
  <meta http-equiv="Content-Style-Type" content="text/css"> 
  <link rel="stylesheet" href="css/blueprint/screen.css" type="text/css" media="screen, projection"> 
  <link rel="stylesheet" href="css/blueprint/print.css" type="text/css" media="print"> 
  <link rel="stylesheet" href="css/main.css" type="text/css" media="screen"> 
  <!--[if IE]>
  <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
<![endif]--> 
 </head> 
 <body> 
  <div class="container"> 
   <h1>Discussion of Similarity Metrics</h1> 
   <h2>Cosine Similarity</h2> 
   <p class="introduction"> </p>
   <h3><u>Analysis</u></h3> This metric is frequently used when trying to determine similarity between two documents. Since there are more words that are incommon between two documents, it is useless to use the other methods of calculating similarities (namely the Euclidean Distance and the Pearson Correlation Coefficient discussed earlier). As a result, the likelihood that two documents do not share the majority is very high (as with the Tanimoto Coefficient) and does not create a satisfactory metric for determining similarities. 
   <br>
   <br> In this similarity metric, the attributes (or words, in the case of the documents) is used as a vector to find the normalized dot product of the two documents. By determining the cosine similarity, the user is effectively trying to find cosine of the angle between the two objects. For cosine similarities resulting in a value of 0, the documents do not share any attributes (or words) because the angle between the objects is 90 degrees. 
   <br>
   <br> Expressed as a mathematical equation: 
   <br> 
   <img src="images/cos.gif">
   <br>
   <br> 
   <h3><u>Python Implementation</u></h3> 
   <p></p> 
   <pre class="code">
# Input: 2 vectors
# Output: the cosine similarity
# !!! Untested !!!
def cosine_similarity(vector1,vector2):
  # Calculate numerator of cosine similarity
  dot = [vector1[i] * vector2[i] for i in range(vector1)]
  
  # Normalize the first vector
  sum_vector1 = 0.0
  sum_vector1 += sum_vector1 + (vector1[i]*vector1[i] for i in range(vector1))
  norm_vector1 = sqrt(sum_vector1)
  
  # Normalize the second vector
  sum_vector2 = 0.0
  sum_vector2 += sum_vector2 + (vector2[i]*vector2[i] for i in range(vector2))
  norm_vector2 = sqrt(sum_vector2)
  
  return (dot/(norm_vector1*norm_vector2))
  </pre> 
   <h2>References</h2> The previous content is based on Chapter 2 of the following book:
   <br> Tan, Pang-Ning, Michael Steinbach, and Vipin Kumar. Introduction to Data Mining. Boston: Pearson Addison Wesley, 2006. 
   <br>
   <br> 
   <p align="left"> <a href="similarity.html"> Next: Similarity Index</a><br> <a href="tani.html"> Back to: Tanimoto Coefficient</a> </p>
  </div>   
 </body>
</html>