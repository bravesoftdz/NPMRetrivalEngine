<!DOCTYPE html PUBLIC "-//w3c//dtd html 4.0 transitional//en">
<html>
 <head> 
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
  <meta name="Author" content="Zdravko Markov"> 
  <meta name="GENERATOR" content="Mozilla/4.76 [en] (WinNT; U) [Netscape]"> 
  <title>Data preprocessing</title> 
 </head> 
 <body> 
  <h1> Data preprocessing</h1> 
  <h2> Why preprocessing ?</h2> 
  <ol> 
   <li> Real world data are generally</li> 
   <ul> 
    <li> Incomplete: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data</li> 
    <li> Noisy: containing errors or outliers</li> 
    <li> Inconsistent: containing discrepancies in codes or names</li> 
   </ul> 
   <li> Tasks in data preprocessing</li> 
   <ul> 
    <li> Data cleaning: fill in missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies.</li> 
    <li> Data integration: using multiple databases, data cubes, or files.</li> 
    <li> Data transformation: normalization and aggregation.</li> 
    <li> Data reduction: reducing the volume but producing the same or similar analytical results.</li> 
    <li> Data discretization: part of data reduction, replacing numerical attributes with nominal ones.</li> 
   </ul> 
  </ol> 
  <h2> Data cleaning</h2> 
  <ol> 
   <li> Fill in missing values (attribute or class value):</li> 
   <ul> 
    <li> Ignore the tuple: usually done when class label is missing.</li> 
    <li> Use the attribute mean (or majority nominal value) to fill in the missing value.</li> 
    <li> Use the attribute mean (or majority nominal value) for all samples belonging to the same class.</li> 
    <li> Predict the missing value by using a learning algorithm: consider the attribute with the missing value as a dependent (class) variable and run a learning algorithm (usually Bayes or decision tree) to predict the missing value.</li> 
   </ul> 
   <li> Identify outliers and smooth out noisy data:</li> 
   <ul> 
    <li> Binning</li> 
    <ul> 
     <li> Sort the attribute values and partition them into bins (see "Unsupervised discretization" below);</li> 
     <li> Then smooth by bin means,&nbsp; bin median, or&nbsp; bin boundaries.</li> 
    </ul> 
    <li> Clustering: group values in clusters and then detect and remove outliers (automatic or manual)</li> 
    <li> Regression: smooth by fitting the data into regression functions.</li> 
   </ul> 
   <li> Correct inconsistent data: use domain knowledge or expert decision.</li> 
  </ol> 
  <h2> Data transformation</h2> 
  <ol> 
   <li> Normalization:</li> 
   <ul> 
    <li> Scaling attribute values to fall within a specified range.</li> 
    <ul> 
     <li> Example: to transform <tt>V in [min, max]</tt> to <tt>V' in [0,1]</tt>, apply <tt>V'=(V-Min)/(Max-Min)</tt></li> 
    </ul> 
    <li> Scaling by using mean and standard deviation (useful when min and max are unknown or when there are outliers): <tt>V'=(V-Mean)/StDev</tt></li> 
   </ul> 
   <li> Aggregation: moving up in the concept hierarchy on numeric attributes.</li> 
   <li> Generalization: moving up in the concept hierarchy on nominal attributes.</li> 
   <li> Attribute construction: replacing or adding new attributes inferred by existing attributes.</li> 
  </ol> 
  <h2> Data reduction</h2> 
  <ol> 
   <li> Reducing the number of attributes</li> 
   <ul> 
    <li> Data cube aggregation: applying roll-up, slice or dice operations.</li> 
    <li> Removing irrelevant attributes: attribute selection (filtering and wrapper methods), searching the attribute space (see Lecture 5: Attribute-oriented analysis).</li> 
    <li> Principle component analysis (numeric attributes only): searching for a lower dimensional space that can best represent the data..</li> 
   </ul> 
   <li> Reducing the number of attribute values</li> 
   <ul> 
    <li> Binning (histograms): reducing the number of attributes by grouping them into intervals (bins).</li> 
    <li> Clustering: grouping values in clusters.</li> 
    <li> Aggregation or generalization</li> 
   </ul> 
   <li> Reducing the number of tuples</li> 
   <ul> 
    <li> Sampling</li> 
   </ul> 
  </ol> 
  <h2> Discretization and generating concept hierarchies</h2> 
  <ol> 
   <li> Unsupervised discretization -&nbsp; class variable is not used.</li> 
   <ul> 
    <li> Equal-interval (equiwidth) binning: split the whole range of numbers in intervals with equal size.</li> 
    <li> Equal-frequency (equidepth) binning: use intervals containing equal number of values.</li> 
   </ul> 
   <li> Supervised discretization - uses the values of the class variable.</li> 
   <ul> 
    <li> Using class boundaries. Three steps:</li> 
    <ul> 
     <li> Sort values.</li> 
     <li> Place breakpoints between values belonging to different classes.</li> 
     <li> If too many intervals, merge intervals with equal or similar class distributions.</li> 
    </ul> 
    <li> Entropy (information)-based discretization. Example:</li> 
    <ul> 
     <li> Information in a class distribution:</li> 
     <ul> 
      <li> Denote a set of five values occurring in tuples belonging to two classes (+ and -) as <tt>[+,+,+,-,-]</tt></li> 
      <li> That is, the first 3 belong to "+" tuples and the last 2 - to "-" tuples</li> 
      <li> Then, <tt>Info([+,+,+,-,-]) = -(3/5)*log(3/5)-(2/5)*log(2/5)</tt> (logs are base 2)</li> 
      <li> 3/5 and 2/5 are relative frequencies (probabilities)</li> 
      <li> Ignoring the order of the values, we can use the following notation:<tt> [3,2] </tt>meaning 3 values from one class and 2 - from the other.</li> 
      <li> <tt>Then, Info([3,2]) = -(3/5)*log(3/5)-(2/5)*log(2/5)</tt></li> 
     </ul> 
     <li> Information in a split <tt>(2/5 and 3/5 are weight coefficients)</tt>:</li> 
     <ul> 
      <li> <tt>Info([+,+],[+,-,-]) = (2/5)*Info([+,+]) + (3/5)*Info([+,-,-])</tt></li> 
      <li> Or<tt>, Info([2,0],[1,2]) = (2/5)*Info([2,0]) + (3/5)*Info([1,2])</tt></li> 
     </ul> 
     <li> Method:</li> 
     <ul> 
      <li> Sort the values;</li> 
      <li> Calculate information in all possible splits;</li> 
      <li> Choose the split that minimizes information;</li> 
      <li> Do not include breakpoints between values belonging to the same class (this will increase information);</li> 
      <li> Apply the same to the resulting intervals until some stopping criterion is satisfied.</li> 
     </ul> 
    </ul> 
   </ul> 
   <li> Generating concept hierarchies: recursively applying partitioning or discretization methods.</li> 
  </ol>   
 </body>
</html>