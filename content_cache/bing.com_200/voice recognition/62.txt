<!doctype html>
<html xmlns="http://www.w3.org/1999/xhtml">
 <head> 
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"> 
  <title>Building Speech-Enabled Applications with ASP.NET</title> 
  <link rel="shortcut icon" type="image/x-icon" href="/Images/CodeIcon.png"> 
  <meta property="fb:page_id" content="218352335971"> 
  <meta property="fb:app_id" content="225285904297383"> 
  <meta name="Description" content="While sleepless the other night, I was channel surfing and ran across a rerun of the 1968 science fiction classic “2001: A Space Odyssey.”If you haven’t..."> 
  <meta name="Keywords" content="ASP.NET WebForms, Speech-Enabled Applications, Techniques"> 
  <meta name="Author" content="CODE Magazine, EPS Software Corp., Robbins, Thom"> 
  <meta name="Robots" content="all"> 
  <meta property="og:title" content="Building Speech-Enabled Applications with ASP.NET"> 
  <meta property="og:type" content="article"> 
  <meta property="og:description" content="While sleepless the other night, I was channel surfing and ran across a rerun of the 1968 science fiction classic “2001: A Space Odyssey.”If you haven’t..."> 
  <meta property="og:url" content="http://www.codemag.com/article/0511041"> 
  <meta property="og:image" content="http://www.codemag.com/Magazine/CoverLarge/f7ddaa17-649a-46d9-bb76-258d7f5e9a4e"> 
  <meta name="twitter:title" content="Building Speech-Enabled Applications with ASP.NET"> 
  <meta name="twitter:description" content="While sleepless the other night, I was channel surfing and ran across a rerun of the 1968 science fiction classic “2001: A Space Odyssey.”If you haven’t..."> 
  <meta name="twitter:image" value="http://www.codemag.com/Magazine/CoverLarge/f7ddaa17-649a-46d9-bb76-258d7f5e9a4e"> 
  <link rel="publisher" href="https://plus.google.com/103030867107221566748/"> 
  <link href="/Content/Reset.css" type="text/css" rel="stylesheet"> 
  <link href="/Content/2015/MicroFormats.css" type="text/css" rel="stylesheet"> 
  <link href="/Content/2015/Default.css" type="text/css" rel="stylesheet"> 
  <link href="/Content/2015/Desktop.css" type="text/css" rel="stylesheet"> 
  <style>
        .areaForeground { color: rgb(66, 182, 74); } .areaBackground { background: rgb(66, 182, 74); } a.areaLink { color: rgb(66, 182, 74); } a.areaLink:hover { color: rgb(66, 182, 74); } a.areaLink:active { color: rgb(66, 182, 74); } a.areaLink:visited { color: rgb(66, 182, 74); }
    </style> 
  <link href="/Content/ViewStyles/Article/ShowArticle.css" type="text/css" rel="stylesheet"> 
  <style>
        h1, h2, h3, h4, h5 {
            color: rgb(8, 57, 129);
        }
        a.areaLink {
            color: rgb(8, 57, 129);
        }
        a.areaLink:visited {
            color: rgb(8, 57, 129);
        }
        figure {
            color: rgb(8, 57, 129);
        }
        #contentColumnSingle .article-tables table th {
            background-color: rgb(8, 57, 129);
        }
        #contentColumnSingle .article-listings h1 {
            background-color: rgb(8, 57, 129);
        }
        #contentColumnSingle .article-tables h1 {
            color: rgb(8, 57, 129);
        }
        .article-pullquote {
            background: rgb(8, 57, 129);
        }
        .article-pullquote-arrow {
            background-image: url("/Images/Styles/SpeechBubbleArrow_8_57_129.png");
        }
        .side-info h1 {
            color: rgb(8, 57, 129);
        }
        .side-info h2 {
            color: rgb(8, 57, 129);
        }
        .tech-list li {
            background-color: rgb(8, 57, 129);
        }
        #menu ul li.magazineMenuSelected a {
            border-bottom: 3px solid rgb(8, 57, 129);
        }
        .download1ColumnButton {
            display: none;
        }                                                                          
        .codeDownloadButton {
            color: white;
            background: rgb(8, 57, 129);
            padding: 10px;
            margin-bottom: 50px;
            font-size: 1.5em;
        }
        .codeDownloadButton a {
            color: white;
            text-decoration: none;
        }
        .codeDownloadButton a:hover {
            color: white;
            text-decoration: none;
        }
        .codeDownloadButton a:visited {
            color: white;
            text-decoration: none;
        }
                
                h1.title {
                    font-family: "Segoe UI", "Helvetica Neue", Helvetica, Arial, Verdana, sans-serif;
                    font-weight: 300;
                }
                .article-body h1 {
                    font-family: "Segoe UI", "Helvetica Neue", Helvetica, Arial, Verdana, sans-serif;
                    font-weight: 300;
                }
                .article-body h2 {
                    font-family: "Segoe UI", "Helvetica Neue", Helvetica, Arial, Verdana, sans-serif;
                    font-weight: 300;
                }
                .article-body h3 {
                    font-family: "Segoe UI", "Helvetica Neue", Helvetica, Arial, Verdana, sans-serif;
                    font-weight: 300;
                }
            

        @media only screen and ( min-device-width : 320px ) and ( max-device-width : 480px ) {
            .download1ColumnButton {
                display: inherit;
            }                                                                          
        }
    </style> 
  <script src="/Scripts/jquery-1.8.2.min.js" type="text/javascript"></script> 
  <script src="/Scripts/jquery.unobtrusive-ajax.min.js" type="text/javascript"></script> 
  <script src="/Scripts/jquery.validate.min.js" type="text/javascript"></script> 
  <script src="/Scripts/jquery.validate.unobtrusive.min.js" type="text/javascript"></script> 
  <script src="/Scripts/jquery.popupWindow.js" type="text/javascript"></script> 
  <script src="/Scripts/Ads/Ads.js" type="text/javascript"></script> 
  <script language="javascript" type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-44188652-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script> 
  <script type="text/javascript">
        function setPageHeight() {
            var windowHeight = $(window).height();
            var pageHeight = $('#page').height();
            var headerHeight = $('#header').height();
            if (windowHeight > pageHeight + headerHeight + 1)
                $('#page').height(windowHeight - headerHeight - 1);
        }

        $(function () {
            setPageHeight();
            $(window).on('resize', setPageHeight);
            $(document).on('scroll', function () {
                $('.subMenu').removeClass('visibleSubMenu');
                if ($(document).scrollTop() > 60) {
                    $('#header').removeClass('headerLarge').addClass('headerSmall');
                } else {
                    $('#header').removeClass('headerSmall').addClass('headerLarge');
                }
                setPageHeight();
            });
            $('#header').on('click', function() {
                $('.subMenu').removeClass('visibleSubMenu');
            });
            $('#menu>ul>li').on('mouseenter', function () {
                $('.subMenu').removeClass('visibleSubMenu');
                $(this).find('.subMenu').addClass('visibleSubMenu');
            });
            $('#main').on('mouseenter', function () {
                $('.subMenu').removeClass('visibleSubMenu');
            });
            $('#mobileMenuIcon').on('click', function () {
                if (!$('#page').hasClass('showMobileMenu')) {
                    $('#page').addClass('showMobileMenu');
                    $('#header').addClass('showMobileMenu');
                    $('#mobileMenu').addClass('showMobileMenu');
                    $('#mobileMenu').css('display', 'block');
                    setTimeout(function () {
                        $('#mobileMenu').css('z-index', 100);
                    }, 300);
                } else {
                    $('#mobileMenu').css('z-index', -1);
                    $('#page').removeClass('showMobileMenu');
                    $('#header').removeClass('showMobileMenu');
                    $('#mobileMenu').removeClass('showMobileMenu');
                    setTimeout(function () {
                        $('#mobileMenu').css('display', 'none');
                    }, 300);
                }
            });
        })
    </script> 
  <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script> 
 </head>
 <body>
  <div id="fb-root"></div> 
  <script>
        (function(d, s, id) {
            var js, fjs = d.getElementsByTagName(s)[0];
            if (d.getElementById(id)) return;
            js = d.createElement(s);
            js.id = id;
            js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=225285904297383";
            fjs.parentNode.insertBefore(js, fjs);
        }(document, 'script', 'facebook-jssdk'));
    </script> 
  <script language="javascript" type="text/javascript">
        $(function() {
            // Making the code snippets look pretty
            $('pre').addClass('prettyprint');
                        
            // Adding a bunch of additional formatting to article sections
            $('<p class="article-pullquote-arrow"></p>').insertAfter('.article-pullquote');
            $('figcaption').html(function(i, html) {
                return html.replace(/(\w+\s\w+)/, '<b>$1</b>');
            });
            $('.article-listings h1').html(function(i, html) {
                return html.replace(/(\w+\s\w+)/, '<b>$1</b>');
            });
            $('.article-tables h1').html(function(i, html) {
                return html.replace(/(\w+\s\w+)/, '<b>$1</b>');
            });

            //$('#largerText').click(function() {
            //    $('.article-body').css('font-size','2em');
            //});

            // Serving up some ads
            var adIndex = 0;
            var firstAdPass = true;
            var ads = [
                { nameFirst: 'Magazine_Article_Rectangle_First', nameSubsequent: 'Magazine_Article_Rectangle_Subsequent', sizes: [[300, 250], [336, 280]], cssClass: 'rectangle-ad' },
                { nameFirst: 'Magazine_Article_Banner_First', nameSubsequent: 'Magazine_Article_Banner_Subsequent', sizes: [468, 60], cssClass: 'banner-ad' }
            ];
            var childIndex = 3;
            var paragraphsBetweenAds = 13;
            var totalParagraphs = $('div.article-body p').length;
            if (totalParagraphs > 100) paragraphsBetweenAds = 17;
            else if (totalParagraphs > 90) paragraphsBetweenAds = 16;
            else if (totalParagraphs > 80) paragraphsBetweenAds = 15;
            else if (totalParagraphs > 70) paragraphsBetweenAds = 14;

            for (var counter = 0; counter < 1000; counter++) {
                var set = $('div.article-body p:nth-of-type(' + childIndex + ')');
                if (set == undefined) break;
                if (set.length < 1) break;

                if (set[0].className == '') {
                    var previousSet = set.prev();
                    var previousIsOk = false;
                    if (previousSet.length > 0) {
                        if (previousSet[0].localName == 'p') {
                            previousIsOk = true;
                        }
                    }
                    var nextIsOk = false;
                    if (previousIsOk) {
                        var nextSet = set.next();
                        if (nextSet.length > 0) {
                            var nextSet2 = nextSet.next();
                            if (nextSet2.length > 0) {
                                var elementName = nextSet[0].localName;
                                if ((elementName == 'p' && nextSet[0].className == '') || elementName == 'h1' || elementName == 'h2' || elementName == 'h3') {
                                    nextIsOk = true;
                                }
                            }
                        }
                    }
                    if (!previousIsOk || !nextIsOk) {
                        childIndex++;
                    } else {
                        if (firstAdPass) {
                            DisplaySelfRefreshingDoubleClickAd(set, ads[adIndex].nameFirst, ads[adIndex].sizes, 'Advertisement', 'after', ads[adIndex].cssClass);
                        } else {
                            DisplaySelfRefreshingDoubleClickAd(set, ads[adIndex].nameSubsequent, ads[adIndex].sizes, 'Advertisement', 'after', ads[adIndex].cssClass);
                        }
                        childIndex += paragraphsBetweenAds;
                        adIndex++;
                        if (adIndex >= ads.length) {
                            firstAdPass = false;
                            adIndex = 0;
                        }
                    }
                } else {
                    childIndex++;
                }
            }
        });
    </script>   
  <div id="mobileMenu"> 
   <ul>
    <li class="neutralMenu"><a href="/">Home</a></li>
    <li class="consultingMenu"><a href="/Consulting">Consulting</a></li>
    <li class="staffingMenu"><a href="/Staffing">Staffing</a></li>
    <li class="magazineMenuSelected"><a href="/Magazine">Magazine</a>
     <div class="subMenu">
      <ul>
       <li><a class="areaLink" href="/Magazine">Magazine Home</a></li>
       <li><a class="areaLink" href="/Magazine/AllIssues">All Issues</a></li>
       <li><a class="areaLink" href="/Magazine/Subscribe">Subscribe</a></li>
       <li><a class="areaLink" href="/My/Magazines">My (Digital) Magazines</a></li>
       <li><a class="areaLink" href="/My/Fulfillment">Where is my Magazine?</a></li>
       <li><a class="areaLink" href="/My">My Subscriber Account</a></li>
       <li><a class="areaLink" href="/Advertise">Advertise</a></li>
       <li><a class="areaLink" href="/Write">Write</a></li>
      </ul>
     </div></li>
    <li class="frameworkMenu"><a href="/Framework">Framework</a></li>
    <li class="trainingMenu"><a href="/Training">Training</a></li>
    <li class="vfpconversionMenu"><a href="/VFPConversion">VFP Conversion</a></li>
    <li class="neutralMenu"><a href="/logon">Sign in!</a></li>
   </ul> 
  </div> 
  <div id="header" class="headerLarge"> 
   <div id="logo"> 
    <img src="/Images/Logos/CODEMagazine_Small_Blue.png">
    <br> 
   </div> 
   <div id="menu"> 
    <ul>
     <li class="neutralMenu"><a href="/">Home</a>
      <div class="subMenu article">
       <ul>
        <li><a class="areaLink" href="/">CODE Home</a></li>
        <li><a class="areaLink" href="/About">About Us</a></li>
        <li><a class="areaLink" href="/videos">Videos</a></li>
        <li><a class="areaLink" href="/Press">Press Releases</a></li>
        <li><a class="areaLink" href="/People">People</a></li>
        <li><a class="areaLink" href="/Jobs">Careers</a></li>
        <li><a class="areaLink" href="/Home/Privacy">Privacy Policy</a></li>
        <li><a class="areaLink" href="/contact">Contact Us</a></li>
       </ul>
      </div></li>
     <li class="consultingMenu"><a href="/Consulting">Consulting</a>
      <div class="subMenu">
       <ul>
        <li><a class="areaLink" href="/Consulting">Consulting Home</a></li>
        <li><a class="areaLink" href="/Consulting/Services">Services &amp; Technologies</a></li>
        <li><a class="areaLink" href="/VFPConversion">VFP Conversion</a></li>
        <li><a class="areaLink" href="/Cloud">Azure &amp; Other Clouds</a></li>
        <li><a class="areaLink" href="/Energy">Energy Software</a></li>
        <li><a class="areaLink" href="/contact">Contact Us</a></li>
       </ul>
      </div></li>
     <li class="staffingMenu"><a href="/Staffing">Staffing</a>
      <div class="subMenu">
       <ul>
        <li><a class="areaLink" href="/Staffing">Staffing Home</a></li>
        <li><a class="areaLink" href="/Staffing">Looking for Staff?</a></li>
        <li><a class="areaLink" href="/Staffing/Apply">Looking for Work?</a></li>
        <li><a class="areaLink" href="/contact">Contact Us</a></li>
       </ul>
      </div></li>
     <li class="magazineMenuSelected"><a href="/Magazine">Magazine</a>
      <div class="subMenu">
       <ul>
        <li><a class="areaLink" href="/Magazine">Magazine Home</a></li>
        <li><a class="areaLink" href="/Magazine/AllIssues">All Issues</a></li>
        <li><a class="areaLink" href="/Magazine/Subscribe">Subscribe</a></li>
        <li><a class="areaLink" href="/My/Magazines">My (Digital) Magazines</a></li>
        <li><a class="areaLink" href="/My/Fulfillment">Where is my Magazine?</a></li>
        <li><a class="areaLink" href="/My">My Subscriber Account</a></li>
        <li><a class="areaLink" href="/Advertise">Advertise</a></li>
        <li><a class="areaLink" href="/Write">Write</a></li>
       </ul>
      </div></li>
     <li class="frameworkMenu"><a href="/Framework">Framework</a>
      <div class="subMenu">
       <ul>
        <li><a class="areaLink" href="/Framework">Framework Home</a></li>
        <li><a class="areaLink" href="/Framework/GetStarted">Get Started &amp; Documentation</a></li>
        <li><a class="areaLink" href="/Framework/Download">Download</a></li>
        <li><a class="areaLink" href="/Framework/Support">Support &amp; Services</a></li>
       </ul>
      </div></li>
     <li class="trainingMenu"><a href="/Training">Training</a>
      <div class="subMenu">
       <ul>
        <li><a class="areaLink" href="/Training">Training Home</a></li>
        <li><a class="areaLink" href="/Training">Classes</a></li>
        <li><a class="areaLink" href="/Training/Mentoring">Mentoring</a></li>
        <li><a class="areaLink" href="/StateOfDotNet">State of .NET</a></li>
        <li><a class="areaLink" href="/Lunch">Lunch with CODE</a></li>
        <li><a class="areaLink" href="/CodeCamps">Code Camps</a></li>
        <li><a class="areaLink" href="/Training/TopicSpecificTraining">ASP.NET MVC</a></li>
        <li><a class="areaLink" href="/DevNet/DevNetTraining">DevNet Training</a></li>
       </ul>
      </div></li>
     <li class="vfpconversionMenu"><a href="/VFPConversion">VFP Conversion</a>
      <div class="subMenu">
       <ul>
        <li><a class="areaLink" href="/VFPConversion">VFP Conversion Home</a></li>
        <li><a class="areaLink" href="/VFPConversion/VFPServices">Services</a></li>
        <li><a class="areaLink" href="/VFPConversion/Tools">Tools</a></li>
        <li><a class="areaLink" href="/VFPConversion/Articles">Articles</a></li>
        <li><a class="areaLink" href="/VFPConversion/History">Fox End of Life</a></li>
        <li><a class="areaLink" href="/contact">Contact Us</a></li>
       </ul>
      </div></li>
     <li class="neutralMenu"><a href="/logon">Sign in!</a></li>
    </ul> 
   </div> 
   <div id="mobileMenuIcon">
    <img src="/Images/MobileMenu.png">
   </div> 
  </div> 
  <div id="page"> 
   <div id="main"> 
    <div id="contentColumnSingle"> 
     <div class="advertisement leaderboard-ad" style="width: 728px; margin-left: auto; margin-right: auto;">
       Advertisement: 
      <div id="div-gpt-ad-1378457852955-1" style="width:728px; height:90px;"> 
       <script type="text/javascript">
                googletag.cmd.push(function () {
                    var slot1 = googletag.defineSlot("/36824843/Magazine_Article_Leaderboard_Top", [728, 90], "div-gpt-ad-1378457852955-1").addService(googletag.pubads());
                    googletag.enableServices();
                    googletag.display("div-gpt-ad-1378457852955-1");
                    setInterval(function () { googletag.pubads().refresh([slot1]); }, 60000);
                });
            </script> 
      </div> 
     </div> 
     <h1 class="title">Building Speech-Enabled Applications with ASP.NET</h1> 
     <p>By <b>Robbins, Thom</b></p> 
     <div class="download1ColumnButton" style="padding-top: 25px; padding-bottom: 25px;"> 
     </div> 
     <div class="side-info"> 
      <p><a href="https://www.codemag.com/subscribe/freearticle6b1?utm_source=articleviewer&amp;utm_medium=banner&amp;utm_campaign=Article-Viewer-Free-Subs"></a><img src="/Images/Logos/DoodleFreeSubscriptionBanner.png" alt="CODE Magazine"></p> 
      <div style="margin-bottom: 20px; overflow: no-display;"> 
       <div class="fb-share-button" style="float: left;" data-href="http://www.codemag.com/article/0511041" data-type="button"></div> 
       <div style="margin-left: 5px; margin-top: 0; float: left;"> 
        <a href="https://twitter.com/share" class="twitter-share-button" data-via="codemagazine" data-count="none">Tweet</a> 
        <script>!function (d, s, id) { var js, fjs = d.getElementsByTagName(s)[0], p = /^http:/.test(d.location) ? 'http' : 'https'; if (!d.getElementById(id)) { js = d.createElement(s); js.id = id; js.src = p + '://platform.twitter.com/widgets.js'; fjs.parentNode.insertBefore(js, fjs); } }(document, 'script', 'twitter-wjs');</script> 
       </div> 
       <div style="margin-left: 5px; margin-top: 0; float: left;"> 
        <script src="//platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script> 
        <script type="IN/Share"></script> 
       </div> 
       <div class="clearfix"></div> 
      </div> 
      <p class="author-image"><img src="/Article/AuthorPhotoSmall/063fa45e-547c-4274-a101-b8fc6a62dba1"></p> 
      <h1 class="author-name">Robbins, Thom</h1> 
      <p>Thom Robbins a Developer Evangelist with Microsoft in New England. He is a frequent speaker at a variety of events including Dev Days and VS Live. Thom is also a regular contributor to various magazines including .Net Magazine and XML Web Service Journal. He spends his time working with developers in New England. When he’s not writing code he can be found with his wife, Denise, at their home in New Hampshire. You can reach Thom at trobbins@microsoft.com or through his blog at http://blogs.msdn.com/trobbins.</p> 
      <div> 
       <h2>This article was published in:</h2> 
       <a href="/Magazine/Issue/f7ddaa17-649a-46d9-bb76-258d7f5e9a4e"><img src="/Magazine/Cover/f7ddaa17-649a-46d9-bb76-258d7f5e9a4e"></a> 
      </div> 
      <div> 
       <h2>This article was filed under:</h2> 
       <ul class="tech-list"> 
        <li><a href="/Magazine/ByCategory/ASP.NET%20WebForms">ASP.NET WebForms</a></li> 
        <li><a href="/Magazine/ByCategory/Speech-Enabled%20Applications">Speech-Enabled Applications</a></li> 
        <li><a href="/Magazine/ByCategory/Techniques">Techniques</a></li> 
       </ul> 
       <div class="clearfix"></div> 
      </div> 
      <div class="advertisement skyscraper-ad">
        Advertisement: 
       <div id="div-gpt-ad-1378457852954-1" style="width:160px; height:600px;"> 
        <script type="text/javascript">
                    googletag.cmd.push(function () {
                        var slot1 = googletag.defineSlot("/36824843/Magazine_Article_Skyscraper_Right", [160, 600], "div-gpt-ad-1378457852954-1").addService(googletag.pubads());
                        googletag.enableServices();
                        googletag.display("div-gpt-ad-1378457852954-1");
                        setInterval(function () { googletag.pubads().refresh([slot1]); }, 60000);
                    });
                </script> 
       </div> 
      </div> 
     </div> 
     <div class="article-body">
      <p>While sleepless the other night, I was channel surfing and ran across a rerun of the 1968 science fiction classic “2001: A Space Odyssey.”</p>
      <p> If you haven’t seen this movie, it’s definitely a must see. HAL, one of the main characters of the movie, is a slightly psychotic speech-enabled super computer. HAL is responsible for steering the Discovery spacecraft on its ill-fated Jupiter mission. As I watched the movie I was completely amazed at HAL’s abilities. HAL handled press interviews, played a wicked game of chess, has varied opinions on art, controls life support, and can read lips. Not to completely destroy the movie if you haven’t seen it, but I have to say that I am grateful that most of the movie’s predictions aren’t true. However, like the HAL of 1968, speech-enabled applications have become a core requirement for both corporate and commercial developers. In this article, I’ll help you explore the Microsoft Speech Platform that comprises the Speech Application Software Development Kit (SASDK) and Microsoft Speech Server 2004. I’ll also show you how you can use these technologies with Visual Studio 2003 to both build and deploy speech-enabled applications.</p>
      <p class="article-pullquote">The SASDK is the core component of the Microsoft Speech platform that enables developers to create and debug speech-based applications for telephones, mobile devices, and desktop PCs.</p>
      <p>The name HAL was derived from a combination of the words “heuristic” and “algorithmic.” These are considered the two main processes involved in human learning. These were important characteristics for early speech developers as well. Initially, speech applications were targeted at a “say anything” programming mentality. The result was a very specialized type of system-level programmer. Among other things, they studied natural speech and language heuristics to develop a set of unique algorithms for their applications. Interestingly, they were actually part application developer, language expert, and hardware engineers. The good news is that the mainstreaming of speech-based technology has enabled us, as mere mortal ASP.NET developers, to leverage this type of technology. The familiar combination of Visual Studio 2003 coupled with a free add-on kit called the Speech Application Software Development Kit (SASDK) allows your Web-based application to include speech functionality. It is the integration of these familiar toolsets into a server-based product called Microsoft Speech Server 2004 that completes the server-side solution and brings speech to the mainstream Windows platform. </p>
      <h2>The Architecture of Speech-Enabled Applications</h2>
      <p>The SASDK is the core component of the Microsoft Speech platform that enables Web developers to create and debug speech-based applications for telephones, mobile devices, and desktop PCs. The SASDK includes a set of samples, documentation, and debugging utilities that are important for developing speech applications. This includes a set of speech authoring tools I will cover later that are directly integrated into the Visual Studio 2003 environment. Finally, the SASDK installs a new project template (<b>Figure 1) </b>that serves as the starting point for any speech application. Typically, the lifecycle of developing a speech application starts with the tools available within the SASDK and Visual Studio 2003, and once completed the application in then deployed to the Microsoft Speech Server 2004 (MSS). </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 1.tif">
       <figcaption>
        Figure 1:The SASDK installs a set of templates that can be used to develop speech-based applications.
       </figcaption>
      </figure>
      <p>One of the main design goals of both the MSS and SASDK was to leverage existing standards and ensure industry compliance to make speech a natural extension of any Web-based application. In addition to the basics of XML, HTML, and JavaScript, there are several speech-related standards as shown in <b>Table 1</b>. </p>
      <h3>Defining the Application</h3>
      <p>Both the SASDK and Microsoft Speech Server are designed to develop and support two distinct types of speech-based applications-voice only and multimodal. By default the developer selects the application type when they create a new project as shown in <b>Figure 2</b>. The role of the SASDK is to provide a developer-based infrastructure to support both the development and debugging of either type on a local machine. On the other hand, the MSS is designed to provide a production-level server-side environment for deployed speech-based applications. <b>Figure 3 </b>shows a sample schematic of a production environment that includes Microsoft Speech Server. </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 2.tif">
       <figcaption>
        Figure 2:Determines the type of speech-based application during the creation of a project.
       </figcaption>
      </figure>
      <figure>
       <img src="/Article/Image/0511041/Figure 3.tif">
       <figcaption>
        Figure 3:An example of what your production speech environment may contain.
       </figcaption>
      </figure>
      <p>Voice-only applications are designed to never expose a visible Web interface to end users. This type of speech application includes both traditional voice only applications and touchtone or Dual Tone Multi-Frequency (DTMF) applications. In either case, all interaction with the application is done by either voice only or keypad presses. The result is a menu option or selection based on the user’s response. Once deployed, the Microsoft Speech Server includes two major components that are designed to support these types of applications in a production environment. The Telephony Application Service (TAS) is responsible for providing a voice only browser or SALT interpreter which is used to process the SALT markup generated by the ASP.NET speech-enabled Web application. Also, the Speech Engine Services (SES) that provides the speech recognition engine also handles the retrieval of the output generated by the application. Finally, the Telephony Interface Manager (TIM) component provides the bridge between the telephony board hardware which is connected to both the network and the TAS. </p>
      <p class="article-pullquote">Both the SASDK and Microsoft Speech Server are designed to develop and support two distinct types of speech based applications-voice only and multimodal.</p>
      <p>Multimodal applications, on the other hand, are designed to combine speech input and output with a Web-based graphical user interface. In a traditional Web-based GUI, the user directs the system through a combination of selections and commands. Each action is translated into a simple sentence that the system can execute. Fundamentally, each sentence contains verbs that act on a direct object. The selection of the mouse defines the direct object of a command, while the menu selection describes the action to perform. For example, by selecting a document and choosing print, the user is telling the computer to “Print this document.” In multimodal systems, speech and mouse input are combined to form more complex commands. For example, by selecting a document and simultaneously saying “Print five of this” the user successfully collapses several simple sentences into a single Click command. Obviously this type of application is best suited for devices that support both speech and ASP.NET. However, for mobile devices like PDAs this is particularly well-suited because conventional keyboard input is difficult. For developers, a multimodal application combines ASP.NET and speech controls with server-side extensions like SALT for application delivery. Once an application is deployed to Microsoft Speech Server it is responsible for providing output markup that includes SALT, HTML, JavaScript, and XML to the client and the speech services needed for voice interaction. </p>
      <h2>Building Speech Applications</h2>
      <p>Like any Web-based application, speech applications have two major components-a Web browser component and server component. Realistically, the device that consumes the application will ultimately determine the physical location of the Speech Services engine. For example, a telephone or DTMF application will natively take advantage of the server-side features of Microsoft Speech Server. However, a desktop Web application will leverage the markup returned by MSS in conjunction with desktop recognition software and the speech add-ins for Microsoft Internet Explorer. </p>
      <p>In addition to the default project template, the SASDK also installs a set of speech-enabled ASP.NET controls. By default these controls are added to the Visual Studio toolbox as shown in <b>Figure 4</b>. Fundamentally, these controls operate identically to the standard set of ASP.NET Web controls except that during the server-side rendering phase of a Web page containing a speech control, the output document contains SALT, SSDN, and SRGD in addition to the standard HTML and JavaScript. The document returned to the speech-enabled client is first parsed and then any additional grammar files specified in the returned markup are downloaded. Additionally, if the Speech Services engine is local, prompts or pre-recorded text are also downloaded. Finally, both the SALT client and Web browser invoke the series of &lt;prompt&gt; and &lt;listen&gt; elements specified by the markup. Any additional client-side elements are invoked by calling the client-side start() function. </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 4.tif">
       <figcaption>
        Figure 4:The set of speech controls installed by the SASDK into Visual Studio 2003.
       </figcaption>
      </figure>
      <p>Once started, the Speech Services engine listens for input from the user when a &lt;listen&gt; element is invoked. Once it receives the response audio or utterances it compares it’s analysis of the audio stream to what is stored in the grammar file, looking for a matching pattern. If the recognizer finds a match a special type of XML document is returned. This document contains markup called Semantic Markup Language (SML) and is used by the client as the interpretation of what the user said. The client then uses this document to determine what to do next. For example, execute a &lt;prompt&gt; or &lt;listen&gt; element. The cycle repeats itself until the application is done or the session ends. </p>
      <p>All ASP.NET speech controls are implemented in the framework namespace <a href="http://Microsoft.Speech.Web.UI">Microsoft.Speech.Web.UI</a>. Within the namespace, these controls are categorized by their functions. By default, these categories are basic, dialog, application controls, and Call management controls. Call Management controls are an abstraction of the Computer Supported Telecommunications Applications (CSTA) messages you’ll use in your application.</p>
      <p>Like any other ASP.NET Web control, the speech controls are designed to provide a high level abstraction on top of the lower-level XML and script emitted during run time. Also, to make the implementation of these controls easier, each control provides a set of property builders as shown in <b>Figure 5</b>.</p>
      <figure>
       <img src="/Article/Image/0511041/Figure 5.tif">
       <figcaption>
        Figure 5:Property builders are used to simplify the design of a speech application.
       </figcaption>
      </figure>
      <h3>The Basic Speech Controls</h3>
      <p>The basic speech controls, which include Prompt and Listen, are designed to create and manipulate the SALT hierarchy of elements. These controls provide server-side functionality that is identical to the elements invoked during run time on the client. The Prompt control is designed to specify the content of the audio output. The Listen controls perform recognition, post processing, recording, and configuration of the speech recognizer. Ideally, the Basic controls are primarily designed for tap- and talk-based client devices and applications designed to confirm responses and manage application flow through a GUI. </p>
      <p>The basic speech controls are designed exclusively to be called by client-side script. Examining the “Hello World” example in <b>Listing 1</b>, you will notice that once the user presses the Web page button this then calls the <b>OnClick </b>client-side event. This event invokes the Start method of the underlying prompt or exposed SALT element. The event processing for the basic speech controls is identical to features of SALT. Fundamentally, these features are based on the system’s ability to recognize user input. The concept of recognition or “reco” is used by SALT to describe the speech input resources and provides event management in cases where valid recognition isn’t returned. For example, you create specific events such as “reco” and “noreco” and then assign the name of these procedures to control properties such as <b>OnClientReco </b>and OnClientNoReco. When the browser detects one of these <b>events</b>, it calls the assigned procedure. The procedure is then able to extract information about the event directly from the event object.</p>
      <p>The <b>Listen </b>control is a server-side representation of the SALT List element. The Listen element specifies possible speech inputs and provides control of the speech recognition process. By default, only one Listen element can be active at a time. However, a Web page can have more than one <b>Listen </b>control and each control can be used more than once.</p>
      <p><b>Listing 2 </b>represents the HTML markup when a Listen control is added to a Web page. As you can see, the main elements of the <b>Listen </b>control are grammars. Grammars are used to direct speech input to a particular recognition engine. Once the audio is recognized, the resulting text is converted and placed into an HTML output. </p>
      <h3>Dialog Speech Controls</h3>
      <p>The dialog speech controls, which include the <b>QA</b>, <b>Command</b>, and <b>Semantic </b>items, are designed to build questions, answers, statements, and digressions for an application. Programmatically, these controls are called through the script element, <b>RunSpeech</b>, which manages both the execution and state of these controls. <b>RunSpeech </b>is a client-side JavaScript object that provides the flow control for voice-only applications. Sometimes referred to as the dialog manager, it is responsible for activating the dialog speech controls on a page in the correct order. <b>RunSpeech </b>activates a Dialog speech control using the following steps:</p>
      <ol>
       <li><b>RunSpeech </b>establishes the Speech Order of each control based on the control’s source order or <b>SpeechIndex </b>property.</li>
       <li><b>Runspeech </b>examines the Dialog Speech controls on the page in Speech Order. Based on the order specified in the page, it locates the first dialog control within that list and then initializes it.</li>
       <li><b>RunSpeech </b>submits the page.</li>
      </ol>
      <p>The QA control within the <b><a href="http://Microsoft.Speech.Web.UI.QA">Microsoft.Speech.Web.UI.QA</a></b>namespace is used to ask questions and obtain responses from application users. It can be used as either a standalone prompt statement or can supply answers for multiple questions without having to ask them. <b>Listing 3 </b>shows an example of how you can mark up this control.</p>
      <p>The <b>Command </b>control contained in the <b><a href="http://Microsoft.Speech.Web.UI">Microsoft.Speech.Web.UI</a>.Command </b>namespace enables developers to add out-of-context phrases or dialogue digressions. These are the statements that occur during conversations that don’t seem to make sense for the given dialog. For example, allowing an application user to say “help” at any point. The following is an example of how you can apply this globally to a speech application. </p>
      <pre><code>&lt;speech:command id="HelpCmd" runat="server"
scope="subType"
type="Help" xpathtrigger="/SML/Command/Help"&gt;
    
   &lt;grammar id="GlobalHelpCmd" runat="server"
src="GlobalCommands.grxml" /&gt;
    
&lt;/speech:command&gt;

</code></pre>
      <p>The <b>SemanticMap </b>and <b>SemanticItem </b>controls track the answers and overall state management of the dialogue. You use Semantic items to store elements of contextual information gathered from a user. While the semantic map simply provides a container for multiple semantic items, each <b>SemanticItem </b>maintains its own state. For example, these include empty, confirmed, or awaiting confirmation. You’ll use the <b>SemanticMap </b>to group the <b>SemanticItem </b>controls together. Keep in mind that while the QA control manages the overall semantics of invoking recognition, the storage of the recognized value is decoupled from the control. This simplifies state management by enabling the concept of centralized application state storage. Additionally, this makes it very easy to implement mixed-initiative dialog in your application. In a mixed-initiative dialog, both the user and the system are directing the dialog. For example, the markup for these controls would look like the following.</p>
      <pre><code>&lt;speech:semanticmap id="TheSemanticMap"
runat="server"&gt;
&lt;speech:semanticitem id="siSubSize"
runat="server" /&gt;
&lt;speech:semanticitem id="siSubType"
runat="server"/&gt;
&lt;speech:semanticitem id="siFootLong"
runat="server" /&gt;
&lt;/speech:semanticmap&gt;

</code></pre>
      <h2>Prompt Authoring</h2>
      <p>Prompts are such an important part of any application because they serve as the voice and interaction point with users. Basically, they act as the main interface for a dialog-driven application. As you begin to build a speech application you will quickly notice that the synthesized voice prompts sound a bit mechanical; definitely not like the smooth slightly British tone of HAL in “2001.” This is because, by default, unless otherwise specified the local text to speech engine will synthesize all prompt interaction. Prompts should be as flexible as any portion of the entire application. Just as you should invest time in creating a well-designed Web page, so should you spend time on designing a clean sounding dynamic prompt system for application users to interact with. As with any application, the goal is to quickly prototype the proof of concept and usability testing. The extensibility of the speech environment makes it easy to have a parallel development track occurring of the dialog and prompt recording. </p>
      <p>The Prompt database is the repository of recorded prompts for an application. It is compiled and downloaded to the telephony browser during run time. Before the speech engine plays any type of prompt, it queries the database and if a match is found, it plays the recoding instead of using the synthesized voice. Within Visual Studio the Prompt Project is used to store these recordings and is available within the new project dialog as shown in <b>Figure 6</b>. The Prompt Project contains a single prompt database with a .promptdb extension. By default, Prompt databases can be shared across multiple applications and mixed together. In practice it’s actually a good idea to use separate prompt databases across a single application to both reduce size and make it more manageable. The database can contain wave recording either directly recorded or imported from external files. </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 6.tif">
       <figcaption>
        Figure 6:The prompts project contains a database that is used to store pre-recorded voice prompts.
       </figcaption>
      </figure>
      <p>You can edit the prompts database through Visual Studio’s Prompt Editor as shown in <b>Figure 7</b>. This window is divided into a Transcription and Extraction window. The Transcription window (top) is used to identify an individual recording and its properties. These include playback properties including volume, quality, and wave format. More importantly, you use the Transcription window to define the text representation of the wave file content. The bottom portion of the Prompt Editor contains the Extraction window. This identifies one or more consecutive speech alignments of a transcription. Essentially, extractions constitute the smallest individual element or words within a transcription that a system can use as part of an individual prompt. </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 7.tif">
       <figcaption>
        Figure 7:Recording new prompts can be done directly within Visual Studio 2003.
       </figcaption>
      </figure>
      <h3>Recording a Prompt</h3>
      <p>The first step in creating a prompt is to add a new transcription using the Prompt Editor. Once this is done you can then record or import a wave file that matches the transcription exactly. For example, a transcription may be as short as a single word or as long as a single sentence. When creating transcriptions you should keep the following things in mind. </p>
      <ul>
       <li>Transcriptions are always full sentences. This makes it easier for a speaker to record with the correct voice inflections.</li>
       <li>Transcriptions contain no punctuation. When recording the prompt editor will automatically remove any punctuation from a transcription because they are not explicitly stated in a recording.</li>
      </ul>
      <p>Once you type the sentence that will be used for transcription you can then record the prompt as shown in <b>Figure 8</b>. Once you’ve recorded the prompt the Prompt Editor will attempt to create an alignment of the sentence and the transcription as shown <b>Figure 9</b>. Once a successful transcription alignment is completed it is time to build extractions. This is done by selecting a series of consecutive alignments from the transcription to form an extraction. Extractions can be combined dynamically at run time to create a prompt. For example, the extractions “ham,” “roast beef,” “club,” and “sandwich” can be combined with “you ordered a” to create the prompt, “You ordered a ham sandwich.” </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 8.tif">
       <figcaption>
        Figure 8:Editing the prompts database within Visual Studio 2003.
       </figcaption>
      </figure>
      <figure>
       <img src="/Article/Image/0511041/Figure 9.tif">
       <figcaption>
        Figure 9:Proper speech recognition requires defining an alignment between the prompts and transcription within Visual Studio 2003.
       </figcaption>
      </figure>
      <p>Once all the application prompts are recorded they are then referenced within a project to create inline prompts as shown in <b>Figure 10</b>. Within an application this creates a prompts file that contains only the extractions identified within the prompts database. By default, anything not marked as an extraction is not available within a referenced application. The result is that when the application runs, the prompt engine matches the prompt text in your application with the extractions in your database. If the required extraction is found it is played, otherwise the text-to-speech engine uses the system-synthesized voice to play the prompt. </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 10.tif">
       <figcaption>
        Figure 10:Sharing a prompts database across projects is simply a process of creating a reference.
       </figcaption>
      </figure>
      <h3>Building Dynamic Prompts</h3>
      <p>Prompts can be defined statically as you saw earlier using the &lt;prompt&gt; tag or <b>QuestionPrompt </b>property of application controls. However, most speech applications tend to use dynamically defined prompts based on extractions. </p>
      <p>Programmatically, this is provided by the Dialog Speech Control through the <b>PromptSelectFunction </b>property of every Dialog and Application speech control. The <b>PromptSelectFunction </b>property is actually a callback function for each control that is executed on the client side. It is responsible for returning the prompt and its associated markup to use when the control is activated. This built in function enables speech applications to check and react to the current state of the dialog as shown in the following code. </p>
      <pre><code>function GetPromptSelectFunction() {
 var lastCommandOrException = "";
 var len = RunSpeech.ActiveQA.History.length;
 if(len &gt; 0) {
 lastCommandOrException =
 RunSpeech.ActiveQA.History[len - 1];
 }
    
 if (lastCommandOrException == "Silence") {
 return "Sorry I couldn't hear you. What menu
selection would you?";
 }
}

</code></pre>
      <p>In this example, the <b>PromptSelectFunction </b>is checking the most recent voice command looking for an exception like silence. If this error is encountered, the prompt is modified to provide valid feedback to the user. <b>PromptSelectFunction </b>can be added inline. However, the <b>PromptFunction </b>Editor tool within Visual Studio is designed to manage the individual prompts and their states and is directly integrated into the Prompt Validation speech engine. This Visual Studio window is activated through the prompt function file as shown in <b>Figure 12</b>.</p>
      <figure>
       <img src="/Article/Image/0511041/Figure 12.tif">
       <figcaption>
        Figure 11:Editing and managing the code and states for each prompts can be done through the prompt function file.
       </figcaption>
      </figure>
      <h2>Grammar Authoring</h2>
      <p>Speech is an interactive process of prompts and commands. Semantic Markup Language Grammars are the set of structured command rules that identify words, phrases, and valid selections that are collected in response to an application prompt. Grammars provide both the exact words and the order in which the commands can be said by application users. A grammar can consist of a single word, a list of acceptable words or complex phrases. Structurally it’s a combination of XML and plain text that is the result of attempting to match the user responses Within MSS, this set of data conforms to the W3C Speech Recognition Grammar Specification (SRGS). An example of a simple grammar file that allows for the selection of a sandwich is shown in <b>Listing 4</b>.</p>
      <p>Grammars form the guidelines that applications must use to recognize the possible commands that a user might issue. Unless the words or phrases are defined in the grammar structure, the application cannot recognize the user’s speech commands and returns an error. You can think of grammar as a vocabulary of what can be said by the user and what can be understood by the application. This is like a lookup table in a database that provides a list of options to the user, rather than accepting free-form text input. </p>
      <p>A very simple application can limit spoken commands to a single word like “open” or “print.” In this case, the grammar is not much more than a list of words. However, most applications require a richer set of commands and sentences. The user interacting with this type of speech application expects to use a normal and natural language level. This increases the expectation for any application and requires additional thought during design. For example, an application must accept, “I would like to buy a roast beef sandwich,” as well as, “Gimme a ham sandwich.”</p>
      <p>A well-defined grammar provides a bit more functionality than that, of course. It won’t just define the options, but also the additional phrases such as a preamble to a sentence. For example, the grammar corresponding to the question above must also recognize “I would like to” in addition to the option “roast beef.” So given this, the grammar is essentially a sentence or sequence of phrases broken down into their smallest component parts. </p>
      <p>Another job of the grammar is to map multiple similar phrases to a single semantic meaning. Consider all the ways a user can ask for help. The user may say “help,” “huh,” or “what are my choices.” Ultimately, however, in all three cases the user is asking for help. The grammar is responsible for defining all three phrases and maps them to a single set of options. The benefit is that a developer only has to write the code to deal with the phrase “help.” </p>
      <h3>Implementing Grammar</h3>
      <p>Within a Visual Studio speech application, grammar files have a .grxml extension and are added independently as shown in <b>Figure 13</b>. Once added to a project, the Grammar Editing Tool, as shown in <b>Figure 14</b>, is used to add and update the independent elements. This tool is designed to provide a graphical layout using a left to right view of the phrases and rules stored in a particular grammar file. Essentially, it provides a visualization of the underlying SRGS format, in a word graph rather than the hierarchical XML. </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 13.tif">
       <figcaption>
        Figure 12:Within a Visual STudio speech application, grammar files have a .grxml extention and are added directly to the project.
       </figcaption>
      </figure>
      <figure>
       <img src="/Article/Image/0511041/Figure 14.tif">
       <figcaption>
        Figure 13:The Grammar Editing Tool is used to manage and define the various elements of the speech prompts.
       </figcaption>
      </figure>
      <p>For developers, the goal of the Grammar Editor is to present a flowchart of the valid grammar paths. A valid phrase is defined by a successful path through this flowchart. Building recognition rules is done by dragging the set of toolbox elements listed in <b>Table 2 </b>onto the design canvas. The design canvas displays the set of valid toolbox shapes and represents the underlying SRGS elements. </p>
      <p>During development the Grammar Editor provides the ability to show both the path of an utterance and the returned SML document as shown in <b>Figure 15</b>. For example, the string, “I would like to buy a ham sandwich” is entered into the Recognition string text box at the top and the path the recognizer took through the grammar is highlighted. At the bottom of the screen the build output window displays a copy of the SML document returned by the recognizer. This feature provides an important way to validate and test that both the grammar and SML document returned are accurate. </p>
      <figure>
       <img src="/Article/Image/0511041/Figure 15.tif">
       <figcaption>
        Figure 14:During development the grammar Editor provides the ability to show both the path of an utterance and the returned SML within the Visual Studio environment.
       </figcaption>
      </figure>
      <p>Structurally the editor provides the list of rules that identify words or phrases that an application user is able to provide. A rule defines a pattern of speech input that is recognized by the application. At run time the speech engine attempts to find a complete path through the rule using the supplied voice input. If a path is found the recognition is successful and results are returned to the application in the form of an SML document. This is an XML-based document that combines the utterance, semantic items, and a confidence value defined by the grammar as shown below.</p>
      <pre><code>&lt;SML confidence="1.000" text="ham"
   utteranceConfidence="1.000"&gt; ham&lt;/SML&gt;

</code></pre>
      <p>The confidence value is a score returned by the recognition engine that indicates the degree of confidence it has in recognizing the audio. Confidence values are often used to drive the confirmation logic within an application. For example, you may want to trigger a confirmation answer if the confidence value falls below a specific threshold such as .8.</p>
      <p>The SASDK also includes the ability to leverage other data types as grammar within an application. The clear benefit is that you don’t have to manually author every specific grammar rule. Adding these external grammars can be done through an included Grammar Library or using a process called data-driven grammar. </p>
      <p>The Grammar Library is a reusable collection of rules provided in SRGS format that are designed to cover a variety of basic types. For example, this includes grammar for recognizing numbers and mapping holiday dates to their actual calendar dates. Data-driven grammar is a feature provided by three Application Speech controls. The <b>ListSelector </b>and <b>DataTableNavigator </b>controls enable you to take SQL Server data, bind it to the control, and automatically make all the data accessible by voice. Logically this means that you don’t have to recreate all the data stored in a database into a grammar file. The third control, the <b>AlphaDigit </b>control, isn’t a data-bound control. Rather, it automatically generates a grammar for recognizing a masked sequence. For example, the mask “DDA” would recognize any string following the format: digit, digit, character.</p>
      <h2>Application Deployment</h2>
      <p>Up to this point the discussion has focused exclusively on the development phase of speech applications using the SASDK. Ultimately, the SASDK is a faithful simulated representation of a Speech Server, coupled with additional development and debugging tools. The benefit is that when it comes time to deploy speech applications they exist as a set of ASP.NET Web pages and artifacts as shown in <b>Table 3</b>. Deployment is simply the process of packaging and deploying these files using the same methodology as any ASP.NET application.</p>
      <p>However, there are some inherent architecture differences to remember between the SASDK and the Speech Server environment. For example, the Telephone Application Simulator (TASIM) provided by the SASDK is used in the development of voice-only applications. It simulates the functions of both the TAS and SES components of a Speech Server. Once the application is deployed you wouldn’t be able to access voice-only applications from Internet Explorer. In a production environment, the TAS component and the SES component are completely separate. The TAS is responsible for handling the processing of incoming calls and SES handles speech input. In the development environment both of these functions are handled by the TASIM. Additionally, in the development of a multimodal application, debugging is provided by the Speech add-in for Internet Explorer, which has been enhanced to include extensions and integration with the Speech Debugger. However, these enhancements aren’t available as part of the standard client installation. </p>
      <p>Finally, when developing applications using the SASDK it is possible to build applications where the paths to grammar files are specified by physical paths. However, within the production Speech Server environment, all paths to external resources such as a grammar file must be specified using a URL not a physical path. Prior to creating your deployment package it is always a good idea to switch into HTML view and verify that all grammar file paths use a relative or absolute URL. </p>
      <p>There are many different ways to deploy the components of Speech Server based on your usage and workload requirements. Each component is designed to work independently. This enables the deployment of single or multi-box configurations. The simplest deployment is to place the TAS, SES, IIS, hardware telephony board, and TIM software together on a single machine. Without a doubt the Web server is the essential component of any Speech Server deployment as it is the place where applications are deployed and the SALT-enabled Web content is generated. Even if you decide to deploy Speech Server components on separate Web Servers, each server running SES must have IIS enabled. </p>
      <p>In this article you’ve looked at how to use the SASDK and Microsoft Speech Server 2004 to develop speech-enabled ASP.NET applications. The SASDK provides the development environment that includes a simulator and debugging tools integrated into Visual Studio 2003. This combination provides developers the ability to build and test voice and multimodal applications on their local machine. Once the application is complete, Microsoft Speech Server provides the production-level support and scalability needed to run these types of applications. Personally, I don’t expect to be talking to HAL anytime soon. However, the possibilities are starting to get better. </p>
     </div>
     <div class="article-listings">
      <h1>Listing 1: Welcome to speech application</h1>
      <pre><code>&lt;%@ Page Language="vb" AutoEventWireup="false" Codebehind="Default.aspx.vb" Inherits="SpeechWebApplication3._Default"%&gt;
&lt;%@ Register TagPrefix="speech" Namespace="Microsoft.Speech.Web.UI" Assembly="Microsoft.Speech.Web, Version=1.0.3200.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35" %&gt;
&lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"&gt;
&lt;HTML&gt;
   &lt;HEAD&gt;
      &lt;title&gt;Default&lt;/title&gt;
      &lt;meta name="GENERATOR" content="Microsoft Visual Studio .NET 7.1"&gt;
      &lt;meta name="CODE_LANGUAGE" content="Visual Basic .NET 7.1"&gt;
      &lt;meta name="vs_defaultClientScript" content="JavaScript"&gt;
      &lt;meta name="vs_targetSchema" content="http://schemas.microsoft.com/intellisense/ie5"&gt;
   &lt;/HEAD&gt;
   &lt;body MS_POSITIONING="GridLayout" xmlns:speech="http://schemas.microsoft.com/speech/WebControls"&gt;
      &lt;form id="Form1" method="post" runat="server"&gt;
         &lt;script &gt;
         function PlayWelcomePrompt() {
            Prompt1.Start();
            }
         &lt;/script&gt;
         &lt;INPUT type="button" value="&lt;Play&gt;" onclick="PlayWelcomePrompt()"&gt;
         &lt;speech:Prompt id="Prompt1" runat="server"&gt;
            &lt;InlineContent&gt;Welcome to the world of speech!&lt;/InlineContent&gt;
         &lt;/speech:Prompt&gt;
      &lt;/form&gt;
   &lt;/body&gt;
&lt;/HTML&gt;

</code></pre>
      <h1>Listing 2: Markup representing a Listen control</h1>
      <pre><code>&lt;speech:listen id="AskDateListen"
      runat="server"
      AutoPostBack="true"
      OnReco="SetCalendar"
      EndSilence="1000"
      InitialTimeout="2000"
      MaxTimeout="15000"&gt;
      &lt;Grammars&gt;
         &lt;speech:Grammar Src="Grammars/DateGrammar.grxml" ID="Grammar2"&gt;&lt;/speech:Grammar&gt;
      &lt;/Grammars&gt;
&lt;/speech:listen&gt;

</code></pre>
      <h1>Listing 3: Asking questions and obtaining responses with the QA control</h1>
      <pre><code>&lt;speech:QA id="SubType" runat="server"&gt;
      
&lt;Prompt InlinePrompt="What type of submarine sandwich would you like?"/&gt;
      
&lt;Answers&gt;
 &lt;speech:Answer SemanticItem="sisubType" ID="subTypeAnswer" XpathTrigger="./subType"/&gt;
&lt;/Answers&gt;
      
&lt;Reco InitialTimeout="2000" BabbleTimeout="15000" EndSilence="1000" MaxTimeout="30000"&gt;
 &lt;Grammars&gt;
   &lt;speech:Grammar Src="subGrammar.grxml#TypeRule"/&gt;
 &lt;/Grammars&gt;
&lt;/Reco&gt;
      
&lt;/speech:QA&gt;

</code></pre>
      <h1>Listing 4: SRGS within a grammar file</h1>
      <pre><code>&lt;?xml version="1.0"?&gt;
&lt;grammar xml:lang="en-US" tag-format="semantics-ms/1.0" version="1.0" root="Root" mode="dtmf" xmlns="http://www.w3.org/2001/06/grammar" xmlns:sapi="http://schemas.microsoft.com/Speech/2002/06/SRGSExtensions"&gt;
   &lt;rule id="sandwich" scope="public"&gt;
      &lt;one-of&gt;
         &lt;item&gt;
            &lt;item&gt;ham&lt;/item&gt;
            &lt;tag&gt;$._value = "ham"&lt;/tag&gt;
         &lt;/item&gt;
         &lt;item&gt;
            &lt;item&gt;roast beef&lt;/item&gt;
            &lt;tag&gt;$._value = "roast beef"&lt;/tag&gt;
         &lt;/item&gt;
         &lt;item&gt;
            &lt;item&gt;italian&lt;/item&gt;
            &lt;tag&gt;$._value = "italian"&lt;/tag&gt;
         &lt;/item&gt;
      &lt;/one-of&gt;
   &lt;/rule&gt;
&lt;/grammar&gt;

</code></pre>
     </div>
     <div class="article-tables">
      <h1> Table 1: The standards of speech applications.</h1>
      <table>
       <tbody>
        <tr>
         <td>Speech Application Language Tags (SALT)</td>
         <td>SALT is an extension of HTML and other markup languages which adds a speech and telephony interface to Web applications. It supports both voice-only and multimodal browsers. SALT defines a small number of XML elements like the &lt;listen&gt; and &lt;prompt&gt; that serve as the core API for user interactions.</td>
        </tr>
        <tr>
         <td>Speech Recognition Grammar Specification (SRGS)</td>
         <td>SRGS provides a way to define the phrases that an application recognizes. This includes words that may be spoken, patterns that words may occur in, and the spoken language of each word.</td>
        </tr>
        <tr>
         <td>Speech Synthesis Markup Language (SSML)</td>
         <td>SSML provides an XML-based markup language for creating the synthetic speech within an application. It enables the control of synthetic speech that includes pronunciation, volume, pitch, and rate.</td>
        </tr>
       </tbody>
      </table>
      <h1> Table 2: The elements of the Grammar Editor toolbox.</h1>
      <table>
       <tbody>
        <tr>
         <th>Element</th>
         <th>Description</th>
        </tr>
        <tr>
         <td>Phrase</td>
         <td>The phrase element represents a single grammatical entry.</td>
        </tr>
        <tr>
         <td>List</td>
         <td>The list element specifies the relationship between a group of phrases.</td>
        </tr>
        <tr>
         <td>Group</td>
         <td>The group element binds a series of phrases together in a sequence.</td>
        </tr>
        <tr>
         <td>Rule Reference</td>
         <td>The rule reference element provides the ability to reference an external encapsulated rule.</td>
        </tr>
        <tr>
         <td>Script Tag</td>
         <td>The script tag element defines the set of valid phrases for this grammar.</td>
        </tr>
        <tr>
         <td>Wild Card</td>
         <td>The wild card element allows any part of a response to be ignored.</td>
        </tr>
        <tr>
         <td>Skip</td>
         <td>The skip element creates an optional group that can be used to insert or format semantic tags at key points in the grammar</td>
        </tr>
        <tr>
         <td>Halt</td>
         <td>The halt element immediately stops recognition when it is encountered.</td>
        </tr>
       </tbody>
      </table>
      <h1> Table 3: The typical components of a speech application.</h1>
      <table>
       <tbody>
        <tr>
         <th>File Type</th>
         <th>Description</th>
        </tr>
        <tr>
         <td>.aspx</td>
         <td>A file containing the visual elements of a Web Forms page.</td>
        </tr>
        <tr>
         <td>.ascx</td>
         <td>A file that persists a user control as a text file.</td>
        </tr>
        <tr>
         <td>.asax</td>
         <td>A file that handles application-level events.</td>
        </tr>
        <tr>
         <td>.ashx</td>
         <td>An ASP.NET Web Handler file used to manage raw HTTP requests.</td>
        </tr>
        <tr>
         <td>.grxml</td>
         <td>A grammar file.</td>
        </tr>
        <tr>
         <td>.cfg</td>
         <td>A binary file created by the SASDK command-line grammar compiler.</td>
        </tr>
        <tr>
         <td>.prompts</td>
         <td>A compiled prompt database that is created when a .promptdb file is compiled with the Speech Prompt Editor.</td>
        </tr>
       </tbody>
      </table>
     </div> 
     <div class="side-info2"> 
      <p class="author-image"><img src="/Article/AuthorPhotoSmall/063fa45e-547c-4274-a101-b8fc6a62dba1"> </p> 
      <h1 class="author-name">Robbins, Thom</h1> 
      <p>Thom Robbins a Developer Evangelist with Microsoft in New England. He is a frequent speaker at a variety of events including Dev Days and VS Live. Thom is also a regular contributor to various magazines including .Net Magazine and XML Web Service Journal. He spends his time working with developers in New England. When he’s not writing code he can be found with his wife, Denise, at their home in New Hampshire. You can reach Thom at trobbins@microsoft.com or through his blog at http://blogs.msdn.com/trobbins.</p> 
      <div> 
       <h2>This article was filed under:</h2> 
       <ul class="tech-list"> 
        <li><a href="/Magazine/ByCategory/ASP.NET%20WebForms">ASP.NET WebForms</a></li> 
        <li><a href="/Magazine/ByCategory/Speech-Enabled%20Applications">Speech-Enabled Applications</a></li> 
        <li><a href="/Magazine/ByCategory/Techniques">Techniques</a></li> 
       </ul> 
       <div class="clearfix"></div> 
      </div> 
      <div> 
       <h2>This article was published in:</h2> 
       <a href="/Magazine/Issue/f7ddaa17-649a-46d9-bb76-258d7f5e9a4e"><img src="/Magazine/Cover/f7ddaa17-649a-46d9-bb76-258d7f5e9a4e"></a> 
      </div> 
     </div> 
     <div style="padding-top: 25px;"> 
      <a href="https://www.codemag.com/subscribe/freearticle6b1?utm_source=articleviewer&amp;utm_medium=end_banner_d&amp;utm_campaign=Article-Viewer-Free-Subs"><img style="text-align: center;" src="/Images/Banners/Red End of Article Banner v2.png"></a> 
     </div> 
     <div style="padding-top: 25px;"> 
      <h4>Have additional technical questions?</h4> 
      <p style="margin-top: 10px;">Get help from the experts at <i>CODE Magazine</i> - sign up for our free hour of consulting!</p> 
      <p>Contact CODE Consulting at <a href="mailto:techhelp@codemag.com">techhelp@codemag.com</a>.</p> 
     </div> 
     <div> 
      <h4 style="margin-top: 50px; margin-bottom: 20px;">Got something to say? Leave a comment!</h4> 
      <div class="fb-comments" data-href="http://www.codemag.com/article/0511041" data-width="550"></div> 
     </div> 
    </div> 
    <div class="clearfix"></div> 
   </div> 
   <div id="footer"> 
    <div class="copyright">
     (c) by EPS Software Corp. 1993 - 2017
    </div> 
    <div class="vcard"> 
     <!-- EPS company address using adr MicroFormat standard --> 
     <div class="adr"> 
      <div class="street-address">
       6605 Cypresswood Dr.
      </div> 
      <div class="extended-address">
       Suite 300
      </div> 
      <span class="locality">Houston</span> 
      <span class="region">TX</span> 
      <span class="postal-code">77379</span> 
      <div class="country-name">
       USA
      </div> 
     </div> 
     <!-- Phone/fax in MicroFormat standard --> 
     <div class="tel">
      <div class="type">
       Voice
      </div>
      <div class="value">
       +1 (832) 717-4445
      </div>
     </div> 
     <div class="tel">
      <div class="type">
       Fax
      </div>
      <div class="value">
       +1 (832) 717-4460
      </div>
     </div> 
     <div class="email">
      Email: 
      <a class="email" href="mailto:info@codemag.com">info@codemag.com</a>
     </div> 
    </div> 
   </div> 
  </div> 
  <!-- the following div tags are there so custom CSS can associated generic elements, such as graphics anywhere in the body content --> 
  <div id="bodyAdorner1"> 
   <span></span> 
  </div> 
  <div id="bodyAdorner2"> 
   <span></span> 
  </div> 
  <div id="bodyAdorner3"> 
   <span></span> 
  </div> 
  <!-- OLD CODE - Wasn't working and Google helped me by adding the code below 8/1/16.  Google Code for Remarketing Tag --> 
  <!--
    Remarketing tags may not be associated with personally identifiable information or placed on pages related to sensitive categories. See more information and instructions on how to setup the tag on: http://google.com/ads/remarketingsetup
    ---------------------------------------------------> 
  <!-- Google Code for Remarketing Tag --> 
  <!--
    Remarketing tags may not be associated with personally identifiable information or placed on pages related to sensitive categories. See more information and instructions on how to setup the tag on: http://google.com/ads/remarketingsetup
    ---------------------------------------------------> 
  <script type="text/javascript">
    /* <![CDATA[ */
    var google_conversion_id = 1067389305;
    var google_custom_params = window.google_tag_params;
    var google_remarketing_only = true;
    /* ]]> */
        </script> 
  <script type="text/javascript" src="//www.googleadservices.com/pagead/conversion.js">
        </script> 
  <noscript> 
   <div style="display:inline;"> 
    <img height="1" width="1" style="border-style:none;" alt="" src="//googleads.g.doubleclick.net/pagead/viewthroughconversion/1067389305/?value=0&amp;guid=ON&amp;script=0"> 
   </div> 
  </noscript>   
 </body>
</html>