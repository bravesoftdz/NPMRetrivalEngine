<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
 <head> 
  <meta content="Pi Robot" name="AUTHOR"> 
  <meta name="description" content="Fully autonomous robot project"> 
  <meta name="keywords" content="robot, robots, robotics, robotic,
   robotics technology, computer vision, robotic vision, robot
   sensors, artificial intelligence, face recognition, face tracking,
   speech recognition, neural network, behavior, sensors, control
   algorithms, recurrent algorithm, robotics kits, robot parts, hobby
   robotics, autonmous robot, robotics projects"> 
  <link rel="stylesheet" href="/styles/robot.css" type="text/css"> 
  <title>Pi Robot</title> 
 </head> 
 <body> 
  <br> 
  <table width="1000" height="700" border="3" align="center" cellpadding="0"> 
   <!-- HEADER PANEL --> 
   <tbody>
    <tr id="header"> 
     <td height="75"><a href="/"><img src="/images/banner-1000.gif" border="0" alt="Pi Robot"></a></td> 
    </tr> 
    <!-- BREADCRUMBS PANEL
  <tr id="breadcrumbs">
    <td height="30">
          </td>
  </tr>
  --> 
    <!-- MAIN PANEL --> 
    <tr height="600" valign="top" id="main"> 
     <td> 
      <table width="1000"> 
       <tbody>
        <tr> 
         <!-- LEFT --> 
         <td height="600" width="125" valign="top" id="left"> 
          <table width="100%" border="0" cellpadding="0" id="navigation"> 
           <tbody>
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/">Home</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/wordpress">New Site</a></td>
            </tr> 
            <tr>
             <td><img src="/images/arrow-right-blue.gif" alt="" align="absmiddle" border="0">&nbsp;&nbsp;<a href="/blog">Blog</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/ros">ROS</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/code">Code</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/contests">Contests</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/hardware">Hardware</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/media">Media</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/photos">Photos</a></td>
            </tr> 
            <tr>
             <td><img src="/images/nav-arrow.gif" alt="" border="0">&nbsp;&nbsp;<a href="/videos">Videos</a></td>
            </tr> 
           </tbody>
          </table> 
          <center> 
           <hr width="80%"> 
           <a href="http://wiki.ros.org" target="_blank"><img src="/images/ros2.png" alt="" border="0"></a> 
           <br>
           <br>
           <br> 
           <a href="http://opencv.org" target="_blank"><img src="/images/openCV.png" alt="" border="0"></a> 
           <br>
           <br>
           <br> 
           <a href="http://www.roborealm.com" target="_blank"><img src="/images/roborealm_icon.gif" alt="" border="0"></a> 
           <br>
           <br>
           <br> 
           <a href="http://www.vanadiumlabs.com/" target="_blank"><img src="/images/vanadium.png" alt="" border="0"></a> 
          </center> <br><br><br> <script type="text/javascript"><!--
	   google_ad_client = "pub-7244882540172191";
	/* 120x600, created 6/25/09 */
	google_ad_slot = "4712455384";
	google_ad_width = 120;
	google_ad_height = 600;
	//-->
        </script> </td> 
         <!-- CENTER --> 
         <td height="625" width="755" valign="top" id="center">   
          <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type"><title>ROS by Example: Speech Recognition and TTS</title> 
          <meta content="Patrick Goebel" name="author"> 
          <meta http-equiv="CONTENT-TYPE" content="text/html; charset=utf-8"> 
          <meta name="GENERATOR" content="OpenOffice.org 3.2  (Linux)"> <style type="text/css">
p {
  margin-bottom: 0.08in;
}
.code {
  border: 1px solid #8cabc1;
  font-family: monospace,Courier,"Courier New";
  font-weight: normal;
  font-size: 14px;
  background-color: #f5f5f5;
  font-style: normal;
  padding-top: 10px;
  padding-left: 5px;
  padding-bottom: 10px;
  color: black;
}
.mono {
  font-family: monospace,Courier,"Courier New";
  font-weight: normal;
  font-size: 16px;
}
.package {
font-family: monospace,Courier,"Courier New";
font-weight: bold;
font-size: 16px;
}


  </style> <script type="text/javascript">
   function hide_show(block) {
      var visibility;
      var display;
      if (document.layers) {
	 visibility = document.layers[block].visibility;
	 if (visibility == 'show') {
	    visibility = 'hide';
	    display = 'none';
         }
	 else {
	    visibility = 'show';
	    display = 'block';
         }
	 document.layers[block].visibility = visibility;
         document.layers[block].display = 'block';
      }
      else if (document.all) {
         visibility = document.all[block].style.visibility;
	 if (visibility == 'none') {
	    visibility = 'hidden';
	    display = 'none';
         }
	 else {
	    visibility = 'none';
	    display = 'block';
         }
         document.all[block].style.display = display;
	 document.all[block].style.visibility = visibility;
      }
      else if (document.getElementById) {
         visibility = document.getElementById(block).style.visibility;
	 if (visibility == 'visible' || visibility == '') {
	    visibility = 'hidden';
	    display = 'none';
         }
	 else {
	    visibility = 'visible';
	    display = 'block';
         }
	 document.getElementById(block).style.visibility = visibility;
         document.getElementById(block).style.display = display;
     }
   }
  </script> <style type="text/css">
span.default {
  background: #ffffff none repeat scroll 0%;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  color: #000000;
  font-size: 10pt;
  text-decoration: none;
}
span.default a {
  background: #ffffff none repeat scroll 0%;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  color: #000000;
  font-size: 10pt;
  text-decoration: underline;
}
span.builtin {
  background: #ffffff none repeat scroll 0%;
  color: #da70d6;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: none;
}
span.builtin a {
  background: #ffffff none repeat scroll 0%;
  color: #da70d6;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: underline;
}
span.function-name {
  background: #ffffff none repeat scroll 0%;
  color: #0000ff;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: none;
}
span.function-name a {
  background: #ffffff none repeat scroll 0%;
  color: #0000ff;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: underline;
}
span.type {
  background: #ffffff none repeat scroll 0%;
  color: #228b22;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: none;
}
span.type a {
  background: #ffffff none repeat scroll 0%;
  color: #228b22;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: underline;
}
span.string {
  background: #ffffff none repeat scroll 0%;
  color: #bc8f8f;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: none;
}
span.string a {
  background: #ffffff none repeat scroll 0%;
  color: #bc8f8f;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: underline;
}
span.keyword {
  background: #ffffff none repeat scroll 0%;
  color: #a020f0;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: none;
}
span.keyword a {
  background: #ffffff none repeat scroll 0% 50%;
  font-stretch: normal;
  font-size: 10pt;
  font-weight: 500;
  font-style: normal;
  text-decoration: underline;
}
span.comment {
  background: #ffffff none repeat scroll 0%;
  color: #b22222;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: none;
}
span.comment a {
  background: #ffffff none repeat scroll 0%;
  color: #b22222;
  font-family: DejaVu Sans Mono;
  font-stretch: normal;
  font-weight: 500;
  font-style: normal;
  font-size: 10pt;
  text-decoration: underline;
}

  </style> <style type="text/css">
.comment {
  color: #cd0000;
}
.function-name {
  color: #0000ee;
  font-weight: bold;
}
.keyword {
  color: #00cdcd;
  font-weight: bold;
}
.string {
  color: #00cd00;
}
.type {
  color: #00cd00;
}
a {
  font-size-adjust: inherit;
  background-color: inherit;
  font-weight: inherit;
  font-style: inherit;
  line-height: inherit;
  font-variant: inherit;
  color: blue;
  font-family: inherit;
  font-size: inherit;
  font-stretch: inherit;
  text-decoration: underline;
}
a:hover {
  text-decoration: underline;
}

  </style>
          <meta http-equiv="CONTENT-TYPE" content="text/html; charset=utf-8">
          <meta name="GENERATOR" content="OpenOffice.org 3.2  (Linux)"><style type="text/css">
	<!--
		@page { margin: 0.79in }
		H2 { margin-bottom: 0.08in }
		H2.western { font-family: "Times New Roman", serif; font-size: 14pt }
		H2.cjk { font-family: "DejaVu Sans"; font-size: 16pt }
		H2.ctl { font-family: "DejaVu Sans"; font-size: 12pt; font-weight: normal }
		P { margin-bottom: 0.08in }
		A:link { so-language: zxx }
	--></style> <h2>ROS by Example: Speech Recognition and Text-to-Speech (TTS)</h2> 
          <div style="margin: 15px 0px 10px 10px; float: right;">
           <iframe src="http://www.youtube.com/embed/10ysYZUX_jA" allowfullscreen frameborder="0" height="315" width="420">&amp;amp;amp;amp;amp;amp;amp;lt;br&amp;amp;amp;amp;amp;amp;amp;gt; </iframe>
          </div> <span style="font-weight: bold;">NOTE</span>: This tutorial is several years out of date and the specific commands for installing software and using ROS have changed since then.&nbsp; An up-to-date version of this tutorial can be found in the book <span style="font-style: italic; font-weight: bold;">ROS By Example: A Do-It-Yourself Guide to the Robot Operating System</span>, available as a downloadable PDF and in paperback on <a href="http://www.lulu.com/spotlight/pirobot">Lulu.com</a>.<br> <br> Speech recognition and Linux have come a long way in the past few years, thanks mostly to the <a href="http://cmusphinx.sourceforge.net/" target="_blank">CMU Sphinx</a> and <a href="http://www.speech.cs.cmu.edu/festival/" target="_blank">Festival</a> projects.&nbsp; There are also ready-made ROS packages for both speech recognition and text-to-speech.&nbsp; Consequently, it is quite easy to add speech control and voice feedback to your robot as we will now show.<br> <br> In this tutorial we will:<br> 
          <ul> 
           <li>Install and test the&nbsp;<a href="http://www.ros.org/wiki/pocketsphinx" target="_blank"><span class="mono">pocketsphinx</span></a> package for speech recognition</li> 
           <li>Learn how to create a custom vocabulary for speech recognition</li> 
           <li>Teleoperate a TurtleBot using voice commands</li> 
           <li>Install and test the Festival text-to-speech system and ROS&nbsp;<a href="http://www.ros.org/wiki/sound_play" target="_blank"><span class="mono">sound_play</span></a> package</li> 
          </ul> <h3>System Requirements</h3> This tutorial has been tested using <a href="http://www.ros.org/wiki/electric/Installation/Ubuntu" target="_blank">ROS Electric</a> and Ubuntu 10.04.&nbsp; There is also a report that it works using ROS Diamondback and Ubuntu 10.10.<br> <h3>Installing PocketSphinx for Speech Recognition</h3> Thanks to Michael Ferguson from the University of Albany (and now at Willow Garage), we can use the ROS&nbsp;<a href="http://www.ros.org/wiki/pocketsphinx" target="_blank"><span class="mono">pocketsphinx</span></a> package for speech recognition.&nbsp; The&nbsp;<span class="mono">pocketsphinx</span> package requires the installation of the Ubuntu package <span class="package">gstreamer0.10-pocketsphinx</span> and we will also need the ROS sound drivers stack (in case you don't already have it) so so let's take care of both first.&nbsp; You will be prompted to install the Festival packages if you don't already have them--answer "Y" of course:<p class="code">$ sudo apt-get install gstreamer0.10-pocketsphinx<br> $ sudo apt-get install ros-electric-sound-drivers </p>The <span class="mono">pocketsphinx</span> package is part of the University of Albany's&nbsp;<span class="mono">rharmony</span> stack so let's install the stack using the following commands.<span class="mono"> </span>First move into your personal ROS path (e.g.&nbsp;<span class="mono">~/ros</span>), then run:<br> <p class="code">$ svn checkout http://albany-ros-pkg.googlecode.com/svn/trunk/rharmony<br> $ rosmake --rosdep-install pocketsphinx<br> </p> The key file in the&nbsp;<span class="mono">pocketsphinx</span> package is the Python script&nbsp;<span class="package">recognizer.py</span> found in the&nbsp;<span class="package">nodes</span> subdirectory.&nbsp; This script does all the hard work of connecting to the audio input stream of your computer and matching voice commands to the words or phrases in the current vocabulary.&nbsp; When the recognizer node matches a word or phrase, it publishes it on the&nbsp;<span class="package">/recognizer/output</span> topic.&nbsp; Other nodes can subscribe to this topic to find out what the user has just said.<br> <h3>Downloading the Tutorial Files<br> </h3> All the files needed for the tutorial can be downloaded via SVN.&nbsp; Move into your personal ROS path (e.g.&nbsp;<span class="mono">~/ros</span>) and run:<br> <p class="code">$ svn checkout http://pi-robot-ros-pkg.googlecode.com/svn/trunk/pi_tutorials/pi_speech_tutorial<br> $ rosmake --rosdep-install pi_speech_tutorial<br> </p> <h3>Testing the PocketSphinx Recognizer</h3> You will get the best speech recognition results using a headset microphone, either USB, standard audio or Bluetooth.&nbsp; Once you have your microphone connected to your computer, make sure it is selected as the input audio device by going to the Ubuntu&nbsp;<span class="package">System</span> menu, then selecting&nbsp;<span class="package">Preferences</span>-&gt;<span class="package">Sound</span>.&nbsp; Once the Sound Preferences window opens, click on the&nbsp;<span class="package">Input</span> tab and select your microphone device from the list (if more than one).&nbsp; Speak a few words into your microphone and you should see the volume meter respond.&nbsp; Then click on the <span class="package">Output</span> tab and select your desired output device as well as adjust the volume slider.&nbsp; Now close the Sound Preferences window.<br> <br> <span style="font-weight: bold;">NOTE</span>:&nbsp; If you disconnect a USB or Bluetooth microphone and then reconnect it later, you will likely have to select it as the input again using the procedure described above.<br> <br> Michael Ferguson includes a vocabulary file suitable for RoboCup@Home competitions that you can use to the test the recognizer.&nbsp; Fire it up now by running:<br> <p class="code">$ roslaunch pocketsphinx robocup.launch<br> </p> You should see a list of INFO messages indicating that the various parts of the recognition model are being loaded.&nbsp; The last few messages will look something like this:<br> <p class="code">INFO: ngram_search_fwdtree.c(195): Creating search tree<br> INFO: ngram_search_fwdtree.c(203): 0 root, 0 non-root channels, 26 single-phone words<br> INFO: ngram_search_fwdtree.c(325): max nonroot chan increased to 328<br> INFO: ngram_search_fwdtree.c(334): 77 root, 200 non-root channels, 6 single-phone words<br> <br> </p> Now say a few of the RoboCup phrases such as "bring me the glass", "go to the kitchen", or "come with me".&nbsp; The output should look something like this:<br> <p class="code"> Partial: BRING<br> Partial: BRING ME<br> Partial: BRING IS<br> Partial: BRING ME THE<br> Partial: BRING ME THE GO<br> Partial: BRING ME THE THE<br> Partial: BRING ME THE GLASS<br> [INFO] [WallTime: 1318719668.724552] bring me the glass<br> Partial: THE<br> Partial: GO<br> Partial: GOOD<br> Partial: GO TO<br> Partial: GOOD IS<br> Partial: GO TO THE<br> Partial: GO TO THE TO<br> Partial: GO TO THE GET<br> Partial: GO TO THE KITCHEN<br> [INFO] [WallTime: 1318719670.184438] go to the kitchen<br> Partial: GO<br> Partial: COME<br> Partial: COME WITH<br> Partial: COME WITH THE<br> Partial: COME WITH ME<br> [INFO] [WallTime: 1318719671.835016] come with me<br> </p> Congratulations&#x2014;you can now talk to your robot!&nbsp;&nbsp; Here we see how the PocketSphinx recognizer builds the recognized phrase over the course of your utterance.&nbsp; To see just the final result, open another terminal, and echo the&nbsp;<span class="mono">/recognizer/output</span> topic:<br> <p class="code">$ rostopic echo /recognizer/output<br> </p> Now try the same three phrases as above and you should see:<br> <p class="code">data: bring me the glass<br> ---<br> data: go to the kitchen<br> ---<br> data: come with me<br> ---<br> </p> For my voice, and using a Bluetooth over-the-ear microphone, the recognizer was surprisingly fast and accurate.<br> <br> To see all the phrases you can use with the demo RoboCup vocabulary, run the following commands:<br> <p class="code">$ roscd pocketsphinx/demo<br> $ more robocup.corpus<br> </p> Now try saying a phrase that is not in the vocabulary, such as "the sky is blue".&nbsp; In my case, the result on the&nbsp;<span class="mono">/recognizer/output</span> topic was "this go is room".&nbsp; As you can see, the recognizer will respond with something no matter what you say.&nbsp; This means that care must be taken to "mute" the speech recognizer if we don't want random conversation to be interpreted as speech commands.&nbsp; We will see how to do this below when we learn how to map speech recognition into actions.<br> <h3>Creating A Vocabulary</h3> It is easy to create a new vocabulary or <span style="font-weight: bold; font-style: italic;">corpus</span> as it is referred to in PocketSphinx.&nbsp;&nbsp; First, create a simple text file with one word or phrase per line.&nbsp; Here is a corpus that could be used to drive your robot around using voice commands.&nbsp; We will store it in a file called&nbsp;<span class="mono">nav_commands.txt</span> in the&nbsp;<span class="mono">config</span> subdirectory of the&nbsp;<span class="mono">pi_speech_tutorial</span> package:<br> <p class="code">$ roscd pi_speech_tutorial/config<br> $ more nav_commands.txt<br> </p> You should see the following list of phrases:<br> <p class="code">pause speech<br> continue speech<br> move forward<br> move backward<br> move back<br> move left<br> move right<br> go forward<br> go backward<br> go back<br> go left<br> go right<br> go straight<br> come forward<br> come backward<br> come left<br> come right<br> turn left<br> turn right<br> rotate left<br> rotate right<br> faster<br> speed up<br> slower<br> slow down<br> quarter speed<br> half speed<br> full speed<br> stop<br> stop now<br> halt<br> abort<br> kill<br> panic<br> help<br> help me<br> freeze<br> turn off<br> shut down<br> cancel<br> </p> Feel free to add, delete or change some of these words or phrases before proceeding to the next step.<br> <br> When you enter your phrases, try not to mix upper and lower case and do not use punctuation marks.&nbsp; Also, if want to include a number such as 54, spell it out as "fifty four".<br> <br> Before we can use this corpus with PocketSphinx, we need to compile it into special dictionary and pronunciation files.&nbsp; This can be done using the online CMU language model (lm) tool located at:<br> <br> <a href="http://www.speech.cs.cmu.edu/tools/lmtool-new.html" target="_blank">http://www.speech.cs.cmu.edu/tools/lmtool-new.html</a><br> <br> Follow the directions to upload your&nbsp;<span class="mono">nav_commands.txt</span> file, click the "Compile Knowledge Base" button, then download the resulting compressed tarball that contains all the language model files.&nbsp; Extract these files into the <span class="mono">config</span> subdirectory of the&nbsp;<span class="mono">pi_speech_tutorial</span> package.&nbsp; The files will all begin with the same number, such as&nbsp;<span class="mono">3026.dic</span>,&nbsp;<span class="mono">3026.lm</span>, etc.&nbsp; These files define your vocabulary as a language model that PocketSphinx can understand.&nbsp; You can rename all these files to something more memorable using a command like the following (the 4-digit number will likely be different in your case):<br> <p class="code">$ roscd pi_speech_tutorial/config<br> $ rename -f 's/3026/nav_commands/' * </p> Next, take a look at the <span class="mono">voice_nav_commands.launch</span> file found in the&nbsp;<span class="mono">launch</span> subdirectory of the&nbsp;<span class="mono">pi_speech_tutorial</span> package.&nbsp; It looks like this:<br> <p class="code">&lt;launch&gt;<br> &nbsp; &lt;node name="recognizer" pkg="pocketsphinx" type="recognizer.py" output="screen"&gt;<br> &nbsp;&nbsp;&nbsp; &lt;param name="lm" value="$(find pi_speech_tutorial)/config/nav_commands.lm"/&gt;<br> &nbsp;&nbsp;&nbsp; &lt;param name="dict" value="$(find pi_speech_tutorial)/config/nav_commands.dic"/&gt;<br> &nbsp; &lt;/node&gt;<br> &lt;/launch&gt;<br> </p> As you can see, we launch the&nbsp;<span class="mono">recognizer.py</span> node in the&nbsp;<span class="mono">pocketsphinx</span> package and we point the <span class="mono">lm</span> and&nbsp;<span class="mono">dict</span> parameters to the files <span class="mono">nav_commands.lm</span> and&nbsp;<span class="mono">nav_commands.dic</span> created in the steps above.&nbsp; Note also that the parameter&nbsp;<span class="mono">output="screen"</span> is what allows us to see the real-time recognition results in the launch window.<br> <br> Launch this file and test speech recognition by monitoring the <span class="mono">/recognizer/output<span class="mono"></span><span class="mono"></span> </span>topic:<br> <p class="code">$ roslaunch pi_speech_tutorial voice_nav_commands.launch </p> And in a separate terminal:<br> <p class="code">$ rostopic echo /recognizer/output<br> </p> Try saying a few navigation phrases such as "move forward", "slow down" and "stop".&nbsp; You should see your commands echoed on the&nbsp;<span class="mono">/recognizer/output</span> topic. <br> <h3>Voice Controlling Your Robot<br> </h3> The <span class="mono"></span><span class="mono">recognizer.py</span> node in the&nbsp;<span class="mono">pocketsphinx</span> package <span class="mono"></span>publishes recognized speech commands to the&nbsp;<span class="mono">/recognizer/output</span> topic.&nbsp; To map these commands to robot actions, we need a second node that subscribes to this topic, looks for appropriate messages, then causes the robot to execute different behaviors depending on the message received.&nbsp; To get us started, Michael Ferguson includes a Python script called&nbsp;<span class="mono">voice_cmd_vel.py</span> in the&nbsp;<span class="mono">pocketsphinx</span> package that maps voice commands into&nbsp;<span class="mono">Twist</span> messages that can be used to control a mobile robot.&nbsp; We will use a slightly modified version of this script called&nbsp;<span class="mono">voice_nav.py</span> found in the&nbsp;<span class="mono">nodes</span> subdirectory of the&nbsp;<span class="mono">pi_speech_tutorial</span> package.<br> <br>The only key difference between the two scripts is the following block in&nbsp;<span class="mono">voice_nav.py</span>:<br> <p class="code"># A mapping from keywords to commands.<br> self.keywords_to_command = {'stop': ['stop', 'halt', 'abort', 'kill', 'panic', 'off', 'freeze', 'shut down', 'help'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'slower': ['slow down', 'slower'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'faster': ['speed up', 'faster'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'forward': ['forward', 'ahead', 'straight'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'backward': ['back', 'backward', 'back up'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'rotate left': ['rotate left'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'rotate right': ['rotate right'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'turn left': ['turn left'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'turn right': ['turn right'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'quarter': ['quarter speed'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'half': ['half speed'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'full': ['full speed'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'pause': ['pause speech'],<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'continue': ['continue speech']}<br> </p> The&nbsp;<span class="mono">keywords_to_command</span> dictionary allows us to map different verbal commands into the same action.&nbsp; For example, it is really important to be able to stop the robot once it is moving.&nbsp; However, the word "stop" is not always recognized by the PocketSphinx recognizer.&nbsp; So we provide a number of alternative ways of telling the robot to stop like "halt", "abort", "help", etc.&nbsp; Of course, these alternatives must be included in our original PocketSphinx vocabulary (corpus).<br> <br> The&nbsp;<span class="mono">voice_nav.py</span> node subscribes to the&nbsp;<span class="mono">/recognizer/output</span> topic and looks for recognized keywords as specified in the&nbsp;<span class="mono">nav_commands.txt</span> corpus.&nbsp; If a match is found, the&nbsp;<span class="mono">keywords_to_commands</span> dictionary maps the matched phrase to an appropriate command word.&nbsp; Our callback function then maps the command word to the appropriate&nbsp;<span class="mono">Twist</span> action sent to the robot.&nbsp; You can look at the&nbsp;<span class="mono">voice_nav.py</span> script for details.<br> <br> Another feature of the&nbsp;<span class="mono">voice_nav.py</span> script is that it will respond to the two special commands "<span style="font-weight: bold;">pause speech</span>" and "<span style="font-weight: bold;">continue speech</span>".&nbsp; If you are voice controlling your robot, but you would like to say something to another person without the robot interpreting your words as movement commands, just say "pause speech".&nbsp; When you want to continue controlling the robot, say "continue speech".<br> <br> To voice control a TurtleBot, move the robot into an open space free of obstacles, then bring up at least the&nbsp;<span class="mono">minimal.launch</span> file on the TurtleBot.&nbsp; On your workstation computer, run the&nbsp;<span class="mono">voice_nav_commands.launch</span> and <span class="mono">turtlebot_voice_nav.launch</span> files:<br> <p class="code">$ roslaunch pi_speech_tutorial voice_nav_commands.launch<br> </p> and in another terminal:<br> <p class="code">$ roslaunch pi_speech_tutorial turtlebot_voice_nav.launch<br> </p> Try a relatively safe voice command first such as "rotate right".&nbsp;&nbsp; Refer to the list of commands above for different ways you can move the robot.&nbsp; The <span class="mono">turtlebot_voice_nav.launch</span> file ncludes parameters you can set that determine the maximum speed of the TurtleBot as well as the increments used when you say "go faster" or "slow down".<br> <br> <h3>Installing and Testing Festival Text-to-Speech</h3> Now that we can talk to our robot, it would be nice if it could talk back to us.&nbsp; Text-to-speech (TTS) is accomplished using the CMU Festival system together with the ROS&nbsp;<a href="http://www.ros.org/wiki/sound_play" target="_blank"><span class="package">sound_play</span></a> package.&nbsp; If you have followed this tutorial from the beginning, you have already done the following step.&nbsp; Otherwise, run it now. You will be prompted to install the Festival packages if you don't already have them--answer "Y" of course:<br> <p class="code">$ sudo apt-get install ros-electric-sound-drivers<br> </p> The <span class="mono">sound_play </span>package uses the CMU Festival TTS library to generate synthetic speech.&nbsp; Let's test it out with the default voice as follows.&nbsp; First fire up the primary <span class="mono">sound_play </span>node:<br> <p class="code"> $ rosrun sound_play soundplay_node.py </p> In another terminal, enter some text to be converted to voice:<br> <p class="code"> $ rosrun sound_play say.py "Greetings Humans. Take me to your leader." </p> The default voice is called&nbsp;<span class="mono">kal_diphone</span>.&nbsp;&nbsp; To see all the English voices currently installed on your system:<br> <p class="code"> $ ls /usr/share/festival/voices/english </p> To get a list of all basic Festival voices available, run the following command:<br> <p class="code"> $ sudo apt-cache search --names-only festvox-* </p> To install the&nbsp;<span class="mono">festvox-don</span> voice (for example), run the command:<br> <p class="code"> $ sudo apt-get install festvox-don </p> And to test out your new voice, add the voice name to the end of the command line like this:<br> <p class="code"> $ rosrun sound_play say.py "Welcome to the future" voice_don_diphone </p> <br> There aren't a huge number of voices to choose from, but a few additional voices can be installed <a href="http://ubuntuforums.org/showthread.php?t=677277" target="_blank">as described here</a> and <a href="http://festvox.org/voicedemos.html" target="_blank">demonstrated here</a>. &nbsp; Here are the steps to get and use two of those voices, one male and one female:<br> <p class="code"> $ sudo apt-get install festlex-cmu<br> $ cd /usr/share/festival/voices/english/<br> $ sudo wget -c http://www.speech.cs.cmu.edu/cmu_arctic/packed/cmu_us_clb_arctic-0.95-release.tar.bz2<br> $ sudo wget -c http://www.speech.cs.cmu.edu/cmu_arctic/packed/cmu_us_bdl_arctic-0.95-release.tar.bz2<br> $ sudo tar jxf cmu_us_clb_arctic-0.95-release.tar.bz2 <br> $ sudo tar jxf cmu_us_bdl_arctic-0.95-release.tar.bz2<br> $ sudo rm cmu_us_clb_arctic-0.95-release.tar.bz2<br> $ sudo rm cmu_us_bdl_arctic-0.95-release.tar.bz2<br> $ sudo ln -s cmu_us_clb_arctic cmu_us_clb_arctic_clunits<br> $ sudo ln -s cmu_us_bdl_arctic cmu_us_bdl_arctic_clunits<br> </p> You can test these two voices like this:<br> <p class="code"> $ rosrun sound_play say.py "I am speaking with a female C M U voice" voice_cmu_us_clb_arctic_clunits<br> $ rosrun sound_play say.py "I am speaking with a male C M U voice" voice_cmu_us_bdl_arctic_clunits<br> </p> <span style="font-weight: bold;">NOTE</span>: If you don't hear the phrase on the first try, try repeating the command.&nbsp; Also, remember that a sound_play node must already be running in another terminal.<br> <br> You can also use sound_play to play wave files or a number of built-in sounds.&nbsp; To play the R2D2 wave file in the pi_speech_tutorial sounds directory, use the command:<br> <p class="code"> $ rosrun sound_play play.py `rospack find pi_speech_tutorial`/sounds/R2D2a.wav<br> </p> Note that the&nbsp;<span class="mono">play.py</span> script requires the absolute path to the wave file which is why we used 'rospack find'.&nbsp; You could also just type out the full path name.<br> <br> To hear one of the built-in sounds, use the&nbsp;<span class="mono">playbuiltin.py</span> script together with a number from 1 to 5; for example:<br> <p class="code"> $ rosrun sound_play playbuiltin.py 4<br> </p> <h3>Using Text-to-Speech within a ROS Node</h3> So far we have only used the Festival voices from the command line.&nbsp; To see how to use text-to-speech from within a ROS node, the following <span class="mono">talkback.py</span> script can be found in the&nbsp;<span class="mono">nodes</span> directory in <span class="mono">pi_speech_tutorial</span>.&nbsp; Note that to use such a script, the primary&nbsp;<span class="mono">sound_play</span> node must already be running:<br> <p class="code">#!/usr/bin/env python<br> <br> import roslib; roslib.load_manifest('pi_speech_tutorial')<br> import rospy<br> from std_msgs.msg import String<br> <br> <span style="background-color: rgb(255, 255, 153);">from sound_play.libsoundplay import SoundClient</span><br> <br> class TalkBack:<br> &nbsp;&nbsp;&nbsp; def __init__(self):<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.on_shutdown(self.cleanup)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.voice = rospy.get_param("~voice", "voice_don_diphone")<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.wavepath = rospy.get_param("~wavepath", "")<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Create the sound client object<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color: rgb(255, 255, 153);">self.soundhandle = SoundClient()</span><br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.sleep(1)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.soundhandle.stopAll()<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Announce that we are ready for input<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color: rgb(255, 255, 153);">self.soundhandle.playWave(self.wavepath + "/R2D2a.wav")</span><br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.sleep(1)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color: rgb(255, 255, 153);">self.soundhandle.say("Ready", self.voice)</span><br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.loginfo("Say one of the navigation commands...")<br> <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Subscribe to the recognizer output<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.Subscriber('/recognizer/output', String, self.talkback)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp; def talkback(self, msg):<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Print the recognized words on the screen<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.loginfo(msg.data)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Speak the recognized words in the selected voice<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color: rgb(255, 255, 153);">self.soundhandle.say(msg.data, self.voice)</span><br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Uncomment to play one of the built-in sounds<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #rospy.sleep(2)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color: rgb(255, 255, 153);">#self.soundhandle.play(5)</span><br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Uncomment to play a wave file<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #rospy.sleep(2)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color: rgb(255, 255, 153);">#self.soundhandle.playWave(self.wavepath + "/R2D2a.wav")</span><br> <br> &nbsp;&nbsp;&nbsp; def cleanup(self):<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.loginfo("Shutting down talkback node...")<br> <br> if __name__=="__main__":<br> &nbsp;&nbsp;&nbsp; rospy.init_node('talkback')<br> &nbsp;&nbsp;&nbsp; try:<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TalkBack()<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rospy.spin()<br> &nbsp;&nbsp;&nbsp; except:<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass<br> </p> <br> The key lines are highlighted in yellow.&nbsp; First we import the&nbsp;<span class="mono">SoundClient</span> class from the sound_play library.&nbsp; Then we assign a&nbsp;<span class="mono">SoundClient</span> object to&nbsp;<span class="mono">self.soundhandle</span> that we can use throughout the script.&nbsp; The three sound_play functions we use are&nbsp;<span class="mono">playWave()</span> to play a wave file,&nbsp;<span class="mono">say()</span> to voice some text and <span class="mono">play()</span> to play one of the builtin sounds.&nbsp; For the complete API, take a look at this <a href="http://www.ros.org/doc/api/sound_play/html/libsoundplay_8py_source.html" target="_blank">ROS wiki page</a>.<br> <br> You can test the script using the <span class="mono">talkback.launch </span>file.&nbsp; Note how the launch file first brings up a&nbsp;<span class="mono">sound_play</span> node before launching the&nbsp;<span class="mono">talkback.py</span> script:<br> <p class="code"> $ roslaunch pi_speech_tutorial talkback.launch <br> </p> <br> You should now be able to write your own script that combines speech recognition and text-to-speech.&nbsp; For example, see if you can figure out how to ask your robot the date and time and get back the answer from the system clock. :-)<br> <br> <br> <br> <br>  </td> 
         <!-- RIGHT PANEL --> 
         <td width="120" valign="top" id="right"><br> <script type="text/javascript"><!--
	   google_ad_client = "pub-7244882540172191";
	/* 120x600, created 6/25/09 */
	google_ad_slot = "4712455384";
	google_ad_width = 120;
	google_ad_height = 600;
	//-->
</script> <script type="text/javascript" src="http://pagead2.googlesyndication.com/pagead/show_ads.js">
</script> </td> 
        </tr> 
       </tbody>
      </table> </td> 
     <!-- FOOTER PANEL --> 
    </tr> 
    <tr> 
     <td> 
      <table width="1000" height="50" border="0"> 
       <tbody>
        <tr> 
         <td id="footer" align="middle">Copyright © 2006-2017 by <a href="mailto:patrick at pirobot.org">Patrick Goebel</a> </td> 
        </tr> 
       </tbody>
      </table> </td>
    </tr> 
   </tbody>
  </table> 
  <script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script> 
  <script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-2938594-3");
pageTracker._trackPageview();
} catch(err) {}</script>   
 </body>
</html>