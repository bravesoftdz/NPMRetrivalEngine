<html>
 <head>
  <title>Java Speech API Programmer's Guide: Speech Recognition</title>
 </head> 
 <!--Intermediate split file from a FrameMaker file--> 
 <body bgcolor="#FFFFFF" text="#000000" link="#990000" alink="#333399" vlink="#333399"> 
  <!--#include virtual="/products/java-media/include/speechGlobal.html"--> 
  <table align="center" cellspacing="20" cellpadding="6" bgcolor="#f0f0ff"> 
   <tbody>
    <tr> 
     <td><code> <a href="index.html">Contents</a> </code> </td>
     <td><code> <a href="Synthesis.html">Previous</a> </code> </td>
     <td><code> &nbsp;&nbsp;<a href="">Next</a>&nbsp;&nbsp; </code> </td>
    </tr>
   </tbody>
  </table>  
  <blockquote> 
   <a name="5529"></a>
   <p align="right"> <font size="+4"> Chapter 6 </font> </p>
   <a name="7408"></a>
   <hr size="7" noshade>
   <h1 align="right"><font size="+4"> Speech Recognition: javax.speech.recognition <br>&nbsp; </font></h1> 
   <p><a name="31012"></a>A speech recognizer is a speech engine that converts speech to text. The <code>javax.speech.recognition</code> package defines the <code>Recognizer</code> interface to support speech recognition plus a set of supporting classes and interfaces. The basic functional capabilities of speech recognizers, some of the uses of speech recognition and some of the limitations of speech recognizers are described in <a href="TechOverview.html#7988">Section 2.2</a>. </p>
   <p><a name="31019"></a>As a type of speech engine, much of the functionality of a <code>Recognizer</code> is inherited from the <code>Engine</code> interface in the <code>javax.speech</code> package and from other classes and interfaces in that package. The <code>javax.speech</code> package and generic speech engine functionality are described in <a href="SpeechEngine.html#">Chapter 4</a>. </p>
   <p><a name="31557"></a>The Java Speech API is designed to keep simple speech applications simple Þ and to make advanced speech applications possible for non-specialist developers. This chapter covers both the simple and advanced capabilities of the <code>javax.speech.recognition</code> package. Where appropriate, some of the more advanced sections are marked so that you can choose to skip them. We begin with a simple code example, and then review the speech recognition capabilities of the API in more detail through the following sections: </p>
   <ul>
    <li><a name="12424"></a><em><a href="Recognition.html#11644">"Hello World!"</a>:</em> a simple example of speech recognition </li>
   </ul> 
   <ul>
    <li><a name="34914"></a><a href="Recognition.html#31145">Recognizer as an Engine</a> </li>
   </ul> 
   <ul>
    <li><a name="34921"></a><a href="Recognition.html#32697">Recognizer State Systems</a> </li>
   </ul> 
   <ul>
    <li><a name="34928"></a><a href="Recognition.html#12452">Recognition Grammars</a> </li>
   </ul> 
   <ul>
    <li><a name="34935"></a><a href="Recognition.html#38507">Rule Grammars</a> </li>
   </ul> 
   <ul>
    <li><a name="34942"></a><a href="Recognition.html#12454">Dictation Grammars</a> </li>
   </ul> 
   <ul>
    <li><a name="34953"></a><a href="Recognition.html#20505">Recognition Results</a> </li>
   </ul> 
   <ul>
    <li><a name="34960"></a><a href="Recognition.html#15438">Recognizer Properties</a> </li>
   </ul> 
   <ul>
    <li><a name="34967"></a><a href="Recognition.html#13004">Speaker Management</a> </li>
   </ul> 
   <ul>
    <li><a name="34974"></a><a href="Recognition.html#38507">Recognizer Audio</a> </li>
   </ul> 
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="11644"></a>
   <h2>6.1 &nbsp; &nbsp; "Hello World!" </h2> 
   <p><a name="12443"></a>The following example shows a simple application that uses speech recognition. For this application we need to define a <em>grammar</em> of everything the user can say, and we need to write the Java software that performs the recognition task. </p>
   <p><a name="31166"></a>A grammar is provided by an application to a speech recognizer to define the words that a user can say, and the patterns in which those words can be spoken. In this example, we define a grammar that allows a user to say "Hello World" or a variant. The grammar is defined using the Java Speech Grammar Format. This format is documented in the <em>Java Speech Grammar Format Specification</em>. </p>
   <p><a name="30410"></a>Place this grammar into a file. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="15788"></a>grammar javax.speech.demo;
<a name="15789"></a>
<a name="31175"></a>public &lt;sentence&gt; = hello world | good morning | 
<a name="31176"></a>                                      hello mighty computer;


      <hr></pre>
    </dd>
   </dl>
   <p><a name="31177"></a>This trivial grammar has a single <em>public rule</em> called "<code>sentence</code>". A rule defines what may be spoken by a user. A public rule is one that may be <em>activated</em> for recognition. </p>
   <p><a name="31180"></a>The following code shows how to create a recognizer, load the grammar, and then wait for the user to say something that matches the grammar. When it gets a match, it deallocates the engine and exits. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="15722"></a>import javax.speech.*;
<a name="24807"></a>import javax.speech.recognition.*;
<a name="24808"></a>import java.io.FileReader;
<a name="24809"></a>import java.util.Locale;
<a name="24810"></a>
<a name="24811"></a>public class HelloWorld extends ResultAdapter {
<a name="31190"></a>	static Recognizer rec;
<a name="31191"></a>
<a name="24815"></a>	// Receives RESULT_ACCEPTED event: print it, clean up, exit
<a name="24816"></a>	public void resultAccepted(ResultEvent e) {
<a name="25049"></a>		Result r = (Result)(e.getSource());
<a name="28440"></a>		ResultToken tokens[] = r.getBestTokens();
<a name="28447"></a>
<a name="25052"></a>		for (int i = 0; i &lt; tokens.length; i++)
<a name="25055"></a>			System.out.print(tokens[i].getSpokenText() + " ");
<a name="25061"></a>		System.out.println();
<a name="31192"></a>
<a name="31193"></a>		// Deallocate the recognizer and exit
<a name="31194"></a>		rec.deallocate();
<a name="24818"></a>		System.exit(0);
<a name="24819"></a>	}
<a name="24820"></a>
<a name="24821"></a>	public static void main(String args[]) {
<a name="24822"></a>		try {
<a name="24824"></a>			// Create a recognizer that supports English.
<a name="24827"></a>			rec = Central.createRecognizer(
<a name="31218"></a>							new EngineModeDesc(Locale.ENGLISH));
<a name="24831"></a>			
<a name="24832"></a>			// Start up the recognizer
<a name="24833"></a>			rec.allocate();
<a name="24834"></a>	 
<a name="24835"></a>			// Load the grammar from a file, and enable it
<a name="24836"></a>			FileReader reader = new FileReader(args[0]);
<a name="24837"></a>			RuleGrammar gram = rec.loadJSGF(reader);
<a name="31329"></a>			gram.setEnabled(true);
<a name="24838"></a>	
<a name="24839"></a>			// Add the listener to get results
<a name="24840"></a>			rec.addResultListener(new HelloWorld());
<a name="24841"></a>	
<a name="24842"></a>			// Commit the grammar
<a name="24844"></a>			rec.commitChanges();
<a name="24845"></a>	
<a name="24846"></a>			// Request focus and start listening
<a name="31211"></a>			rec.requestFocus();
<a name="24847"></a>			rec.resume();
<a name="24848"></a>		} catch (Exception e) {
<a name="24849"></a>			e.printStackTrace();
<a name="24850"></a>		}
<a name="24851"></a>	}
<a name="15779"></a>}


      <hr></pre>
    </dd>
   </dl>
   <p><a name="31231"></a>This example illustrates the basic steps which all speech recognition applications must perform. Let's examine each step in detail. </p>
   <ul>
    <li><a name="31232"></a><em>Create:</em> The <code>Central</code> class of <code>javax.speech</code> package is used to obtain a speech recognizer by calling the <code>createRecognizer</code> method. The <code>EngineModeDesc</code> argument provides the information needed to locate an appropriate recognizer. In this example we requested a recognizer that understands English (since the grammar is written for English). </li>
   </ul> 
   <ul>
    <li><a name="31233"></a><em>Allocate:</em> The <code>allocate</code> methods requests that the <code>Recognizer</code> allocate all necessary resources. </li>
   </ul> 
   <ul>
    <li><a name="31234"></a><em>Load and enable grammars:</em> The <code>loadJSGF</code> method reads in a JSGF document from a reader created for the file that contains the javax.speech.demo grammar. (Alternatively, the <code>loadJSGF</code> method can load a grammar from a URL.) Next, the grammar is <em>enabled</em>. Once the recognizer receives focus (see below), an enabled grammar is <em>activated</em> for recognition: that is, the recognizer compares incoming audio to the active grammars and listens for speech that matches those grammars. </li>
   </ul> 
   <ul>
    <li><a name="31301"></a><em>Attach a ResultListener:</em> The <code>HelloWorld</code> class extends the <code>ResultAdapter</code> class which is a trivial implementation of the <code>ResultListener</code> interface. An instance of the <code>HelloWorld</code> class is attached to the Recognizer to receive result events. These events indicate progress as the recognition of speech takes place. In this implementation, we process the <code>RESULT_ACCEPTED</code> event, which is provided when the recognizer completes recognition of input speech that matches an active grammar. </li>
   </ul> 
   <ul>
    <li><a name="31358"></a><em>Commit changes:</em> Any changes in grammars and the grammar enabled status needed to be <em>committed</em> to take effect (that includes creation of a new grammar). The reasons for this are described in <a href="Recognition.html#32232">Section 6.4.2</a>. </li>
   </ul> 
   <ul>
    <li><a name="31365"></a><em>Request focus and resume:</em> For recognition of the grammar to occur, the recognizer must be in the <code>RESUMED</code> state and must have the speech focus. The <code>requestFocus</code> and <code>resume</code> methods achieve this. </li>
   </ul> 
   <ul>
    <li><a name="31382"></a><em>Process result:</em> Once the <code>main</code> method is completed, the application waits until the user speaks. When the user speaks something that matches the loaded grammar, the recognizer issues a <code>RESULT_ACCEPTED</code> event to the listener we attached to the recognizer. The source of this event is a <code>Result</code> object that contains information about what the recognizer heard. The <code>getBestTokens</code> method returns an array of <code>ResultTokens</code>, each of which represents a single spoken word. These words are printed. </li>
   </ul> 
   <ul>
    <li><a name="31235"></a><em>Deallocate:</em> Before exiting we call <code>deallocate</code> to free up the recognizer's resources. </li>
   </ul> 
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="31145"></a>
   <h2>6.2 &nbsp; &nbsp; Recognizer as an Engine </h2> 
   <p><a name="31411"></a>The basic functionality provided by a <code>Recognizer</code> includes grammar management and the production of results when a user says things that match active grammars. The <code>Recognizer</code> interface extends the <code>Engine</code> interface to provide this functionality. </p>
   <p><a name="31412"></a>The following is a list of the functionality that the <code>javax.speech.recognition</code> package inherits from the <code>javax.speech</code> package and outlines some of the ways in which that functionality is specialized. </p>
   <ul>
    <li><a name="31413"></a>The properties of a speech engine defined by the <code>EngineModeDesc</code> class apply to recognizers. The <code>RecognizerModeDesc</code> class adds information about dictation capabilities of a recognizer and about users who have trained the engine. Both <code>EngineModeDesc</code> and <code>RecognizerModeDesc</code> are described in <a href="SpeechEngine.html#9171">Section 4.2</a>. </li>
   </ul> 
   <ul>
    <li><a name="31420"></a>Recognizers are searched, selected and created through the <code>Central</code> class in the <code>javax.speech</code> package as described in <a href="SpeechEngine.html#12411">Section 4.3</a>. That section explains default creation of a recognizer, recognizer selection according to defined properties, and advanced selection and creation mechanisms. </li>
   </ul> 
   <ul>
    <li><a name="32521"></a>Recognizers inherit the basic state systems of an engine from the <code>Engine</code> interface, including the four allocation states, the pause and resume state, the state monitoring methods and the state update events. The engine state systems are described in <a href="SpeechEngine.html#12921">Section 4.4</a>. The two state systems added by recognizers are described in <a href="Recognition.html#32697">Section 6.3</a>. </li>
   </ul> 
   <ul>
    <li><a name="31440"></a>Recognizers produce all the standard engine events (see <a href="SpeechEngine.html#14106">Section 4.5</a>). The <code>javax.speech.recognition</code> package also extends the <code>EngineListener</code> interface as <code>RecognizerListener</code> to provide events that are specific to recognizers. </li>
   </ul> 
   <ul>
    <li><a name="32683"></a>Other engine functionality inherited as an engine includes the runtime properties (see <a href="SpeechEngine.html#16719">Section 4.6.1</a> and <a href="Recognition.html#15438">Section 6.8</a>), audio management (see <a href="SpeechEngine.html#16984">Section 4.6.2</a>) and vocabulary management (see <a href="SpeechEngine.html#17081">Section 4.6.3</a>). </li>
   </ul> 
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="32697"></a>
   <h2>6.3 &nbsp; &nbsp; Recognizer State Systems </h2> 
   <a name="32551"></a>
   <h3>6.3.1 &nbsp; &nbsp; Inherited States </h3> 
   <p><a name="32552"></a>As mentioned above, a <code>Recognizer</code> inherits the basic state systems defined in the <code>javax.speech</code> package, particularly through the <code>Engine</code> interface. The basic engine state systems are described in <a href="SpeechEngine.html#12921">Section 4.4</a>. In this section the two state systems added for recognizers are described. These two states systems represent the status of recognition processing of audio input against grammars, and the recognizer focus. </p>
   <p><a name="32640"></a>As a summary, the following state system functionality is inherited from the <code>javax.speech</code> package. </p>
   <ul>
    <li><a name="32643"></a>The basic engine state system represents the current allocation state of the engine: whether resources have been obtained for the engine. The four allocation states are <code>ALLOCATED</code>, <code>DEALLOCATED</code>, <code>ALLOCATING_RESOURCES</code> and <code>DEALLOCATING_RESOURCES</code>. </li>
   </ul> 
   <ul>
    <li><a name="32724"></a>The <code>PAUSED</code> and <code>RESUMED</code> states are sub-states of the <code>ALLOCATED</code> state. The paused and resumed states of a recognizer indicate whether audio input is on or off. Pausing a recognizer is analogous to turning off the input microphone: input audio is lost. <a href="SpeechEngine.html#13953">Section 4.4.7</a> describes the effect of pausing and resuming a recognizer in more detail. </li>
   </ul> 
   <ul>
    <li><a name="32732"></a>The <code>getEngineState</code> method of the <code>Engine</code> interface returns a <code>long</code> value representing the current engine state. The value has a bit set for each of the current states of the recognizer. For example and <code>ALLOCATED</code> recognizer in the <code>RESUMED</code> state will have both the <code>ALLOCATED</code> and <code>RESUMED</code> bits set. </li>
   </ul> 
   <ul>
    <li><a name="32774"></a>The <code>testEngineState</code> and <code>waitEngineState</code> methods are convenience methods for monitoring engine state. The test method tests for presence in a specified state. The wait method blocks until a specific state is reached. </li>
   </ul> 
   <ul>
    <li><a name="32733"></a>An <code>EngineEvent</code> is issued to <code>EngineListeners</code> each time an engine changes state. The event class includes the new and old engine states. </li>
   </ul> 
   <p><a name="32641"></a>The recognizer adds two sub-state systems to the <code>ALLOCATED</code> state: that's in addition to the inherited pause and resume sub-state system. The two new sub- state systems represent the current activities of the recognizer's internal processing (the <code>LISTENING</code>, <code>PROCESSING</code> and <code>SUSPENDED</code> states) and the current recognizer focus (the <code>FOCUS_ON</code> and <code>FOCUS_OFF</code> states). </p>
   <p><a name="32810"></a>These new sub-state systems are parallel states to the <code>PAUSED </code>and <code>RESUMED</code> states and operate nearly independently as shown in <a href="Recognition.html#32810">Figure 6-1</a> (an extension of <a href="SpeechEngine.html#18933">Figure 4-2</a>).<img src="Recognition1.gif"> </p> 
   <a name="32913"></a>
   <h3>6.3.2 &nbsp; &nbsp; Recognizer Focus </h3> 
   <p><a name="33107"></a>The <code>FOCUS_ON</code> and <code>FOCUS_OFF</code> states indicate whether this instance of the <code>Recognizer</code> currently has the speech focus. Recognizer focus is a major determining factor in grammar activation, which, in turn, determines what the recognizer is listening for at any time. The role of recognizer focus in activation and deactivation of grammars is described in <a href="Recognition.html#32313">Section 6.4.3</a>. </p>
   <p><a name="33075"></a>A change in engine focus is indicated by a <code>RecognizerEvent</code> (which extends <code>EngineEvent</code>) being issued to <code>RecognizerListeners</code>. A <code>FOCUS_LOST</code> event indicates a change in state from <code>FOCUS_ON</code> to <code>FOCUS_OFF</code>. A <code>FOCUS_GAINED</code> event indicates a change in state from <code>FOCUS_OFF</code> to <code>FOCUS_ON</code>. </p>
   <p><a name="33041"></a>When a <code>Recognizer</code> has focus, the <code>FOCUS_ON</code> bit is set in the engine state. When a <code>Recognizer</code> does not have focus, the <code>FOCUS_OFF</code> bit is set. The following code examples monitor engine state: </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="33050"></a>Recognizer rec;
<a name="33051"></a>
<a name="33054"></a>if (rec.testEngineState(Recognizer.FOCUS_ON)) {
<a name="33055"></a>	// we have focus so release it
<a name="33056"></a>	rec.releaseFocus();
<a name="33087"></a>}
<a name="33063"></a>// wait until we lose it
<a name="33057"></a>rec.waitEngineState(Recognizer.FOCUS_OFF);


      <hr></pre>
    </dd>
   </dl>
   <p><a name="32987"></a>Recognizer focus is relevant to computing environments in which more than one application is using an underlying recognition. For example, in a desktop environment a user might be running a single speech recognition product (the underlying engine), but have multiple applications using the speech recognizer as a resource. These applications may be a mixture of Java and non-Java applications. Focus is not usually relevant in a telephony environment or in other speech application contexts in which there is only a single application processing the audio input stream. </p>
   <p><a name="32971"></a>The recognizer's focus should track the application to which the user is currently talking. When a user indicates that it wants to talk to an application (e.g., by selecting the application window, or explicitly saying "switch to application X"), the application requests speech focus by calling the <code>requestFocus</code> method of the <code>Recognizer</code>. </p>
   <p><a name="33120"></a>When speech focus is no longer required (e.g., the application has been iconized) it should call <code>releaseFocus</code> method to free up focus for other applications. </p>
   <p><a name="33129"></a>Both methods are asynchronous Þ- the methods may return before the focus is gained or lost - since focus change may be deferred. For example, if a recognizer is in the middle of recognizing some speech, it will typically defer the focus change until the result is completed. The focus events and the engine state monitoring methods can be used to determine when focus is actually gained or lost. </p>
   <p><a name="32967"></a>The focus policy is determined by the underlying recognition engine - it is not prescribed by the <code>java.speech.recognition</code> package. In most operating environments it is reasonable to assume a policy in which the last application to request focus gets the focus. </p>
   <p><a name="33020"></a>Well-behaved applications adhere to the following convention to maximize recognition performance, to minimize their impact upon other applications and to maintain a satisfactory user interface experience. An application should only request focus when it is confident that the user's speech focus (attention) is directed towards it, and it should release focus when it is not required. </p> 
   <a name="33018"></a>
   <h3>6.3.3 &nbsp; &nbsp; Recognition States </h3> 
   <p><a name="33225"></a>The most important (and most complex) state system of a recognizer represents the current recognition activity of the recognizer. An <code>ALLOCATED</code> <code>Recognizer</code> is always in one of the following three states: </p>
   <ul>
    <li><a name="33229"></a><code>LISTENING</code> state: The <code>Recognizer</code> is listening to incoming audio for speech that may match an active grammar but has not detected speech yet. A recognizer remains in this state while listening to silence and when audio input runs out because the engine is paused. </li>
   </ul> 
   <ul>
    <li><a name="33257"></a><code>PROCESSING</code> state: The <code>Recognizer</code> is processing incoming speech that may match an active grammar. While in this state, the recognizer is producing a result. </li>
   </ul> 
   <ul>
    <li><a name="33265"></a><code>SUSPENDED</code> state: The <code>Recognizer</code> is temporarily suspended while grammars are updated. While suspended, audio input is buffered for processing once the recognizer returns to the <code>LISTENING</code> and <code>PROCESSING</code> states. </li>
   </ul> 
   <p><a name="33241"></a>This sub-state system is shown in <a href="Recognition.html#32810">Figure 6-1</a>. The typical state cycle of a recognizer is triggered by user speech. The recognizer starts in the <code>LISTENING</code> state, moves to the <code>PROCESSING</code> state while a user speaks, moves to the <code>SUSPENDED</code> state once recognition of that speech is completed and while grammars are updates in response to user input, and finally returns to the <code>LISTENING</code> state. </p>
   <p><a name="33377"></a>In this first event cycle a <code>Result</code> is typically produced that represents what the recognizer heard. Each <code>Result</code> has a state system and the <code>Result</code> state system is closely coupled to this <code>Recognizer</code> state system. The <code>Result</code> state system is discussed in <a href="Recognition.html#20505">Section 6.7</a>. Many applications (including the <em><a href="Recognition.html#11644">"Hello World!"</a></em> example) do not care about the recognition state but do care about the simpler <code>Result</code> state system. </p>
   <p><a name="33378"></a>The other typical event cycle also starts in the <code>LISTENING</code> state. Upon receipt of a non-speech event (e.g., keyboard event, mouse click, timer event) the recognizer is suspended temporarily while grammars are updated in response to the event, and then the recognizer returns to listening. </p>
   <p><a name="33436"></a>Applications in which grammars are affected by more than speech events need to be aware of the recognition state system. </p>
   <p><a name="33215"></a>The following sections explain these event cycles in more detail and discuss why speech input events are different in some respects from other event types. </p> 
   <a name="33418"></a>
   <h4>6.3.3.1 &nbsp; &nbsp; Speech Events vs. Other Events </h4> 
   <p><a name="33346"></a>A keyboard event, a mouse event, a timer event, a socket event are all instantaneous in time - there is a defined instant at which they occur. The same is not true of speech for two reasons. </p>
   <p><a name="33302"></a>Firstly, speech is a temporal activity. Speaking a sentence takes time. For example, a short command such as "reload this web page" will take a second or two to speak, thus, it is not instantaneous. At the start of the speech the recognizer changes state, and as soon as possible after the end of the speech the recognizer produces a result containing the spoken words. </p>
   <p><a name="33303"></a>Secondly, recognizers cannot always recognize words immediately when they are spoken and cannot determine immediately when a user has stopped speaking. The reasons for these technical constraints upon recognition are outside the scope of this guide, but knowing about them is helpful in using a recognizer. (Incidentally, the same principals are generally true of human perception of speech.) </p>
   <p><a name="33449"></a>A simple example of why recognizers cannot always respond might be listening to a currency amount. If the user says "two dollars" or says "two dollars, fifty seconds" with a short pause after the word "dollars" the recognizer can't know immediately whether the user has finished speaking after the "dollars". What a recognizer must do is wait a short period - usually less than a second Þ- to see if the user continues speaking. A second is a long time for a computer and complications can arise if the user clicks a mouse or does something else in that waiting period. (<a href="Recognition.html#15438">Section 6.8</a> explains the time-out parameters that affect this delay.) </p>
   <p><a name="33481"></a>A further complication is introduced by the input audio buffering described in <a href="Recognition.html#32697">Section 6.3</a>. </p>
   <p><a name="33453"></a>Putting all this together, there is a requirement for the recognizers to explicitly represent internal state through the <code>LISTENING</code>, <code>PROCESSING</code> and <code>SUSPENDED</code> states. </p> 
   <a name="33518"></a>
   <h4>6.3.3.2 &nbsp; &nbsp; Speech Input Event Cycle </h4> 
   <p><a name="33534"></a>The typical recognition state cycle for a <code>Recognizer</code> occurs as speech input occurs. Technically speaking, this cycle represents the recognition of a single <code>Result</code>. The result state system and result events are described in detail in <a href="Recognition.html#20505">Section 6.7</a>. The cycle described here is a clockwise trip through the <code>LISTENING</code>, <code>PROCESSING</code> and <code>SUSPENDED</code> states of an <code>ALLOCATED</code> recognizer as shown in <a href="Recognition.html#32810">Figure 6-1</a>. </p>
   <p><a name="33584"></a>The <code>Recognizer</code> starts in the <code>LISTENING</code> state with a certain set of grammars enabled and active. When incoming audio is detected that may match an active grammar, the <code>Recognizer</code> transitions from the <code>LISTENING</code> state to the <code>PROCESSING</code> state with a <code>RECOGNIZER_PROCESSING</code> event. </p>
   <p><a name="33606"></a>The <code>Recognizer</code> then creates a new <code>Result</code> object and issues a <code>RESULT_CREATED</code> event (a <code>ResultEvent</code>) to provide the result to the application. At this point the result is usually empty: it does not contain any recognized words. As recognition proceeds words are added to the result along with other useful information. </p>
   <p><a name="33549"></a>The <code>Recognizer</code> remains in the <code>PROCESSING</code> state until it completes recognition of the result. While in the <code>PROCESSING</code> state the <code>Result</code> may be updated with new information. </p>
   <p><a name="33653"></a>The recognizer indicates completion of recognition by issuing a <code>RECOGNIZER_SUSPENDED</code> event to transition from the <code>PROCESSING</code> state to the <code>SUSPENDED</code> state. Once in that state, the recognizer issues a result <em>finalization</em> event to <code>ResultListeners</code> (<code>RESULT_ACCEPTED</code> or <code>RESULT_REJECTED</code> event) to indicate that all information about the result is finalized (words, grammars, audio etc.). </p>
   <p><a name="33688"></a>The <code>Recognizer</code> remains in the <code>SUSPENDED</code> state until processing of the result finalization event is completed. Applications will often make grammar changes during the result finalization because the result causes a change in application state or context. </p>
   <p><a name="33793"></a>In the <code>SUSPENDED</code> state the <code>Recognizer</code> buffers incoming audio. This buffering allows a user to continue speaking without speech data being lost. Once the <code>Recognizer</code> returns to the <code>LISTENING </code>state the buffered audio is processed to give the user the perception of real-time processing. </p>
   <p><a name="33723"></a>Once the result finalization event has been issued to all listeners, the <code>Recognizer</code> automatically commits all grammar changes and issues a <code>CHANGES_COMMITTED</code> event to return to the <code>LISTENING</code> state. (It also issues <code>GRAMMAR_CHANGES_COMMITTED</code> events to <code>GrammarListeners</code> of changed grammars.) The commit applies all grammar changes made at any point up to the end of result finalization, such as changes made in the result finalization events. </p>
   <p><a name="33564"></a>The <code>Recognizer</code> is now back in the <code>LISTENING</code> state listening for speech that matches the new grammars. </p>
   <p><a name="33565"></a>In this event cycle the first two recognizer state transitions (marked by <code>RECOGNIZER_PROCESSING</code> and <code>RECOGNIZER_SUSPENDED</code> events) are triggered by user actions: starting and stopping speaking. The third state transition (<code>CHANGES_COMMITTED</code> event) is triggered programmatically some time after the <code>RECOGNIZER_SUSPENDED</code> event. </p>
   <p><a name="33810"></a>The <code>SUSPENDED</code> state serves as a temporary state in which recognizer configuration can be updated without loosing audio data. </p> 
   <a name="33522"></a>
   <h4>6.3.3.3 &nbsp; &nbsp; Non-Speech Event Cycle </h4> 
   <p><a name="33533"></a>For applications that deal only with spoken input the state cycle described above handles most normal speech interactions. For applications that handle other asynchronous input, additional state transitions are possible. Other types of asynchronous input include graphical user interface events (e.g., <code>AWTEvent</code>), timer events, multi-threading events, socket events and so on. </p>
   <p><a name="34023"></a>The cycle described here is temporary transition from the <code>LISTENING</code> state to the <code>SUSPENDED</code> and back as shown in <a href="Recognition.html#32810">Figure 6-1</a>. </p>
   <p><a name="33820"></a>When a non-speech event occurs which changes the application state or application data it may be necessary to update the recognizer's grammars. The <code>suspend</code> and <code>commitChanges</code> methods of a <code>Recognizer</code> are used to handle non- speech asynchronous events. The typical cycle for updating grammars in response to a non-speech asynchronous events is as follows. </p>
   <p><a name="33827"></a>Assume that the <code>Recognizer</code> is in the <code>LISTENING</code> state (the user is not currently speaking). As soon as the event is received, the application calls <code>suspend</code> to indicate that it is about to change grammars. In response, the recognizer issues a <code>RECOGNIZER_SUSPENDED</code> event and transitions from the <code>LISTENING</code> state to the <code>SUSPENDED</code> state. </p>
   <p><a name="33831"></a>With the <code>Recognizer</code> in the <code>SUSPENDED</code> state, the application makes all necessary changes to the grammars. (The grammar changes affected by this event cycle and the pending commit are described in <a href="Recognition.html#32232">Section 6.4.2</a>.) </p>
   <p><a name="33978"></a>Once all grammar changes are completed the application calls the <code>commitChanges</code> method. In response, the recognizer applies the new grammars and issues a <code>CHANGES_COMMITTED</code> event to transition from the <code>SUSPENDED</code> state back to the <code>LISTENING</code> state. (It also issues <code>GRAMMAR_CHANGES_COMMITTED</code> events to all changed grammars.) </p>
   <p><a name="33836"></a>Finally, the <code>Recognizer</code> resumes recognition of the buffered audio and then live audio with the new grammars. </p>
   <p><a name="33929"></a>The suspend and commit process is designed to provide a number of features to application developers which help give users the perception of a responsive recognition system. </p>
   <p><a name="34003"></a>Because audio is buffered from the time of the asynchronous event to the time at which the <code>CHANGES_COMMITTED</code> occurs, the audio is processed as if the new grammars were applied exactly at the time of the asynchronous event. The user has the perception of real-time processing. </p>
   <p><a name="34004"></a>Although audio is buffered in the <code>SUSPENDED</code> state, applications should make grammar changes and call <code>commitChanges</code> as quickly as possible. This minimizes the amount of data in the audio buffer and hence the amount of time it takes for the recognizer to "catch up". It also minimizes the possibility of a buffer overrun. </p>
   <p><a name="33846"></a>Technically speaking, an application is not required to call <code>suspend</code> prior to calling <code>commitChanges</code>. If the <code>suspend</code> call is committed the <code>Recognizer</code> behaves as if suspend had been called immediately prior to calling <code>commitChanges</code>. However, an application that does not call <code>suspend</code> risks a commit occurring unexpectedly while it updates grammars with the effect of leaving grammars in an inconsistent state. </p> 
   <a name="33526"></a>
   <h3>6.3.4 &nbsp; &nbsp; Interactions of State Systems </h3> 
   <p><a name="34021"></a>The three sub-state systems of an allocated recognizer (shown in <a href="Recognition.html#32810">Figure 6-1</a>) normally operate independently. There are, however, some indirect interactions. </p>
   <p><a name="34052"></a>When a recognizer is paused, audio input is stopped. However, recognizers have a buffer between audio input and the internal process that matches audio against grammars, so recognition can continue temporarily after a recognizer is paused. In other words, a <code>PAUSED</code> recognizer may be in the <code>PROCESSING</code> state. </p>
   <p><a name="34064"></a>Eventually the audio buffer will empty. If the recognizer is in the <code>PROCESSING</code> state at that time then the result it is working on is immediately finalized and the recognizer transitions to the <code>SUSPENDED</code> state. Since a well-behaved application treats <code>SUSPENDED</code> state as a temporary state, the recognizer will eventually leave the <code>SUSPENDED</code> state by committing grammar changes and will return to the <code>LISTENING</code> state. </p>
   <p><a name="34022"></a>The <code>PAUSED</code>/<code>RESUMED</code> state of an engine is shared by multiple applications, so it is possible for a recognizer to be paused and resumed because of the actions of another application. Thus, an application should always leave its grammars in a state that would be appropriate for a <code>RESUMED</code> recognizer. </p>
   <p><a name="34094"></a>The focus state of a recognizer is independent of the <code>PAUSED</code> and <code>RESUMED</code> states. For instance, it is possible for a paused <code>Recognizer</code> to have <code>FOCUS_ON</code>. When the recognizer is resumed, it will have the focus and its grammars will be activated for recognition. </p>
   <p><a name="34102"></a>The focus state of a recognizer is very loosely coupled with the recognition state. An application that has no <code>GLOBAL</code> grammars (described in <a href="Recognition.html#32313">Section 6.4.3</a>) will not receive any recognition results unless it has recognition focus. </p>
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="12452"></a>
   <h2>6.4 &nbsp; &nbsp; Recognition Grammars </h2> 
   <p><a name="18663"></a>A <em>grammar</em> defines what a recognizer should listen for in incoming speech. Any grammar defines the set of tokens a user can say (a token is typically a single word) and the patterns in which those words are spoken. </p>
   <p><a name="31573"></a>The Java Speech API supports two types of grammars: <em>rule grammars</em> and <em>dictation grammars</em>. These grammars differ in how patterns of words are defined. They also differ in their programmatic use: a rule grammar is defined by an application, whereas a dictation grammar is defined by a recognizer and is built into the recognizer. </p>
   <p><a name="31572"></a>A rule grammar is provided by an application to a recognizer to define a set of rules that indicates what a user may say. Rules are defined by tokens, by references to other rules and by logical combinations of tokens and rule references. Rule grammars can be defined to capture a wide range of spoken input from users by the progressive combination of simple grammars and rules. </p>
   <p><a name="31585"></a>A dictation grammar is built into a recognizer. It defines a set of words (possibly tens of thousands of words) which may be spoken in a relatively unrestricted way. Dictation grammars are closest to the goal of unrestricted natural speech input to computers. Although dictation grammars are more flexible than rule grammars, recognition of rule grammars is typically faster and more accurate. </p>
   <p><a name="31614"></a>Support for a dictation Þgrammar is optional for a recognizer. As <a href="SpeechEngine.html#9171">Section 4.2</a> explains, an application that requires dictation functionality can request it when creating a recognizer. </p>
   <p><a name="31632"></a>A recognizer may have many rule grammars loaded at any time. However, the current <code>Recognizer</code> interface restricts a recognizer to a single dictation grammar. The technical reasons for this restriction are outside the scope of this guide. </p> 
   <a name="31641"></a>
   <h3>6.4.1 &nbsp; &nbsp; Grammar Interface </h3> 
   <p><a name="31644"></a>The <code>Grammar</code> interface is the root interface that is extended by all grammars. The grammar functionality that is shared by all grammars is presented through this interface. </p>
   <p><a name="31640"></a>The <code>RuleGrammar</code> interface is an extension of the <code>Grammar</code> interface to support rule grammars. The <code>DictationGrammar</code> interface is an extension of the <code>Grammar</code> interface to support dictation grammars. </p>
   <p><a name="31671"></a>The following are the capabilities presented by the grammar interface: </p>
   <ul>
    <li><a name="31672"></a><em>Grammar naming</em>: Every grammar loaded into a recognizer must have a unique name. The <code>getName</code> method returns that name. Grammar names allow references to be made between grammars. The grammar naming convention is described in the Java Speech Grammar Format Specification Briefly, the grammar naming convention is very similar to the class naming convention for the Java programming language. For example, a grammar from Acme Corp. for dates might be called "<code>com.acme.speech.dates</code>". </li>
   </ul> 
   <ul>
    <li><a name="31673"></a><em>Enabling and disabling</em>: Grammars may be enabled or disabled using the <code>setEnabled</code> method. When a grammar is enabled and when specified activation conditions are met, the grammar is activated. Once a grammar is active a recognizer will listen to incoming audio for speech that matches that grammar. Enabling and activation are described in more detail below (<a href="Recognition.html#32313">Section 6.4.3</a>). </li>
   </ul> 
   <ul>
    <li><a name="31702"></a><em>Activation mode</em>: This is the property of a grammar that determines which conditions need to be met for a grammar to be activated. The activation mode is managed through the <code>getActivationMode</code> and <code>setActivationMode</code> methods (described in <a href="Recognition.html#32313">Section 6.4.3</a>). The three available activation modes are defined as constants of the <code>Grammar</code> interface: <code>RECOGNIZER_FOCUS</code>, <code>RECOGNIZER_MODAL</code> and <code>GLOBAL</code>. </li>
   </ul> 
   <ul>
    <li><a name="31703"></a><em>Activation</em>: the <code>isActive</code> method returns a <code>boolean</code> value that indicates whether a <code>Grammar</code> is currently active for recognition. </li>
   </ul> 
   <ul>
    <li><a name="31704"></a><em>GrammarListener</em>: the <code>addGrammarListener</code> and <code>removeGrammarListener</code> methods allow a <code>GrammarListener</code> to be attached to and removed from a <code>Grammar</code>. The <code>GrammarEvents</code> issued to the listener indicate when grammar changes have been committed and whenever the grammar activation state changes. </li>
   </ul> 
   <ul>
    <li><a name="31705"></a><em>ResultListener</em>: the <code>addResultListener</code> and <code>removeResultListener</code> methods allow a <code>ResultListener</code> to be attached to and removed from a <code>Grammar</code>. This listener receives notification of all events for any result that matches the grammar. </li>
   </ul> 
   <ul>
    <li><a name="31755"></a><em>Recognizer</em>: the <code>getRecognizer</code> method returns a reference to the <code>Recognizer</code> that owns the <code>Grammar</code>. </li>
   </ul> 
   <a name="32232"></a>
   <h3>6.4.2 &nbsp; &nbsp; Committing Changes </h3> 
   <p><a name="32237"></a>The Java Speech API supports <em>dynamic grammars</em>; that is, it supports the ability for an application to modify grammars at runtime. In the case of rule grammars any aspect of any grammar can be changed at any time. </p>
   <p><a name="32250"></a>After making any change to a grammar through the <code>Grammar</code>, <code>RuleGrammar</code> or <code>DictationGrammar</code> interfaces an application must <em>commit the changes</em>. This applies to changes in definitions of rules in a <code>RuleGrammar</code>, to changing context for a <code>DictationGrammar</code>, to changing the enabled state, or to changing the activation mode. (It does not apply to adding or removing a <code>GrammarListener</code> or <code>ResultListener</code>.) </p>
   <p><a name="32245"></a>Changes are committed by calling the <code>commitChanges</code> method of the <code>Recognizer</code>. The commit is required for changes to affect the recognition process: that is, the processing of incoming audio. </p>
   <p><a name="32263"></a>The commit changes mechanism has two important properties: </p>
   <ul>
    <li><a name="32265"></a>Updates to grammar definitions and the enabled property take effect <em>atomically</em> (all changes take effect at once). There are no intermediate states in which some, but not all, changes have been applied. </li>
   </ul> 
   <ul>
    <li><a name="32297"></a>The <code>commitChanges</code> method is a method of <code>Recognizer</code> so all changes to all grammars are committed at once. Again, there are no intermediate states in which some, but not all, changes have been applied. </li>
   </ul> 
   <p><a name="32260"></a>There is one instance in which changes are committed without an explicit call to the <code>commitChanges</code> method. Whenever a recognition result is <em>finalized</em> (completed), an event is issued to <code>ResultListeners</code> (it is either a <code>RESULT_ACCEPTED</code> or <code>RESULT_REJECTED</code> event). Once processing of that event is completed changes are normally committed. This supports the common situation in which changes are often made to grammars in response to something a user says. </p>
   <p><a name="32346"></a>The event-driven commit is closely linked to the underlying state system of a <code>Recognizer</code>. The state system for recognizers is described in detail in <a href="Recognition.html#32697">Section 6.3</a>. </p> 
   <a name="32313"></a>
   <h3>6.4.3 &nbsp; &nbsp; Grammar Activation </h3> 
   <p><a name="31822"></a>A grammar is <em>active</em> when the recognizer is matching incoming audio against that grammar to determine whether the user is saying anything that matches that grammar. When a grammar is inactive it is not being used in the recognition process. </p>
   <p><a name="31830"></a>Applications to do not directly activate and deactivate grammars. Instead they provided methods for (1) enabling and disabling a grammar, (2) setting the activation mode for each grammar, and (3) requesting and releasing the speech focus of a recognizer (as described in <a href="Recognition.html#32913">Section 6.3.2</a>.) </p>
   <p><a name="31833"></a>The enabled state of a grammar is set with the <code>setEnabled</code> method and tested with the <code>isEnabled</code> method. For programmers familiar with AWT or Swing, enabling a speech grammar is similar to enabling a graphical component. </p>
   <p><a name="31834"></a>Once enabled, certain conditions must be met for a grammar to be activated. The activation mode indicates when an application wants the grammar to be active. There are three activation modes: <code>RECOGNIZER_FOCUS</code>, <code>RECOGNIZER_MODAL</code> and <code>GLOBAL</code>. For each mode a certain set of activation conditions must be met for the grammar to be activated for recognition. The activation mode is managed with the <code>setActivationMode</code> and <code>getActivationMode</code> methods. </p>
   <p><a name="32373"></a>The enabled flag and the activation mode are both parameters of a grammar that need to be committed to take effect. As <a href="Recognition.html#32232">Section 6.4.2</a> described, changes need to be committed to affect the recognition processes. </p>
   <p><a name="32133"></a>Recognizer focus is a major determining factor in grammar activation and is relevant in computing environments in which more than one application is using an underlying recognition (e.g., desktop computing with multiple speech-enabled applications). <a href="Recognition.html#32913">Section 6.3.2</a> describes how applications can request and release focus and monitor focus through <code>RecognizerEvents</code> and the engine state methods. </p>
   <p><a name="32165"></a>Recognizer focus is used to turn on and off activation of grammars. The roll of focus depends upon the activation mode. The three activation modes are described here in order from highest priority to lowest. An application should always use the lowest priority mode that is appropriate to its user interface functionality. </p>
   <ul>
    <li><a name="31847"></a><code>GLOBAL</code> activation mode: if enabled, the <code>Grammar</code> is always active irrespective of whether the <code>Recognizer</code> of this application has focus. </li>
   </ul> 
   <ul>
    <li><a name="31958"></a><code>RECOGNIZER_MODAL</code> activation mode: if enabled, the <code>Grammar</code> is always active when the application's <code>Recognizer</code> has focus. Furthermore, enabling a modal grammar deactivates any grammars in the same <code>Recognizer</code> with the <code>RECOGNIZER_FOCUS</code> activation mode. (The term "modal" is analogous to "modal dialog boxes" in graphical programming.) </li>
   </ul> 
   <ul>
    <li><a name="31979"></a><code>RECOGNIZER_FOCUS</code> activation mode (default mode): if enabled, the <code>Grammar</code> is active when the <code>Recognizer</code> of this application has focus. The exception is that if any other grammar of this application is enabled with <code>RECOGNIZER_MODAL</code> activation mode, then this grammar is not activated. </li>
   </ul> 
   <p><a name="31856"></a>The current activation state of a grammar can be tested with the <code>isActive</code> method. Whenever a grammar's activation changes either a <code>GRAMMAR_ACTIVATED</code> or <code>GRAMMAR_DEACTIVATED</code> event is issued to each attached <code>GrammarListener</code>. A grammar activation event typically follows a <code>RecognizerEvent</code> that indicates a change in focus (<code>FOCUS_GAINED</code> or <code>FOCUS_LOST</code>), or a <code>CHANGES_COMMMITTED</code> <code>RecognizerEvent</code> that indicates that a change in the enabled setting of a grammar has been applied to the recognition process. </p>
   <p><a name="32182"></a>An application may have zero, one or many grammars enabled at any time. Thus, an application may have zero, one or many grammars active at any time. As the conventions below indicate, well-behaved applications always <em>minimize</em> the number of active grammars. </p>
   <p><a name="32183"></a>The activation and deactivation of grammars is independent of <code>PAUSED</code> and <code>RESUMED</code> states of the <code>Recognizer</code>. For instance, a grammar can be active even when a recognizer is <code>PAUSED</code>. However, when a <code>Recognizer</code> is paused, audio input to the <code>Recognizer</code> is turned off, so speech won't be detected. This is useful, however, because when the recognizer is resumed, recognition against the active grammars immediately (and automatically) resumes. </p>
   <p><a name="31869"></a>Activating too many grammars and, in particular, activating multiple complex grammars has an adverse impact upon a recognizer's performance. In general terms, increasing the number of active grammars and increasing the complexity of those grammars can both lead to slower recognition response time, greater CPU load and reduced recognition accuracy (i.e., more mistakes). </p>
   <p><a name="32196"></a>Well-behaved applications adhere to the following conventions to maximize recognition performance and minimize their impact upon other applications: </p>
   <ul>
    <li><a name="32216"></a>Never apply the <code>GLOBAL</code> activation mode to a <code>DictationGrammar</code> (most recognizers will throw an exception if this is attempted). </li>
   </ul> 
   <ul>
    <li><a name="32217"></a>Always use the default activation mode <code>RECOGNIZER_FOCUS</code> unless there is a good reason to use another mode. </li>
   </ul> 
   <ul>
    <li><a name="32051"></a>Only use the <code>RECOGNIZER_MODAL</code> when it is certain that deactivating the <code>RECOGNIZER_FOCUS</code> grammars will not adversely affect the user interface. </li>
   </ul> 
   <ul>
    <li><a name="32059"></a>Minimize the complexity and the number of <code>RuleGrammars</code> with <code>GLOBAL</code> activation mode. As a general rule, one very simple <code>GLOBAL</code> rule grammar should be sufficient for nearly all applications. </li>
   </ul> 
   <ul>
    <li><a name="32066"></a>Only enable a grammar when it is appropriate for a user to say something matching that grammar. Otherwise disable the grammar to improve recognition response time and recognition accuracy for other grammars. </li>
   </ul> 
   <ul>
    <li><a name="32087"></a>Only request focus when confident that the user's speech focus (attention) is directed to grammars of your application. Release focus when it is not required. </li>
   </ul> 
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="16150"></a>
   <h2>6.5 &nbsp; &nbsp; Rule Grammars </h2> 
   <a name="32502"></a>
   <h3>6.5.1 &nbsp; &nbsp; Rule Definitions </h3> 
   <p><a name="34377"></a>A rule grammar is defined by a set of <em>rules</em>. These rules are defined by logical combinations of tokens to be spoken and references to other rules. The references may refer to other rules defined in the same rule grammar or to rules imported from other grammars. </p>
   <p><a name="34266"></a>Rule grammars follow the style and conventions of grammars in the Java Speech Grammar Format (defined in the <em>Java Speech Grammar Format Specification</em>). Any grammar defined in the JSGF can be converted to a <code>RuleGrammar</code> object. Any <code>RuleGrammar</code> object can be printed out in JSGF. (Note that conversion from JSGF to a <code>RuleGrammar</code> and back to JSGF will preserve the logic of the grammar but may lose comments and may change formatting.) </p>
   <p><a name="34307"></a>Since the <code>RuleGrammar</code> interface extends the <code>Grammar</code> interface, a <code>RuleGrammar</code> inherits the basic grammar functionality described in the previous sections (naming, enabling, activation etc.). </p>
   <p><a name="26454"></a>The easiest way to load a <code>RuleGrammar</code>, or set of <code>RuleGrammar</code> objects is from a Java Speech Grammar Format file or URL. The <code>loadJSGF</code> methods of the <code>Recognizer</code> perform this task. If multiple grammars must be loaded (where a grammar references one or more imported grammars), importing by URL is most convenient. The application must specify the base URL and the name of the root grammar to be loaded. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="26463"></a>Recognizer rec;
<a name="26485"></a>URL base = new URL("http://www.acme.com/app");
<a name="26488"></a>String grammarName = "com.acme.demo";
<a name="26465"></a>
<a name="26474"></a>Grammar gram = rec.loadURL(base, grammarName);


      <hr></pre>
    </dd>
   </dl>
   <p><a name="26461"></a>The recognizer converts the base URL and grammar name to a URL using the same conventions as <code>ClassLoader</code> (the Java platform mechanism for loading class files). By converting the periods in the grammar name to slashes ('/'), appending a <code>".gram"</code> suffix and combining with the base URL, the location is "<code>http:// www.acme.com/app/com/acme/demo.gram</code>". </p>
   <p><a name="26505"></a>If the demo grammar imports sub-grammars, they will be loaded automatically using the same location mechanism. </p>
   <p><a name="34276"></a>Alternatively, a <code>RuleGrammar</code> can be created by calling the <code>newRuleGrammar</code> method of a <code>Recognizer</code>. This method creates an empty grammar with a specified grammar name. </p>
   <p><a name="17038"></a>Once a <code>RuleGrammar</code> has been loaded, or has been created with the <code>newRuleGrammar</code> method, the following methods of a <code>RuleGrammar</code> are used to create, modify and manage the rules of the grammar. </p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="17044"></a>
     <b> Table 6-1 RuleGrammar methods for Rule management </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="17048"></a><b>Name </b> &nbsp;</td>
      <td><a name="17050"></a><b>Description </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17052"></a><code>setRule </code> &nbsp;</td>
      <td><a name="17054"></a>Assign a Rule object to a rulename. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="26374"></a><code>getRule </code> &nbsp;</td>
      <td><a name="26376"></a>Return the Rule object for a rulename. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17154"></a><code>getRuleInternal </code> &nbsp;</td>
      <td><a name="17156"></a>Return a reference to the recognizer's internal Rule object for a rulename (for fast, read-only access). &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17056"></a><code>listRuleNames </code> &nbsp;</td>
      <td><a name="17058"></a>List known rulenames. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17064"></a><code>isRulePublic </code> &nbsp;</td>
      <td><a name="17066"></a>Test whether a rulename is public. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17068"></a><code>deleteRule </code> &nbsp;</td>
      <td><a name="17070"></a>Delete a rule. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="34289"></a><code>setEnabled </code> &nbsp;</td>
      <td><a name="34291"></a>Enable and disable this RuleGrammar or rules of the grammar. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17072"></a><code>isEnabled </code> &nbsp;</td>
      <td><a name="17074"></a>Test whether a RuleGrammar or a specified rule is enabled. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="17294"></a>Any of the methods of <code>RuleGrammar</code> that affect the grammar (<code>setRule</code>, <code>deleteRule</code>, <code>setEnabled</code> etc.) take effect only after they are committed (as described in <a href="Recognition.html#32232">Section 6.4.2</a>). </p>
   <p><a name="34331"></a>The rule definitions of a <code>RuleGrammar</code> can be considered as a collection of named <code>Rule</code> objects. Each <code>Rule</code> object is referenced by its rulename (a <code>String</code>). The different types of <code>Rule</code> object are described in <a href="Recognition.html#17033">Section 6.5.3</a>. </p>
   <p><a name="17333"></a>Unlike most collections in Java, the <code>RuleGrammar</code> is a collection that does not share objects with the application. This is because recognizers often need to perform special processing of the rule objects and store additional information internally. The implication for applications is that a call to <code>setRule</code> is required to change any rule. The following code shows an example where changing a rule object does not affect the grammar. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="17334"></a>RuleGrammar gram;
<a name="17322"></a>
<a name="17348"></a>// Create a rule for the word blue
<a name="17359"></a>// Add the rule to the RuleGrammar and make it public
<a name="17345"></a>RuleToken word = new RuleToken("blue");
<a name="17323"></a>gram.setRule("ruleName", word, true);
<a name="17354"></a>
<a name="17355"></a>// Change the word
<a name="17356"></a>word.setText("green");
<a name="17325"></a>
<a name="17326"></a>// getRule returns blue (not green)
<a name="17328"></a>System.out.println(gram.getRule("ruleName"));


      <hr></pre>
    </dd>
   </dl>
   <p><a name="17318"></a>To ensure that the changed <code>"green"</code> token is loaded into the grammar, the application must call <code>setRule</code> again after changing the word to <code>"green"</code>. Furthermore, for either change to take effect in the recognition process, the changes need to be committed (see <a href="Recognition.html#32232">Section 6.4.2</a>). </p> 
   <a name="34376"></a>
   <h3>6.5.2 &nbsp; &nbsp; Imports </h3> 
   <p><a name="34380"></a>Complex systems of rules are most easily built by dividing the rules into multiple grammars. For example, a grammar could be developed for recognizing numbers. That grammar could then be <em>imported</em> into two separate grammars that defines dates and currency amounts. Those two grammars could then be imported into a travel booking application and so on. This type of hierarchical grammar construction is similar in many respects to object oriented and shares the advantage of easy reusage of grammars. </p>
   <p><a name="34381"></a>An import declaration in JSGF and an import in a <code>RuleGrammar</code> are most similar to the import statement of the Java programming language. Unlike a "#include" in the C programming language, the imported grammar is not copied, it is simply referencable. (A full specification of import semantics is provided in the Java Speech Grammar Format specification.) </p>
   <p><a name="17468"></a>The <code>RuleGrammar</code> interface defines three methods for handling imports as shown in <a href="Recognition.html#17376">Table 6-2</a>.</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="17376"></a>
     <b> Table 6-2 RuleGrammar import methods </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="17380"></a><b>Name </b> &nbsp;</td>
      <td><a name="17382"></a><b>Description </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17412"></a><code>addImport </code> &nbsp;</td>
      <td><a name="17414"></a>Add a grammar or rule for import. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17416"></a><code>removeImport </code> &nbsp;</td>
      <td><a name="17418"></a>Remove the import of a rule or grammar. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="17420"></a><code>getImports </code> &nbsp;</td>
      <td><a name="17422"></a>Return a list of all imported grammars or all rules imported from a specific grammar. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="34433"></a>The <code>resolve</code> method of the <code>RuleGrammar</code> interface is useful in managing imports. Given any rulename, the <code>resolve</code> method returns an object that represents the fully-qualified rulename for the rule that it references. </p> 
   <a name="17033"></a>
   <h3>6.5.3 &nbsp; &nbsp; Rule Classes </h3> 
   <p><a name="34444"></a>A <code>RuleGrammar</code> is primarily a collection of defined rules. The programmatic rule structure used to control <code>Recognizers</code> follows exactly the definition of rules in the Java Speech Grammar Format. Any rule is defined by a <code>Rule</code> object. It may be any one of the <code>Rule</code> classes described <a href="Recognition.html#16499">Table 6-3</a>. The exceptions are the <code>RuleParse</code> class, which is returned by the <code>parse</code> method of <code>RuleGrammar</code>, and the <code>Rule</code> class which is an abstract class and the parent of all other <code>Rule</code> objects. </p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="16499"></a>
     <b> Table 6-3 Rule objects </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="16503"></a><b>Name </b> &nbsp;</td>
      <td><a name="16505"></a><b>Description </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16507"></a><code>Rule </code> &nbsp;</td>
      <td><a name="16509"></a>Abstract root object for rules. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16511"></a><code>RuleName </code> &nbsp;</td>
      <td><a name="16572"></a>Rule that references another defined rule. <a name="34479"></a>JSGF example: &lt;ruleName&gt; &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16515"></a><code>RuleToken </code> &nbsp;</td>
      <td><a name="16517"></a>Rule consisting of a single speakable token (e.g. a word). <a name="34480"></a>JSGF examples: elephant, "New York" &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16519"></a><code>RuleSequence </code> &nbsp;</td>
      <td><a name="16521"></a>Rule consisting of a sequence of sub-rules. <a name="34484"></a>JSGF example: buy &lt;number&gt; shares of &lt;company&gt; &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16523"></a><code>RuleAlternatives </code> &nbsp;</td>
      <td><a name="16641"></a>Rule consisting of a set of alternative sub-rules. <a name="34485"></a>JSGF example: green | red | yellow &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16527"></a><code>RuleCount </code> &nbsp;</td>
      <td><a name="16529"></a>Rule containing a sub-rule that may be spoken optionally, zero or more times, or one or more times. <a name="34486"></a>JSGF examples: &lt;color&gt;*, [optional] &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16531"></a><code>RuleTag </code> &nbsp;</td>
      <td><a name="16533"></a>Rule that attaches a tag to a sub-rule. <a name="34487"></a>JSGF example: {action=open} &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="16604"></a><code>RuleParse </code> &nbsp;</td>
      <td><a name="16606"></a>Special rule object used to represent results of a parse. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="16652"></a>The following is an example of a grammar in Java Speech Grammar Format. The <em><a href="Recognition.html#11644">"Hello World!"</a></em> example shows how this JSGF grammar can be loaded from a text file. Below we consider how to create the same grammar programmatically. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="16655"></a>grammar com.sun.speech.test;
<a name="27904"></a>
<a name="16656"></a>public &lt;test&gt; = [a] test {TAG} | another &lt;rule&gt;;
<a name="16751"></a>&lt;rule&gt; = word;


      <hr></pre>
    </dd>
   </dl>
   <p><a name="30809"></a>The following code shows the simplest way to create this grammar. It uses the <code>ruleForJSGF</code> method to convert partial JSGF text to a <code>Rule</code> object. Partial JSGF is defined as any legal JSGF text that may appear on the right hand side of a rule definition - technically speaking, any legal JSGF rule expansion. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="30810"></a>Recognizer rec;
<a name="30811"></a>
<a name="30851"></a>// Create a new grammar
<a name="30812"></a>RuleGrammar gram = rec.newRuleGrammar("com.sun.speech.test");
<a name="30813"></a>
<a name="30852"></a>// Create the &lt;test&gt; rule
<a name="34504"></a>Rule test = gram.ruleForJSGF("[a] test {TAG} | another &lt;rule&gt;");
<a name="30814"></a>gram.setRule("test", // rulename
<a name="30815"></a>		test, // rule definition
<a name="30816"></a>		true); // true -&gt; make it public
<a name="34513"></a>
<a name="34514"></a>// Create the &lt;rule&gt; rule
<a name="30817"></a>gram.setRule("rule", gram.ruleForJSGF("word"), false);
<a name="30818"></a>
<a name="30853"></a>// Commit the grammar
<a name="30820"></a>rec.commitChanges();


      <hr></pre>
    </dd>
   </dl> 
   <a name="16752"></a>
   <h4>6.5.3.1 &nbsp; &nbsp; Advanced Rule Programming </h4> 
   <p><a name="34525"></a>In advanced programs there is often a need to define rules using the set of <code>Rule</code> objects described above. For these applications, using rule objects is more efficient than creating a JSGF string and using the <code>ruleForJSGF</code> method. </p>
   <p><a name="30857"></a>To create a rule by code, the detailed structure of the rule needs to be understood. At the top level of our example grammar, the <code>&lt;test&gt;</code> rule is an alternative: the user may say something that matches <code>"[a] test {TAG}"</code> or say something matching <code>"another &lt;rule&gt;"</code>. The two alternatives are each sequences containing two items. In the first alternative, the brackets around the token <code>"a"</code> indicate it is optional. The <code>"{TAG}"</code> following the second token (<code>"test"</code>) attaches a tag to the token. The second alternative is a sequence with a token (<code>"another"</code>) and a reference to another rule (<code>"&lt;rule&gt;"</code>). </p>
   <p><a name="16764"></a>The code to construct this <code>Grammar</code> follows (this code example is not compact - it is written for clarity of details). </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="16707"></a>Recognizer rec;
<a name="16720"></a>
<a name="25069"></a>RuleGrammar gram = rec.newRuleGrammar("com.sun.speech.test");
<a name="25070"></a>
<a name="25071"></a>// Rule we are building
<a name="25072"></a>RuleAlternatives test;
<a name="25073"></a>
<a name="25074"></a>// Temporary rules
<a name="25075"></a>RuleCount r1;
<a name="25076"></a>RuleTag r2;
<a name="25077"></a>RuleSequence seq1, seq2;
<a name="25078"></a>
<a name="25079"></a>// Create "[a]"
<a name="25080"></a>r1 = new RuleCount(new RuleToken("a"), RuleCount.OPTIONAL);
<a name="25081"></a>
<a name="25082"></a>// Create "test {TAG}" - a tagged token
<a name="25083"></a>r2 = new RuleTag(new RuleToken("test"), "TAG");
<a name="25084"></a>
<a name="25085"></a>// Join "[a]" and "test {TAG}" into a sequence "[a] test {TAG}"
<a name="25086"></a>seq1 = new RuleSequence(r1);
<a name="25088"></a>seq1.append(r2);
<a name="25089"></a>
<a name="25090"></a>// Create the sequence "another &lt;rule&gt;";
<a name="25091"></a>seq2 = new RuleSequence(new RuleToken("another"));
<a name="25093"></a>seq2.append(new RuleName("rule"));
<a name="25094"></a>
<a name="25095"></a>// Build "[a] test {TAG} | another &lt;rule&gt;"
<a name="25096"></a>test = new RuleAlternatives(seq1);
<a name="25097"></a>test.append(seq2);
<a name="25099"></a>
<a name="25100"></a>// Add &lt;test&gt; to the RuleGrammar as a public rule
<a name="25101"></a>gram.setRule("test", test, true);
<a name="27907"></a>
<a name="27908"></a>// Provide the definition of &lt;rule&gt;, a non-public RuleToken
<a name="27912"></a>gram.setRule("rule", new RuleToken("word"), false);
<a name="25102"></a>
<a name="25106"></a>// Commit the grammar changes
<a name="16820"></a>rec.commitChanges();


      <hr></pre>
    </dd>
   </dl> 
   <a name="17003"></a>
   <h3>6.5.4 &nbsp; &nbsp; Dynamic Grammars </h3> 
   <p><a name="34556"></a>Grammars may be modified and updated. The changes allow an application to account for shifts in the application's context, changes in the data available to it, and so on. This flexibility allows application developers considerable freedom in creating dynamic and natural speech interfaces. </p>
   <p><a name="26619"></a>For example, in an email application the list of known users may change during the normal operation of the program. The <code>&lt;sendEmail&gt;</code> command, </p>
   <dl>
    <dd>
     <pre><a name="26620"></a>&lt;sendEmail&gt; = send email to &lt;user&gt;;
</pre>
    </dd>
   </dl>
   <p><a name="16920"></a>references the <code>&lt;user&gt;</code> rule which may need to be changed as new email arrives. This code snippet shows the update and commit of a change in users. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="16938"></a>Recognizer rec;
<a name="16939"></a>RuleGrammar gram;
<a name="16949"></a>
<a name="16957"></a>String names[] = {"amy", "alan", "paul"};
<a name="16942"></a>Rule userRule = new RuleAlternatives(names);
<a name="16958"></a>
<a name="16959"></a>gram.setRule("user", userRule);
<a name="16966"></a>
<a name="34584"></a>// apply the changes
<a name="16971"></a>rec.commitChanges();


      <hr></pre>
    </dd>
   </dl>
   <p><a name="16972"></a>Committing grammar changes can, in certain cases, be a slow process. It might take a few tenths of seconds or up to several seconds. The time to commit changes depends on a number of factors. First, recognizers have different mechanisms for committing changes making some recognizers faster than others. Second, the time to commit changes may depend on the extent of the changes - more changes may require more time to commit. Thirdly, the time to commit may depend upon the type of changes. For example, some recognizers optimize for changes to lists of tokens (e.g. name lists). Finally, faster computers make changes more quickly. </p>
   <p><a name="34596"></a>The other factor which influences dynamic changes is the timing of the commit. As <a href="Recognition.html#32232">Section 6.4.2</a> describes, grammar changes are not always committed instantaneously. For example, if the recognizer is busy recognizing speech (in the <code>PROCESSING</code> state), then the commit of changes is deferred until the recognition of that speech is completed. </p> 
   <a name="14724"></a>
   <h3>6.5.5 &nbsp; &nbsp; Parsing </h3> 
   <p><a name="17508"></a>Parsing is the process of matching text to a grammar. Applications use parsing to break down spoken input into a form that is more easily handled in software. Parsing is most useful when the structure of the grammars clearly separates the parts of spoken text that an application needs to process. Examples are given below of this type of structuring. </p>
   <p><a name="34656"></a>The text may be in the form of a <code>String</code> or array of <code>String</code> objects (one <code>String</code> per token), or in the form of a <code>FinalRuleResult</code> object that represents what a recognizer heard a user say. The <code>RuleGrammar</code> interface defines three forms of the <code>parse</code> method - one for each form of text. </p>
   <p><a name="34606"></a>The <code>parse</code> method returns a <code>RuleParse</code> object (a descendent of <code>Rule</code>) that represents how the text matches the <code>RuleGrammar</code>. The structure of the <code>RuleParse</code> object mirrors the structure of rules defined in the <code>RuleGrammar</code>. Each <code>Rule</code> object in the structure of the rule being parsed against is mirrored by a matching <code>Rule</code> object in the returned <code>RuleParse</code> object. </p>
   <p><a name="34627"></a>The difference between the structures comes about because the text being parsed defines a single phrase that a user has spoken whereas a <code>RuleGrammar</code> defines all the phrases the user could say. Thus the text defines a single path through the grammar and all the choices in the grammar (alternatives, and rules that occur optionally or occur zero or more times) are resolvable. </p>
   <p><a name="34644"></a>The mapping between the objects in the rules defined in the <code>RuleGrammar</code> and the objects in the <code>RuleParse</code> structure is shown in <a href="Recognition.html#25399">Table 6-4</a>. Note that except for the <code>RuleCount</code> and <code>RuleName</code> objects, the object in the parse tree are of the same type as rule object being parsed against (marked with "**"), but the internal data may differ.</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="25399"></a>
     <b> Table 6-4 Matching Rule definitions and RuleParse objects </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="25403"></a><b>Object in definition </b> &nbsp;</td>
      <td><a name="25405"></a><b>Matching object in RuleParse </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="25407"></a><code>RuleToken </code> &nbsp;</td>
      <td><a name="25409"></a>Maps to an identical RuleToken object. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="25411"></a><code>RuleTag </code> &nbsp;</td>
      <td><a name="25413"></a>Maps to a RuleTag object with the same tag and with the contained rule mapped according to its rule type. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="25415"></a><code>RuleSequence </code> &nbsp;</td>
      <td><a name="25417"></a>Maps to a RuleSequence object with identical length and with each rule in the sequence mapped according to its rule type. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="25419"></a><code>RuleAlternatives </code> &nbsp;</td>
      <td><a name="25421"></a>Maps to a RuleAlternatives object containing a single item which is the one rule in the set of alternatives that was spoken. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="25423"></a><code>RuleCount ** </code> &nbsp;</td>
      <td><a name="25425"></a>Maps to a RuleSequence object containing an item for each time the rule contained by the RuleCOunt object is spoken. The sequence may have a length of zero, one or more. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="25427"></a><code>RuleName ** </code> &nbsp;</td>
      <td><a name="25429"></a>Maps to a RuleParse object with the name in the RuleName object being the fully-qualified version of the original rulename, and with the Rule object contained by the RuleParse object being an appropriate match of the definition of RuleName. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="25549"></a>As an example, take the following simple extract from a grammar. The public rule, <code>&lt;command&gt;</code>, may be spoken in many ways. For example, "open", "move that door" or "close that door please". </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="17512"></a>public &lt;command&gt; = &lt;action&gt; [&lt;object&gt;] [&lt;polite&gt;];
<a name="17549"></a>&lt;action&gt; = open {OP} | close {CL} | move {MV};
<a name="17561"></a>&lt;object&gt; = [&lt;this_that_etc&gt;] window | door;
<a name="17550"></a>&lt;this_that_etc&gt; = a | the | this | that | the current;
<a name="17520"></a>&lt;polite&gt; = please | kindly;


      <hr></pre>
    </dd>
   </dl>
   <p><a name="25545"></a>Note how the rules are defined to clearly separate the segments of spoken input that an application must process. Specifically, the <code>&lt;action&gt;</code> and <code>&lt;object&gt;</code> rules indicate how an application must respond to a command. Furthermore, anything said that matches the <code>&lt;polite&gt;</code> rule can be safely ignored, and usually the <code>&lt;this_that_etc&gt;</code> rule can be ignored too. </p>
   <p><a name="34665"></a>The parse for "open" against <code>&lt;command&gt;</code> has the following structure which matches the structure of the grammar above. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="25849"></a>RuleParse(&lt;command&gt; =
<a name="25850"></a>	RuleSequence(
<a name="25851"></a>		RuleParse(&lt;action&gt; =
<a name="25852"></a>			RuleAlternatives(
<a name="25853"></a>				RuleTag(
<a name="25854"></a>					RuleToken("open"), "OP")))))


      <hr></pre>
    </dd>
   </dl>
   <p><a name="25845"></a>The match of the <code>&lt;command&gt;</code> rule is represented by a <code>RuleParse</code> object. Because the definition of <code>&lt;command&gt;</code> is a sequence of 3 items (2 of which are optional), the parse of <code>&lt;command&gt;</code> is a sequence. Because only one of the 3 items is spoken (in "open"), the sequence contains a single item. That item is the parse of the <code>&lt;action&gt;</code> rule. </p>
   <p><a name="25887"></a>The reference to <code>&lt;action&gt;</code> in the definition of <code>&lt;command&gt;</code> is represented by a <code>RuleName</code> object in the grammar definition, and this maps to a <code>RuleParse</code> object when parsed. The <code>&lt;action&gt;</code> rule is defined by a set of three alternatives (<code>RuleAlternatives</code> object) which maps to another <code>RuleAlternatives</code> object in the parse but with only the single spoken alternative represented. Since the phrase spoken was "open", the parse matches the first of the three alternatives which is a tagged token. Therefore the parse includes a <code>RuleTag</code> object which contains a <code>RuleToken</code> object for "open". </p>
   <p><a name="25901"></a>The following is the parse for "close that door please". </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="25902"></a>RuleParse(&lt;command&gt; = 
<a name="25903"></a>	RuleSequence( 
<a name="25677"></a>		RuleParse(&lt;action&gt; = 
<a name="25678"></a>			RuleAlternatives( 
<a name="25686"></a>				RuleTag( 
<a name="25696"></a>					RuleToken("close"), "CL")))
<a name="26075"></a>		RuleSequence( 
<a name="25708"></a>			RuleParse(&lt;object&gt; = 
<a name="25714"></a>				RuleSequence( 
<a name="25722"></a>					RuleSequence( 
<a name="25732"></a>						RuleParse(&lt;this_that_etc&gt; = 
<a name="25744"></a>							RuleAlternatives( 
<a name="25758"></a>								RuleToken("that")))) 
<a name="25818"></a>					RuleAlternatives( 
<a name="25806"></a>						RuleToken("door")))) 
<a name="25800"></a>		RuleSequence( 
<a name="25635"></a>			RuleParse(&lt;polite&gt; = 
<a name="25789"></a>				RuleAlternatives( 
<a name="26096"></a>					RuleToken("please"))))
<a name="26093"></a>	))


      <hr></pre>
    </dd>
   </dl>
   <p><a name="17736"></a>There are three parsing issues that application developers should consider. </p>
   <ul>
    <li><a name="26111"></a>Parsing may fail because there is no legal match. In this instance the <code>parse</code> methods return <code>null</code>. </li>
   </ul> 
   <ul>
    <li><a name="26118"></a>There may be several legal ways to parse the text against the grammar. This is known as an <em>ambiguous</em> parse. In this instance the <code>parse</code> method will return one of the legal parses but the application is not informed of the ambiguity. As a general rule, most developers will want to avoid ambiguous parses by proper grammar design. Advanced applications will use specialized parsers if they need to handle ambiguity. </li>
   </ul> 
   <ul>
    <li><a name="26131"></a>If a <code>FinalRuleResult</code> is parsed against the <code>RuleGrammar</code> and the rule within that grammar that it matched, then it should successfully parse. However, it is not guaranteed to parse if the <code>RuleGrammar</code> has been modified of if the <code>FinalRuleResult</code> is a <code>REJECTED</code> result. (Result rejection is described in <a href="Recognition.html#20505">Section 6.7</a>.) </li>
   </ul> 
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="12454"></a>
   <h2>6.6 &nbsp; &nbsp; Dictation Grammars </h2> 
   <p><a name="34786"></a>Dictation grammars come closest to the ultimate goal of a speech recognition system that takes natural spoken input and transcribes it as text. Dictation grammars are used for free text entry in applications such as email and word processing. </p>
   <p><a name="34800"></a>A <code>Recognizer</code> that supports dictation provides a single <code>DictationGrammar</code> which is obtained from the recognizer's <code>getDictationGrammar</code> method. A recognizer that supports the Java Speech API is not required to provide a <code>DictationGrammar</code>. Applications that require a recognizer with dictation capability can explicitly request dictation when creating a recognizer by setting the <code>DictationGrammarSupported</code> property of the <code>RecognizerModeDesc</code> to true (see <a href="SpeechEngine.html#9171">Section 4.2</a> for details). </p>
   <p><a name="34835"></a>A <code>DictationGrammar</code> is more complex than a rule grammar, but fortunately, a <code>DictationGrammar</code> is often easier to use than an rule grammar. This is because the <code>DictationGrammar</code> is built into the recognizer so most of the complexity is handled by the recognizer and hidden from the application. However, recognition of a dictation grammar is typically more computationally expensive and less accurate than that of simple rule grammars. </p>
   <p><a name="17798"></a>The <code>DictationGrammar</code> inherits its basic functionality from the <code>Grammar</code> interface. That functionality is detailed in <a href="Recognition.html#12452">Section 6.4</a> and includes grammar naming, enabling, activation, committing and so on. </p>
   <p><a name="27220"></a>As with all grammars, changes to a <code>DictationGrammar</code> need to be committed before they take effect. Commits are described in <a href="Recognition.html#32232">Section 6.4.2</a>. </p>
   <p><a name="27235"></a>In addition to the specific functionality described below, a <code>DictationGrammar</code> is typically adaptive. In an adaptive system, a recognizer improves its performance (accuracy and possibly speed) by adapting to the style of language used by a speaker. The recognizer may adapt to the specific sounds of a speaker (the way they say words). Equally importantly for dictation, a recognizer can adapt to a user's normal vocabulary and to the patterns of those words. Such adaptation (technically known as language model adaptation) is a part of the recognizer's implementation of the <code>DictationGrammar</code> and does not affect an application. The adaptation data for a dictation grammar is maintained as part of a speaker profile (see <a href="Recognition.html#13004">Section 6.9</a>). </p>
   <p><a name="12513"></a>The <code>DictationGrammar</code> extends and specializes the <code>Grammar</code> interface by adding the following functionality: </p>
   <ul>
    <li><a name="27216"></a>Indication of the current textual context, </li>
   </ul> 
   <ul>
    <li><a name="17847"></a>Control of word lists. </li>
   </ul> 
   <p><a name="27186"></a>The following methods provided by the DictationGrammar interface allow an application to manage word lists and text context.</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="27076"></a>
     <b> Table 6-5 DictationGrammar interface methods </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="27080"></a><b>Name </b> &nbsp;</td>
      <td><a name="27082"></a><b>Description </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="27262"></a><code>setContext </code> &nbsp;</td>
      <td><a name="27264"></a>Provide the recognition engine with the preceding and following textual context. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="27084"></a><code>addWord </code> &nbsp;</td>
      <td><a name="27086"></a>Add a word to the DictationGrammar. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="27204"></a><code>removeWord </code> &nbsp;</td>
      <td><a name="27206"></a>Remove a word from the DictationGrammar. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="27088"></a><code>listAddedWords </code> &nbsp;</td>
      <td><a name="27090"></a>List the words that have been added to the DictationGrammar. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="27092"></a><code>listRemovedWords </code> &nbsp;</td>
      <td><a name="27094"></a>List the words that have been removed from the DictationGrammar. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p> 
   <a name="26774"></a>
   <h3>6.6.1 &nbsp; &nbsp; Dictation Context </h3> 
   <p><a name="26775"></a>Dictation recognizers use a range of information to improve recognition accuracy. Learning the words a user speaks and the patterns of those words can substantially improve accuracy. </p>
   <p><a name="26779"></a>Because patterns of words are important, <em>context</em> is important. The context of a word is simply the set of surrounding words. As an example, consider the following sentence <em>"If I have seen further it is by standing on the shoulders of Giants"</em> (Sir Isaac Newton). If we are editing this sentence and place the cursor after the word "<em>standing"</em> then the preceding context is <em>"...further it is by standing"</em> and the following context is <em>"on the shoulders of Giants..."</em>. </p>
   <p><a name="38538"></a>Given this context, the recognizer is able to more reliably predict what a user might say, and greater predictability can improve recognition accuracy. In this example, the user might insert the word <em>"up"</em> but is less likely to insert the word <em>"JavaBeans"</em>. </p>
   <p><a name="26788"></a>Through the <code>setContext</code> method of the <code>DictationGrammar</code> interface, an application should tell the recognizer the current textual context. Furthermore, if the context changes (for example, due to a mouse click to move the cursor) the application should update the context. </p>
   <p><a name="26805"></a>Different recognizers process context differently. The main consideration for the application is the amount of context to provide to the recognizer. As a minimum, a few words of preceding and following context should be provided. However, some recognizers may take advantage of several paragraphs or more. </p>
   <p><a name="26816"></a>There are two <code>setContext</code> methods: </p>
   <dl>
    <dd>
     <pre><a name="26820"></a>void setContext(String preceding, String following);<br>
void setContext(String preceding[], String following[]);
</pre>
    </dd>
   </dl>
   <p><a name="26821"></a>The first form takes plain text context strings. The second version should be used when the result tokens returned by the recognizer are available. Internally, the recognizer processes context according to tokens so providing tokens makes the use of context more efficient and more reliable because it does not have to guess the tokenization. </p>
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="20505"></a>
   <h2>6.7 &nbsp; &nbsp; Recognition Results </h2> 
   <p><a name="20506"></a>A recognition <em>result</em> is provided by a <code>Recognizer</code> to an application when the recognizer "hears" incoming speech that matches an active grammar. The result tells the application what words the user said and provides a range of other useful information, including alternative guesses and audio data. </p>
   <p><a name="35400"></a>In this section, both the basic and advanced capabilities of the result system in the Java Speech API are described. The sections relevant to basic rule grammar-based applications are those that cover result finalization (<a href="Recognition.html#38541">Section 6.7.1</a>), the hierarchy of result interfaces (<a href="Recognition.html#35662">Section 6.7.2</a>), the data provided through those interfaces (<a href="Recognition.html#37970">Section 6.7.3</a>), and common techniques for handling finalized rule results (<a href="Recognition.html#18915">Section 6.7.9</a>). </p>
   <p><a name="38406"></a>For dictation applications the relevant sections include those listed above plus the sections covering token finalization (<a href="Recognition.html#36731">Section 6.7.8</a>), handling of finalized dictation results (<a href="Recognition.html#19563">Section 6.7.10</a>) and result correction and training (<a href="Recognition.html#37704">Section 6.7.12</a>). </p>
   <p><a name="38404"></a>For more advanced applications relevant sections might include the result life cycle (<a href="Recognition.html#35736">Section 6.7.4</a>), attachment of ResultListeners (<a href="Recognition.html#35135">Section 6.7.5</a>), the relationship of recognizer and result states (<a href="Recognition.html#36801">Section 6.7.6</a>), grammar finalization (<a href="Recognition.html#37327">Section 6.7.7</a>), result audio (<a href="Recognition.html#28834">Section 6.7.11</a>), rejected results (<a href="Recognition.html#37935">Section 6.7.13</a>), result timing (<a href="Recognition.html#12857">Section 6.7.14</a>), and the loading and storing of vendor formatted results (<a href="Recognition.html#23002">Section 6.7.15</a>). </p> 
   <a name="38541"></a>
   <h3>6.7.1 &nbsp; &nbsp; Result Finalization </h3> 
   <p><a name="35067"></a>The <em><a href="Recognition.html#11644">"Hello World!"</a></em> example illustrates the simplest way to handle results. In that example, a <code>RuleGrammar</code> was loaded, committed and enabled, and a <code>ResultListener</code> was attached to a <code>Recognizer</code> to receive events associated with every result that matched that grammar. In other words, the <code>ResultListener</code> was attached to receive information about words spoken by a user that is heard by the recognizer. </p>
   <p><a name="35298"></a>The following is a modified extract of the <em><a href="Recognition.html#11644">"Hello World!"</a></em> example to illustrate the basics of handling results. In this case, a <code>ResultListener</code> is attached to a <code>Grammar</code> (instead of a <code>Recognizer</code>) and it prints out every thing the recognizer hears that matches that grammar. (There are, in fact, three ways in which a <code>ResultListener</code> can be attached: see <a href="Recognition.html#35135">Section 6.7.5</a>.) </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="35209"></a>import javax.speech.*;
<a name="35210"></a>import javax.speech.recognition.*;
<a name="35213"></a>
<a name="35214"></a>public class MyResultListener extends ResultAdapter {
<a name="35217"></a>	// Receives RESULT_ACCEPTED event: print it
<a name="35218"></a>	public void resultAccepted(ResultEvent e) {
<a name="35219"></a>		Result r = (Result)(e.getSource());
<a name="35220"></a>		ResultToken tokens[] = r.getBestTokens();
<a name="35221"></a>
<a name="35222"></a>		for (int i = 0; i &lt; tokens.length; i++)
<a name="35223"></a>			System.out.print(tokens[i].getSpokenText() + " ");
<a name="35224"></a>		System.out.println();
<a name="35229"></a>	}
<a name="35230"></a>
<a name="35311"></a>	// somewhere in app, add a ResultListener to a grammar
<a name="35271"></a>	{
<a name="35242"></a>		RuleGrammar gram = ...;
<a name="35246"></a>		gram.addResultListener(new MyResultListener());
<a name="35257"></a>	}
<a name="35258"></a>}


      <hr></pre>
    </dd>
   </dl>
   <p><a name="35068"></a>The code shows the <code>MyResultListener</code> class which is as an extension of the <code>ResultAdapter</code> class. The <code>ResultAdapter</code> class is a convenience implementation of the <code>ResultListener</code> interface (provided in the <code>javax.speech.recognition</code> package). When extending the <code>ResultAdapter</code> class we simply implement the methods for the events that we care about. </p>
   <p><a name="35306"></a>In this case, the <code>RESULT_ACCEPTED</code> event is handled. This event is issued to the <code>resultAccepted</code> method of the <code>ResultListener</code> and is issued when a result is <em>finalized</em>. Finalization of a result occurs after a recognizer completed processing of a result. More specifically, finalization occurs when all information about a result has been produced by the recognizer and when the recognizer can guarantee that the information will not change. (Result finalization should not be confused with object finalization in the Java programming language in which objects are cleaned up before garbage collection.) </p>
   <p><a name="35069"></a>There are actually two ways to finalize a result which are signalled by the <code>RESULT_ACCEPTED</code> and <code>RESULT_REJECTED</code> events. A result is accepted when a recognizer is confidently that it has correctly heard the words spoken by a user (i.e., the tokens in the <code>Result</code> exactly represent what a user said). </p>
   <p><a name="35387"></a>Rejection occurs when a <code>Recognizer</code> is not confident that it has correctly recognized a result: that is, the tokens and other information in the result do not necessarily match what a user said. Many applications will ignore the <code>RESULT_REJECTED</code> event and most will ignore the detail of a result when it is rejected. In some applications, a <code>RESULT_REJECTED</code> event is used simply to provide users with feedback that something was heard but no action was taken, for example, by displaying "???" or sounding an error beep. Rejected results and the differences between accepted and rejected results are described in more detail in <a href="Recognition.html#37935">Section 6.7.13</a> . </p>
   <p><a name="35158"></a>An accepted result is not necessarily a correct result. As is pointed out in <a href="TechOverview.html#7806">Section 2.2.3</a>, recognizers make errors when recognizing speech for a range of reasons. The implication is that even for an accepted result, application developers should consider the potential impact of a misrecognition. Where a misrecognition could cause an action with serious consequences or could make changes that can't be undone (e.g., "delete all files"), the application should check with users before performing the action. As recognition systems continue to improve the number of errors is steadily decreasing, but as with human speech recognition there will always be a chance of a misunderstanding. </p> 
   <a name="35662"></a>
   <h3>6.7.2 &nbsp; &nbsp; Result Interface Hierarchy </h3> 
   <p><a name="35663"></a>A finalized result can include a considerable amount of information. This information is provided through four separate interfaces and through the implementation of these interfaces by a recognition system. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="35669"></a>// Result: the root result interface
<a name="35670"></a>interface Result;
<a name="35671"></a>
<a name="35682"></a>// FinalResult: info on all finalized results
<a name="35672"></a>interface FinalResult extends Result;
<a name="35685"></a>
<a name="35689"></a>// FinalRuleResult: a finalized result matching a RuleGrammar
<a name="35686"></a>interface FinalRuleResult extends FinalResult;
<a name="35673"></a>
<a name="35697"></a>// FinalDictationResult: a final result for a DictationGrammar
<a name="35698"></a>interface FinalDictationResult extends FinalResult;
<a name="35683"></a>
<a name="35702"></a>// A result implementation provided by a Recognizer
<a name="35703"></a>public class EngineResult
<a name="35679"></a>			implements FinalRuleResult, FinalDictationResult;


      <hr></pre>
    </dd>
   </dl>
   <p><a name="35878"></a>At first sight, the result interfaces may seem complex. The reasons for providing several interfaces are as follows: </p>
   <ul>
    <li><a name="35885"></a>The information available for a result is different in different states of the result. Before finalization, a limited amount of information is available through the <code>Result</code> interface. Once a result is finalized (accepted or rejected), more detailed information is available through the <code>FinalResult</code> interface and either the <code>FinalRuleResult</code> or <code>FinalDictationResult</code> interface. </li>
   </ul> 
   <ul>
    <li><a name="35906"></a>The type of information available for a finalized result is different for a result that matches a <code>RuleGrammar</code> than for a result that matches a <code>DictationGrammar</code>. The differences are explicitly represented by having separate interfaces for <code>FinalRuleResult</code> and <code>FinalDictationResult</code>. </li>
   </ul> 
   <ul>
    <li><a name="35934"></a>Once a result object is created as a specific Java class it cannot change be changed to another class. Therefore, because a result object must eventually support the final interface it must implement them when first created. Therefore, every result implements all three final interfaces when it is first created: <code>FinalResult</code>, <code>FinalRuleResult</code> and <code>FinalDictationResult</code>. </li>
   </ul> 
   <ul>
    <li><a name="35931"></a>When a result is first created a recognizer does not always know whether it will eventually match a <code>RuleGrammar</code> or a <code>DictationGrammar</code>. Therefore, every result object implements both the <code>FinalRuleResult</code> and <code>FinalDictationResult</code> interfaces. </li>
   </ul> 
   <ul>
    <li><a name="35974"></a>A call made to any method of any of the final interfaces before a result is finalized causes a <code>ResultStateException</code>. </li>
   </ul> 
   <ul>
    <li><a name="35976"></a>A call made to any method of the <code>FinalRuleResult</code> interface for a result that matches a <code>DictationGrammar</code> causes a <code>ResultStateException</code>. Similarly, a call made to any method of the <code>FinalDictationResult</code> interface for a result that matches a <code>RuleGrammar</code> causes a <code>ResultStateException</code>. </li>
   </ul> 
   <ul>
    <li><a name="35909"></a>All the result functionality is provided by interfaces in the <code>java.speech.recognition</code> package rather than by classes. This is because the Java Speech API can support multiple recognizers from multiple vendors and interfaces allow the vendors greater flexibility in implementing results. </li>
   </ul> 
   <p><a name="35882"></a>The multitude of interfaces is, in fact, designed to simplify application programming and to minimize the chance of introducing bugs into code by allowing compile-time checking of result calls. The two basic principles for calling the result interfaces are the following: </p>
   <ol> 
    <li><a name="35879"></a>If it is safe to call the methods of a particular interface then it is safe to call the methods of any of the parent interfaces. For example, for a finalized result matching a <code>RuleGrammar</code>, the methods of the <code>FinalRuleResult</code> interface are safe, so the methods of the <code>FinalResult</code> and <code>Result</code> interfaces are also safe. Similarly, for a finalized result matching a <code>DictationGrammar</code>, the methods of <code>FinalDictationResult</code>, <code>FinalResult</code> and <code>Result</code> can all be called safely. </li>
    <li><a name="36040"></a>Use type casting of a result object to ensure compile-time checks of method calls. For example, in events to an unfinalized result, cast the result object to the <code>Result</code> interface. For a <code>RESULT_ACCEPTED</code> finalization event with a result that matches a <code>DictationGrammar</code>, cast the result to the <code>FinalDictationResult</code> interface. </li>
   </ol> 
   <p><a name="36005"></a>In the next section the different information available through the different interfaces is described. In all the following sections that deal with result states and result events, details are provided on the appropriate casting of result objects. </p> 
   <a name="35877"></a>
   <h3>6.7.3 &nbsp; &nbsp; Result Information </h3> 
   <p><a name="35876"></a>As the previous section describes, different information is available for a result depending upon the state of the result and, for finalized results, depending upon the type of grammar it matches (<code>RuleGrammar</code> or <code>DictationGrammar</code>). </p> 
   <a name="36235"></a>
   <h4>6.7.3.1 &nbsp; &nbsp; Result Interface </h4> 
   <p><a name="35868"></a>The information available through the <code>Result</code> interface is available for any result in any state - finalized or unfinalized - and matching any grammar. </p>
   <ul>
    <li><a name="35869"></a><em>Result state</em>: The <code>getResultState</code> method returns the current state of the result. The three possible state values defined by static values of the <code>Result</code> interface are <code>UNFINALIZED</code>, <code>ACCEPTED</code> and <code>REJECTED</code>. (Result states are described in more detail in <a href="Recognition.html#35736">Section 6.7.4</a>.) </li>
   </ul> 
   <ul>
    <li><a name="36106"></a><em>Grammar</em>: The <code>getGrammar</code> method returns a reference to the matched <code>Grammar</code>, if it is known. For an <code>ACCEPTED</code> result, this method will return a <code>RuleGrammar</code> or a <code>DictationGrammar</code>. For a <code>REJECTED</code> result, this method may return a grammar, or may return <code>null</code> if the recognizer could not identify the grammar for this result. In the <code>UNFINALIZED</code> state, this method returns <code>null</code> before a <code>GRAMMAR_FINALIZED</code> event, and non-null afterwards. </li>
   </ul> 
   <ul>
    <li><a name="36110"></a><em>Number of finalized tokens</em>: The <code>numTokens</code> method returns the total number of finalized tokens for a result. For an unfinalized result this may be zero or greater. For a finalized result this number is always greater than zero for an <code>ACCEPTED</code> result but may be zero or more for a <code>REJECTED</code> result. Once a result is finalized this number will not change. </li>
   </ul> 
   <ul>
    <li><a name="36148"></a><em>Finalized tokens</em>: The <code>getBestToken</code> and <code>getBestTokens</code> methods return either a specified finalized best-guess token of a result or all the finalized best-guess tokens. The <code>ResultToken</code> object and token finalization are described in the following sections. </li>
   </ul> 
   <ul>
    <li><a name="36152"></a><em>Unfinalized tokens</em>: In the <code>UNFINALIZED</code> state, the <code>getUnfinalizedTokens</code> method returns a list of unfinalized tokens. An unfinalized token is a recognizer's current guess of what a user has said, but the recognizer may choose to change these tokens at any time and any way. For a finalized result, the <code>getUnfinalizedTokens</code> method always returns <code>null</code>. </li>
   </ul> 
   <p><a name="36174"></a>In addition to the information detailed above, the <code>Result</code> interface provides the <code>addResultListener</code> and <code>removeResultListener</code> methods which allow a <code>ResultListener</code> to be attached to and removed from an individual result. <code>ResultListener</code> attachment is described in more detail in <a href="Recognition.html#35135">Section 6.7.5</a>. </p> 
   <a name="36237"></a>
   <h4>6.7.3.2 &nbsp; &nbsp; FinalResult Interface </h4> 
   <p><a name="36196"></a>The information available through the <code>FinalResult</code> interface is available for any finalized result, including results that match either a <code>RuleGrammar</code> or <code>DictationGrammar</code>. </p>
   <ul>
    <li><a name="36193"></a><em>Audio data</em>: a <code>Recognizer</code> may optionally provide audio data for a finalized result. This data is provided as <code>AudioClip</code> for a token, a sequence of tokens, or for the entire result. Result audio and its management are described in more detail in <a href="Recognition.html#28834">Section 6.7.11</a>. </li>
   </ul> 
   <ul>
    <li><a name="36230"></a><em>Training data</em>: many recognizer's have the ability to be trained and corrected. By training a recognizer or correcting its mistakes, a recognizer can adapt its recognition processes so that performance (accuracy and speed) improve over time. Several methods of the FinalResult interface support this capability and are described in detail in <a href="Recognition.html#37704">Section 6.7.12</a>. </li>
   </ul> 
   <a name="36241"></a>
   <h4>6.7.3.3 &nbsp; &nbsp; FinalDictationResult Interface </h4> 
   <p><a name="36204"></a>The <code>FinalDictationResult</code> interface contains a single method. </p>
   <ul>
    <li><a name="36205"></a><em>Alternative tokens</em>: The <code>getAlternativeTokens</code> method allows an application to request a set of alternative guesses for a single token or for a sequence of tokens in that result. In dictation systems, alternative guesses are typically used to facilitate correction of dictated text. Dictation recognizers are designed so that when they do make a misrecognition, the correct word sequence is usually amongst the best few alternative guesses. <a href="Recognition.html#19563">Section 6.7.10</a> </li>
   </ul> 
   <a name="36246"></a>
   <h4>6.7.3.4 &nbsp; &nbsp; FinalRuleResult Interface </h4> 
   <p><a name="36247"></a>Like the <code>FinalDictationResult</code> interface, the <code>FinalRuleResult</code> interface provides alternative guesses. The <code>FinalRuleResult</code> interface also provides some additional information that is useful in processing results that match a <code>RuleGrammar</code>. </p>
   <ul>
    <li><a name="36311"></a><em>Alternative tokens</em>: The <code>getAlternativeTokens</code> method allows an application to request a set of alternative guesses for the entire result (not for tokens). The <code>getNumberGuesses</code> method returns the actual number of alternative guesses available. </li>
   </ul> 
   <ul>
    <li><a name="36340"></a><em>Alternative grammars</em>: The alternative guesses of a result matching a <code>RuleGrammar</code> do not all necessarily match the same grammar. The <code>getRuleGrammar</code> method returns a reference to the <code>RuleGrammar</code> matched by an alternative. </li>
   </ul> 
   <ul>
    <li><a name="36344"></a><em>Rulenames</em>: When a result matches a <code>RuleGrammar</code>, it matches a specific defined rule of that <code>RuleGrammar</code>. The <code>getRuleName</code> method returns the rulename for the matched rule. <a href="Recognition.html#18915">Section 6.7.9</a> <code>RuleGrammar</code> results. </li>
   </ul> 
   <ul>
    <li><a name="36359"></a><em>Tags</em>: A tag is a string attached to a component of a <code>RuleGrammar</code> definition. Tags are useful in simplifying the software for processing results matching a <code>RuleGrammar</code> (explained in <a href="Recognition.html#18915">Section 6.7.9</a>). The <code>getTags</code> method returns the tags for the best guess for a <code>FinalRuleResult</code>. </li>
   </ul> 
   <a name="35736"></a>
   <h3>6.7.4 &nbsp; &nbsp; Result Life Cycle </h3> 
   <p><a name="35200"></a>A <code>Result</code> is produced in response to a user's speech. Unlike keyboard input, mouse input and most other forms of user input, speech is not instantaneous (see <a href="Recognition.html#33418">Section 6.3.3.1</a> for more detail). As a consequence, a speech recognition result is not produced instantaneously. Instead, a <code>Result</code> is produced through a sequence of events starting some time after a user starts speaking and usually finishing some time after the user stops speaking. </p>
   <p><a name="36440"></a><a href="Recognition.html#36440">Figure 6-2</a> shows the state system of a <code>Result</code> and the associated <code>ResultEvents</code>. As in the recognizer state diagram (<a href="Recognition.html#32810">Figure 6-1</a>), the blocks represent states, and the labelled arcs represent transitions that are signalled by <code>ResultEvents</code>.<img src="Recognition2.gif"> </p>
   <p><a name="35461"></a>Every result starts in the <code>UNFINALIZED</code> state when a <code>RESULT_CREATED</code> event is issued. While unfinalized, the recognizer provides information including finalized and unfinalized tokens and the identity of the grammar matched by the result. As this information is added, the <code>RESULT_UPDATED</code> and <code>GRAMMAR_FINALIZED</code> events are issued </p>
   <p><a name="36643"></a>Once all information associated with a result is finalized, the entire result is finalized. As <a href="Recognition.html#38541">Section 6.7.1</a> explained, a result is finalized with either a <code>RESULT_ACCEPTED</code> or <code>RESULT_REJECTED</code> event placing it in either the <code>ACCEPTED</code> or <code>REJECTED</code> state. At that point all information associated with the result becomes available including the best guess tokens and the information provided through the three final result interfaces (see <a href="Recognition.html#37970">Section 6.7.3</a>). </p>
   <p><a name="36691"></a>Once finalized the information available through all the result interfaces is fixed. The only exceptions are for the release of audio data and training data. If audio data is released, an <code>AUDIO_RELEASED</code> event is issued (see detail in <a href="Recognition.html#28834">Section 6.7.11</a>). If training information is released, an <code>TRAINING_INFO_RELEASED</code> event is issued (see detail in <a href="Recognition.html#37704">Section 6.7.12</a>). </p>
   <p><a name="36642"></a>Applications can track result states in a number of ways. Most often, applications handle result in <code>ResultListener</code> implementation which receives <code>ResultEvents</code> as recognition proceeds. </p>
   <p><a name="36632"></a>As <a href="Recognition.html#37970">Section 6.7.3</a> explains, a recognizer conveys a range of information to an application through the stages of producing a recognition result. However, as the example in <a href="Recognition.html#38541">Section 6.7.1</a> shows, many applications only care about the last step and event in that process - the <code>RESULT_ACCEPTED</code> event. </p>
   <p><a name="35650"></a>The state of a result is also available through the <code>getResultState</code> method of the <code>Result</code> interface. That method returns one of the three result states: <code>UNFINALIZED</code>, <code>ACCEPTED</code> or <code>REJECTED</code>. </p> 
   <a name="35135"></a>
   <h3>6.7.5 &nbsp; &nbsp; ResultListener Attachment </h3> 
   <p><a name="36487"></a>A <code>ResultListener</code> can be attached in one of three places to receive events associated with results: to a <code>Grammar</code>, to a <code>Recognizer</code> or to an individual <code>Result</code>. The different places of attachment give an application some flexibility in how they handle results. </p>
   <p><a name="36511"></a>To support <code>ResultListeners</code> the <code>Grammar</code>, <code>Recognizer</code> and <code>Result</code> interfaces all provide the <code>addResultListener</code> and <code>removeResultListener</code> methods. </p>
   <p><a name="36540"></a>Depending upon the place of attachment a listener receives events for different results and different subsets of result events. </p>
   <ul>
    <li><a name="36464"></a><code>Grammar</code>: A <code>ResultListener</code> attached to a <code>Grammar</code> receives all <code>ResultEvents</code> for any result that has been finalized to match that grammar. Because the grammar is known once a <code>GRAMMAR_FINALIZED</code> event is produced, a <code>ResultListener</code> attached to a <code>Grammar</code> receives that event and subsequent events. Since grammars are usually defined for specific functionality it is common for most result handling to be done in the methods of listeners attached to each grammar. </li>
   </ul> 
   <ul>
    <li><a name="36581"></a><code>Result</code>: A <code>ResultListener</code> attached to a <code>Result</code> receives all <code>ResultEvents</code> starting at the time at which the listener is attached to the <code>Result</code>. Note that because a listener cannot be attached until a result has been created with the <code>RESULT_CREATED</code> event, it can never receive that event. </li>
   </ul> 
   <ul>
    <li><a name="36585"></a><code>Recognizer</code>: A <code>ResultListener</code> attached to a <code>Recognizer</code> receives all <code>ResultEvents</code> for all results produced by that <code>Recognizer</code> for all grammars. This form of listener attachment is useful for very simple applications (e.g., <em><a href="Recognition.html#11644">"Hello World!"</a></em>) and when centralized processing of results is required. Only <code>ResultListeners</code> attached to a <code>Recognizer</code> receive the <code>RESULT_CREATED</code> event. </li>
   </ul> 
   <a name="36801"></a>
   <h3>6.7.6 &nbsp; &nbsp; Recognizer and Result States </h3> 
   <p><a name="36794"></a>The state system of a recognizer is tied to the processing of a result. Specifically, the <code>LISTENING</code>, <code>PROCESSING</code> and <code>SUSPENDED</code> state cycle described in <a href="Recognition.html#33018">Section 6.3.3</a> and shown in <a href="Recognition.html#32810">Figure 6-1</a> follows the production of an event. </p>
   <p><a name="36795"></a>The transition of a <code>Recognizer</code> from the <code>LISTENING</code> state to the <code>PROCESSING</code> state with a <code>RECOGNIZER_PROCESSING</code> event indicates that a recognizer has started to produce a result. The <code>RECOGNIZER_PROCESSING</code> event is followed by the <code>RESULT_CREATED</code> event to <code>ResultListeners</code>. </p>
   <p><a name="36919"></a>The <code>RESULT_UPDATED</code> and <code>GRAMMAR_FINALIZED</code> events are issued to <code>ResultListeners</code> while the recognizer is in the <code>PROCESSING</code> state. </p>
   <p><a name="36928"></a>As soon as the recognizer completes recognition of a result, it makes a transition from the <code>PROCESSING</code> state to the <code>SUSPENDED</code> state with a <code>RECOGNIZER_SUSPENDED</code> event. Immediately following that recognizer event, the result finalization event (either <code>RESULT_ACCEPTED</code> or <code>RESULT_REJECTED</code>) is issued. While the result finalization event is processed, the recognizer remains suspended. Once result finalization event is completed, the recognizer automatically transitions from the <code>SUSPENDED</code> state back to the <code>LISTENING</code> state with a <code>CHANGES_COMMITTED</code> event. Once back in the <code>LISTENING</code> state the recognizer resumes processing of audio input with the grammar committed with the <code>CHANGES_COMMITTED</code> event. </p> 
   <a name="36814"></a>
   <h4>6.7.6.1 &nbsp; &nbsp; Updating Grammars </h4> 
   <p><a name="36815"></a>In many applications, grammar definitions and grammar activation need to be updated in response to spoken input from a user. For example, if speech is added to a traditional email application, the command "save this message" might result in a window being opened in which a mail folder can be selected. While that window is open, the grammars that control that window need to be activated. Thus during the event processing for the "save this message" command grammars may need be created, updated and enabled. All this would happen during processing of the <code>RESULT_ACCEPTED</code> event. </p>
   <p><a name="36816"></a>For any grammar changes to take effect they must be committed (see <a href="Recognition.html#32232">Section 6.4.2</a>). Because this form of grammar update is so common while processing the <code>RESULT_ACCEPTED</code> event (and sometimes the <code>RESULT_REJECTED</code> event), recognizers implicitly commit grammar changes after either result finalization event has been processed. </p>
   <p><a name="36855"></a>This implicit is indicated by the <code>CHANGES_COMMITTED</code> event that is issued when a Recognizer makes a transition from the <code>SUSPENDED</code> state to the <code>LISTENING</code> state following result finalization and the result finalization event processing (see <a href="Recognition.html#33018">Section 6.3.3</a> for details). </p>
   <p><a name="36867"></a>One desirable effect of this form of commit becomes useful in component systems. If changes in multiple components are triggered by a finalized result event, and if many of those components change grammars, then they do not each need to call the <code>commitChanges</code> method. The downside of multiple calls to the <code>commitChanges</code> method is that a syntax check be performed upon each. Checking syntax can be computationally expensive and so multiple checks are undesirable. With the implicit commit once all components have updated grammars computational costs are reduced. </p> 
   <a name="37327"></a>
   <h3>6.7.7 &nbsp; &nbsp; Grammar Finalization </h3> 
   <p><a name="37332"></a>At any time during processing a result a <code>GRAMMAR_FINALIZED</code> event can be issued for that result indicating the <code>Grammar</code> matched by the result has been determined. This event is issued is issued only once. It is required for any <code>ACCEPTED</code> result, but is optional for result that is eventually rejected. </p>
   <p><a name="37355"></a>As <a href="Recognition.html#35135">Section 6.7.5</a> describes, the <code>GRAMMAR_FINALIZED</code> event is the first event received by a <code>ResultListener</code> attached to a <code>Grammar</code>. </p>
   <p><a name="37359"></a>The <code>GRAMMAR_FINALIZED</code> event behaves the same for results that match either a <code>RuleGrammar</code> or a <code>DictationGrammar</code>. </p>
   <p><a name="37380"></a>Following the <code>GRAMMAR_FINALIZED</code> event, the <code>getGrammar</code> method of the <code>Result</code> interface returns a non-null reference to the matched grammar. By issuing a <code>GRAMMAR_FINALIZED</code> event the <code>Recognizer</code> guarantees that the <code>Grammar</code> will not change. </p>
   <p><a name="37381"></a>Finally, the <code>GRAMMAR_FINALIZED</code> event does not change the result's state. A <code>GRAMMAR_FINALIZED</code> event is issued only when a result is in the <code>UNFINALIZED</code> state, and leaves the result in that state. </p> 
   <a name="36731"></a>
   <h3>6.7.8 &nbsp; &nbsp; Token Finalization </h3> 
   <p><a name="37008"></a>A result is a dynamic object a it is being recognized. One way in which a result can be dynamic is that tokens are updated and finalized as recognition of speech proceeds. The result events allow a recognizer to inform an application of changes in the either or both the finalized and unfinalized tokens of a result. </p>
   <p><a name="37014"></a>The finalized and unfinalized tokens can be updated on any of the following result event types: <code>RESULT_CREATED</code>, <code>RESULT_UPDATED</code>, <code>RESULT_ACCEPTED</code>, <code>RESULT_REJECTED</code>. </p>
   <p><a name="37025"></a>Finalized tokens are accessed through the <code>getBestTokens</code> and <code>getBestToken</code> methods of the <code>Result</code> interface. The unfinalized tokens are accessed through the <code>getUnfinalizedTokens</code> method of the <code>Result</code> interface. (See <a href="Recognition.html#37970">Section 6.7.3</a> for details.) </p>
   <p><a name="36752"></a>A finalized token is a <code>ResultToken</code> in a <code>Result</code> that has been recognized in the incoming speech as matching a grammar. Furthermore, when a recognizer finalizes a token it indicates that it will not change the token at any point in the future. The <code>numTokens</code> method returns the number of finalized tokens. </p>
   <p><a name="36753"></a>Many recognizers do not finalize tokens until recognition of an entire result is complete. For these recognizers, the <code>numTokens</code> method returns zero for a result in the <code>UNFINALIZED</code> state. </p>
   <p><a name="36754"></a>For recognizers that do finalize tokens while a <code>Result</code> is in the <code>UNFINALIZED</code> state, the following conditions apply: </p>
   <ul>
    <li><a name="36755"></a>The <code>Result</code> object may contain zero or more finalized tokens when the <code>RESULT_CREATED</code> event is issued. </li>
   </ul> 
   <ul>
    <li><a name="36756"></a>The recognizer issues <code>RESULT_UPDATED</code> events to the <code>ResultListener</code> during recognition each time one or more tokens are finalized. </li>
   </ul> 
   <ul>
    <li><a name="36757"></a>Tokens are finalized strictly in the order in which they are spoken (i.e., left to right in English text). </li>
   </ul> 
   <p><a name="36758"></a>A result in the <code>UNFINALIZED</code> state may also have unfinalized tokens. An unfinalized token is a token that the recognizer has heard, but which it is not yet ready to finalize. Recognizers are not required to provide unfinalized tokens, and applications can safely choose to ignore unfinalized tokens. </p>
   <p><a name="37107"></a>For recognizers that provide unfinalized tokens, the following conditions apply: </p>
   <ul>
    <li><a name="37108"></a>The <code>Result</code> object may contain zero or more unfinalized tokens when the <code>RESULT_CREATED</code> event is issued. </li>
   </ul> 
   <ul>
    <li><a name="37109"></a>The recognizer issues <code>RESULT_UPDATED</code> events to the <code>ResultListener</code> during recognition each time the unfinalized tokens change. </li>
   </ul> 
   <ul>
    <li><a name="37127"></a>For an unfinalized result, unfinalized tokens may be updated at any time and in any way. Importantly, the number of unfinalized tokens may increase, decrease or return to zero and the values of those tokens may change in any way the recognizer chooses. </li>
   </ul> 
   <ul>
    <li><a name="37156"></a>Unfinalized tokens always represent a guess for the speech following the finalized tokens. </li>
   </ul> 
   <p><a name="37157"></a>Unfinalized tokens are highly changeable, so why are they useful? Many applications can provide users with visual feedback of unfinalized tokens - particularly for dictation results. This feedback informs users of the progress of the recognition and helps the user to know that something is happening. However, because these tokens may change and are more likely than finalized tokens to be incorrect, the applications should visually distinguish the unfinalized tokens by using a different font, different color or even a different window. </p>
   <p><a name="19400"></a>The following is an example of finalized tokens and unfinalized tokens for the sentence "I come from Australia". The lines indicate the token values after the single <code>RESULT_CREATED</code> event, the multiple <code>RESULT_UPDATED</code> events and the final <code>RESULT_ACCEPTED</code> event. The finalized tokens are in bold, the unfinalized tokens are in italics. </p>
   <ol> 
    <li><a name="19405"></a><code>RESULT_CREATED</code>: <em>I come </em></li>
    <li><a name="19406"></a><code>RESULT_UPDATED</code>: <em>I come from </em></li>
    <li><a name="19407"></a><code>RESULT_UPDATED</code>: <strong>I come</strong> <em>from</em> </li>
    <li><a name="19408"></a><code>RESULT_UPDATED</code>: <strong>I come from</strong> <em>a strange land </em></li>
    <li><a name="37253"></a><code>RESULT_UPDATED</code>: <strong>I come from</strong> <em>Australia </em></li>
    <li><a name="37246"></a><code>RESULT_ACCEPTED</code>: <strong>I come from Australia </strong></li>
   </ol> 
   <p><a name="19422"></a>Recognizers can vary in how they support finalized and unfinalized tokens in a number of ways. For an unfinalized result, a recognizer may provide finalized tokens, unfinalized tokens, both or neither. Furthermore, for a recognizer that does support finalized and unfinalized tokens during recognition, the behavior may depend upon the number of active grammars, upon whether the result is for a <code>RuleGrammar</code> or <code>DictationGrammar</code>, upon the length of spoken sentences, and upon other more complex factors. Fortunately, unless there is a functional requirement to display or otherwise process intermediate result, an application can safely ignore all but the <code>RESULT_ACCEPTED</code> event. </p> 
   <a name="18915"></a>
   <h3>6.7.9 &nbsp; &nbsp; Finalized Rule Results </h3> 
   <p><a name="19568"></a>The are some common design patterns for processing accepted finalized results that match a <code>RuleGrammar</code>. First we review what we know about these results. </p>
   <ul>
    <li><a name="37324"></a>It is safe to cast an accepted result that matches a <code>RuleGrammar</code> to the <code>FinalRuleResult</code> interface. It is safe to call any method of the <code>FinalRuleResult</code> interface or its parents: <code>FinalResult</code> and <code>Result</code>. </li>
   </ul> 
   <ul>
    <li><a name="37396"></a>The <code>getGrammar</code> method of the <code>Result</code> interface return a reference to the matched <code>RuleGrammar</code>. The <code>getRuleGrammar</code> method of the <code>FinalRuleResult</code> interface returns references to the <code>RuleGrammars</code> matched by the alternative guesses. </li>
   </ul> 
   <ul>
    <li><a name="37417"></a>The <code>getBestToken</code> and <code>getBestTokens</code> methods of the <code>Result</code> interface return the recognizer's best guess of what a user said. </li>
   </ul> 
   <ul>
    <li><a name="37418"></a>The <code>getAlternativeTokens</code> method returns alternative guesses for the entire result. </li>
   </ul> 
   <ul>
    <li><a name="37436"></a>The tags for the best guess are available from the <code>getTags</code> method of the <code>FinalRuleResult</code> interface. </li>
   </ul> 
   <ul>
    <li><a name="37419"></a>Result audio (see <a href="Recognition.html#28834">Section 6.7.11</a>) and training information (see <a href="Recognition.html#37704">Section 6.7.12</a>) are optionally available. </li>
   </ul> 
   <a name="37592"></a>
   <h4>6.7.9.1 &nbsp; &nbsp; Result Tokens </h4> 
   <p><a name="37601"></a>A <code>ResultToken</code> in a result matching a <code>RuleGrammar</code> contains the same information as the <code>RuleToken</code> object in the <code>RuleGrammar</code> definition. This means that the tokenization of the result follows the tokenization of the grammar definition including compound tokens. For example, consider a grammar with the following Java Speech Grammar Format fragment which contains four tokens: </p>
   <dl>
    <dd>
     <pre><a name="37602"></a>&lt;rule&gt; = I went to "San Francisco";
</pre>
    </dd>
   </dl>
   <p><a name="37605"></a>If the user says "I went to New York" then the result will contain the four tokens defined by JSGF: "I", "went", "to", "San Francisco". </p>
   <p><a name="37609"></a>The <code>ResultToken</code> interface defines more advanced information. Amongst that information the <code>getStartTime</code> and <code>getEndTime</code> methods may optionally return time-stamp values (or <code>-1</code> if the recognizer does not provide time-alignment information). </p>
   <p><a name="37610"></a>The <code>ResultToken</code> interface also defines several methods for a recognizer to provide presentation hints. Those hints are ignored for <code>RuleGrammar</code> results Þ- they are only used for dictation results (see <a href="Recognition.html#20512">Section 6.7.10.2</a>). </p>
   <p><a name="37619"></a>Furthermore, the <code>getSpokenText</code> and <code>getWrittenText</code> methods will return an identical string which is equal to the string defined in the matched grammar. </p> 
   <a name="37596"></a>
   <h4>6.7.9.2 &nbsp; &nbsp; Alternative Guesses </h4> 
   <p><a name="27746"></a>In a <code>FinalRuleResult</code>, alternative guesses are alternatives for the entire result, that is, for a complete utterance spoken by a user. (A <code>FinalDictationResult</code> can provide alternatives for single tokens or sequences of tokens.) Because more than one <code>RuleGrammar</code> can be active at a time, an alternative token sequence may match a rule in a different <code>RuleGrammar</code> than the best guess tokens, or may match a different rule in the same <code>RuleGrammar</code> as the best guess. Thus, when processing alternatives for a <code>FinalRuleResult</code>, an application should use the <code>getRuleGrammar</code> and <code>getRuleName</code> methods to ensure that they analyze the alternatives correctly. </p>
   <p><a name="28921"></a>Alternatives are numbered from zero up. The 0th alternative is actually the best guess for the result so <code>FinalRuleResult.getAlternativeTokens(0)</code> returns the same array as <code>Result.getBestTokens()</code>. (The duplication is for programming convenience.) Likewise, the <code>FinalRuleResult.getRuleGrammar(0)</code> call will return the same result as <code>Result.getGrammar()</code>. </p>
   <p><a name="27744"></a>The following code is an implementation of the <code>ResultListener</code> interface that processes the <code>RESULT_ACCEPTED</code> event. The implementation assumes that a <code>Result</code> being processed matches a <code>RuleGrammar</code>. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="19673"></a>class MyRuleResultListener extends ResultAdapter
<a name="19674"></a>{
<a name="19675"></a>	public void resultAccepted(ResultEvent e)
<a name="19676"></a>	{
<a name="19717"></a>		// Assume that the result matches a RuleGrammar.
<a name="27791"></a>		// Cast the result (source of event) appropriately
<a name="19678"></a>		FinalRuleResult res = (FinalRuleResult) e.getSource();
<a name="25268"></a>
<a name="19714"></a>		// Print out basic result information
<a name="29017"></a>		PrintStream out = System.out;
<a name="19746"></a>		out.println("Number guesses: " + res.getNumberGuesses());
<a name="19730"></a>
<a name="19741"></a>		// Print out the best result and all alternatives
<a name="19742"></a>		for (int n=0; n &lt; res.getNumberGuesses(); n++) {
<a name="28951"></a>			// Extract the n-best information
<a name="28950"></a>			String gname = res.getRuleGrammar(n).getName();
<a name="28984"></a>			String rname = res.getRuleName(n);
<a name="28961"></a>			ResultToken[] tokens = res.getAlternativeTokens(n);
<a name="28952"></a>
<a name="28941"></a>			out.print("Alt " + n + ": ");
<a name="28930"></a>			out.print("&lt;" + gname + "." + rname + "&gt; :");
<a name="19768"></a>			for (int t=0; t &lt; tokens.length; t++)
<a name="19767"></a>				out.print(" " + tokens[t].getSpokenText());
<a name="19769"></a>			out.println();
<a name="19762"></a>		}
<a name="19685"></a>	}
<a name="19686"></a>}


      <hr></pre>
    </dd>
   </dl>
   <p><a name="19670"></a>For a grammar with commands to control a windowing system (shown below), a result might look like: </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="19808"></a>Number guesses: 3
<a name="28969"></a>Alt 0: &lt;com.acme.actions.command&gt;: move the window to the back
<a name="28975"></a>Alt 1: &lt;com.acme.actions.command&gt;: move window to the back
<a name="28977"></a>Alt 2: &lt;com.acme.actions.command&gt;: open window to the front


      <hr></pre>
    </dd>
   </dl>
   <p><a name="29002"></a>If more than one grammar or more than one public rule was active, the <code>&lt;grammarName.ruleName&gt;</code> values could vary between the alternatives. </p> 
   <a name="19831"></a>
   <h4>6.7.9.3 &nbsp; &nbsp; Result Tags </h4> 
   <p><a name="29023"></a>Processing commands generated from a <code>RuleGrammar</code> becomes increasingly difficult as the complexity of the grammar rises. With the Java Speech API, speech recognizers provide two mechanisms to simplify the processing of results: tags and parsing. </p>
   <p><a name="19836"></a>A tag is a label attached to an entity within a <code>RuleGrammar</code>. The Java Speech Grammar Format and the <code>RuleTag</code> class define how tags can be attached to a grammar. The following is a grammar for very simple control of windows which includes tags attached to the important words in the grammar. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="19858"></a>grammar com.acme.actions;
<a name="19859"></a>
<a name="19867"></a>public &lt;command&gt; = &lt;action&gt; &lt;object&gt; [&lt;where&gt;]
<a name="19860"></a>&lt;action&gt; = open {ACT_OP}| close {ACT_CL} | move {ACT_MV};
<a name="19861"></a>&lt;object&gt; = [a | an | the] (window {OBJ_WIN} | icon {OBJ_ICON});
<a name="19888"></a>&lt;where&gt; = [to the] (back {WH_BACK} | front {WH_FRONT});


      <hr></pre>
    </dd>
   </dl>
   <p><a name="19847"></a>This grammar allows users to speak commands such as </p>
   <dl>
    <dd>
     <pre><a name="29031"></a><strong>	</strong><em>open window<br>
</em><strong>	</strong><em>move</em> the <em>icon<br>
</em><strong>	</strong><em>move</em> the <em>window</em> to the <em>back<br>
</em><strong>	</strong><em>move window back
</em></pre>
    </dd>
   </dl>
   <p><a name="29030"></a>The italicized words are the ones that are tagged in the grammar - these are the words that the application cares about. For example, in the third and fourth example commands, the spoken words are different but the tagged words are identical. Tags allow an application to ignore trivial words such as "the" and "to". </p>
   <p><a name="29040"></a>The <code>com.acme.actions</code> grammar can be loaded and enabled using the code in the <em><a href="Recognition.html#11644">"Hello World!"</a></em> example. Since the grammar has a single public rule, <code>&lt;command&gt;</code>, the recognizer will listen for speech matching that rule, such as the example results given above. </p>
   <p><a name="19891"></a>The tags for the best result are available through the <code>getTags</code> method of the <code>FinalRuleResult</code> interface. This method returns an array of tags associated with the tokens (words) and other grammar entities matched by the result. If the best sequence of tokens is "move the window to the front", the list of tags is the following <code>String</code> array: </p>
   <dl>
    <dd>
     <pre><a name="19902"></a>	String tags[] = {"ACT_MV", "OBJ_WIN", "WH_FRONT"};
</pre>
    </dd>
   </dl>
   <p><a name="19848"></a>Note how the order of the tags in the result is preserved (forward in time). These tags are easier for most applications to interpret than the original text of what the user said. </p>
   <p><a name="29076"></a>Tags can also be used to handle synonyms - multiple ways of saying the same thing. For example, "programmer", "hacker", "application developer" and "computer dude" could all be given the same tag, say "DEV". An application that looks at the "DEV" tag will not care which way the user spoke the title. </p>
   <p><a name="19914"></a>Another use of tags is for <em>internationalization</em> of applications. Maintaining applications for multiple languages and locales is easier if the code is insensitive to the language being used. In the same way that the "DEV" tag isolated an application from different ways of saying "programmer", tags can be used to provide an application with similar input irrespective of the language being recognized. </p>
   <p><a name="24694"></a>The following is a grammar for French with the same functionality as the grammar for English shown above. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="19923"></a>grammar com.acme.actions.fr;
<a name="19924"></a>
<a name="19925"></a>public &lt;command&gt; = &lt;action&gt; &lt;object&gt; [&lt;where&gt;]
<a name="19926"></a>&lt;action&gt; = ouvrir {ACT_OP}| fermer {ACT_CL} | deplacer {ACT_MV};
<a name="19927"></a>&lt;object&gt; = fenetre {OBJ_WIN} | icone {OBJ_ICON};
<a name="19928"></a>&lt;where&gt; = au-dessous {WH_BACK} | au-dessus {WH_FRONT};


      <hr></pre>
    </dd>
   </dl>
   <p><a name="20453"></a>For this simple grammar, there are only minor differences in the structure of the grammar (e.g. the <code>"[to the]"</code> tokens in the <code>&lt;where&gt;</code> rule for English are absent in French). However, in more complex grammars the syntactic differences between languages become significant and tags provide a clearer improvement. </p>
   <p><a name="29120"></a>Tags do not completely solve internationalization problems. One issue to be considered is word ordering. A simple command like "open the window" can translate to the form "the window open" in some languages. More complex sentences can have more complex transformations. Thus, applications need to be aware of word ordering, and thus tag ordering when developing international applications. </p> 
   <a name="37480"></a>
   <h4>6.7.9.4 &nbsp; &nbsp; Result Parsing </h4> 
   <p><a name="37483"></a>More advanced applications <em>parse</em> results to get even more information than is available with tags. Parsing is the capability to analyze how a sequence of tokens matches a <code>RuleGrammar</code>. Parsing of text against a <code>RuleGrammar</code> is discussed in <a href="Recognition.html#14724">Section 6.5.5</a> . </p>
   <p><a name="26172"></a>Parsing a <code>FinalRuleResult</code> produces a <code>RuleParse</code> object. The <code>getTags</code> method of a <code>RuleParse</code> object provides the same tag information as the <code>getTags</code> method of a <code>FinalRuleResult</code>. However, the <code>FinalRuleResult</code> provides tag information for only the best-guess result, whereas parsing can be applied to the alternative guesses. </p>
   <p><a name="37505"></a>An API requirement that simplifies parsing of results that match a <code>RuleGrammar</code> is that for a such result to be <code>ACCEPTED</code> (not rejected) it must exactly match the grammar - technically speaking, it must be possible to parse a <code>FinalRuleResult</code> against the <code>RuleGrammar</code> it matches. This is not guaranteed, however, if the result was rejected or if the <code>RuleGrammar</code> has been modified since it was committed and produced the result. </p> 
   <a name="19563"></a>
   <h3>6.7.10 &nbsp; &nbsp; Finalized Dictation Results </h3> 
   <p><a name="37530"></a>The are some common design patterns for processing accepted finalized results that match a <code>DictationGrammar</code>. First we review what we know about these results. </p>
   <ul>
    <li><a name="37531"></a>It is safe to cast an accepted result that matches a <code>DictationGrammar</code> to the <code>FinalDictationResult</code> interface. It is safe to call any method of the <code>FinalDictationResult</code> interface or its parents: <code>FinalResult</code> and <code>Result</code>. </li>
   </ul> 
   <ul>
    <li><a name="37532"></a>The <code>getGrammar</code> method of the <code>Result</code> interface return a reference to the matched <code>DictationGrammar</code>. </li>
   </ul> 
   <ul>
    <li><a name="37533"></a>The <code>getBestToken</code> and <code>getBestTokens</code> methods of the <code>Result</code> interface return the recognizer's best guess of what a user said. </li>
   </ul> 
   <ul>
    <li><a name="37534"></a>The <code>getAlternativeTokens</code> method of the <code>FinalDictationResult</code> interface returns alternative guesses for any token or sequence of tokens. </li>
   </ul> 
   <ul>
    <li><a name="37539"></a>Result audio (see <a href="Recognition.html#28834">Section 6.7.11</a>) and training information (see <a href="Recognition.html#37704">Section 6.7.12</a>) are optionally available. </li>
   </ul> 
   <p><a name="37566"></a>The <code>ResultTokens</code> provided in a <code>FinalDictationResult</code> contain specialized information that includes hints on textual presentation of tokens. <a href="Recognition.html#20512">Section 6.7.10.2</a> discusses the presentation hints in detail. In this section the methods for obtaining and using alternative tokens are described. </p> 
   <a name="37584"></a>
   <h4>6.7.10.1 &nbsp; &nbsp; Alternative Guesses </h4> 
   <p><a name="37622"></a>Alternative tokens for a dictation result are most often used by an application for display to users for correction of dictated text. A typical scenario is that a user speaks some text - perhaps a few words, a few sentences, a few paragraphs or more. The user reviews the text and detects a recognition error. This means that the best guess token sequence is incorrect. However, very often the correct text is one of the top alternative guesses. Thus, an application will provide a user the ability to review a set of alternative guesses and to select one of them if it is the correct text. Such a correction mechanism is often more efficient than typing the correction or dictating the text again. If the correct text is not amongst the alternatives an application must support other means of entering the text. </p>
   <p><a name="29354"></a>The <code>getAlternativeTokens</code> method is passed a starting and an ending <code>ResultToken</code>. These tokens must have been obtained from the same result either through a call to <code>getBestToken</code> or <code>getBestTokens</code> in the <code>Result</code> interface, or through a previous call to <code>getAlternativeTokens</code>. </p>
   <dl>
    <dd>
     <pre><a name="29361"></a>ResultToken[][] getAlternativeTokens(<br>
						ResultToken fromToken,<br>
						ResultToken toToken,<br>
						int max);
</pre>
    </dd>
   </dl>
   <p><a name="29355"></a>To obtain alternatives for a single token (rather than alternatives for a sequence), set <code>toToken</code> to <code>null</code>. </p>
   <p><a name="29400"></a>The <code>int</code> parameter allows the application to specify the number of alternatives it wants. The recognizer may choose to return any number of alternatives up to the maximum number including just one alternative (the original token sequence). Applications can indicate in advance the number of alternatives it may request by setting the <code>NumResultAlternatives</code> parameter through the recognizer's <code>RecognizerProperties</code> object. </p>
   <p><a name="29391"></a>The two-dimensional array returned by the <code>getAlternativeTokens</code> method is the most difficult aspect of dictation alternatives to understand. The following example illustrates the major features of the return value. </p>
   <p><a name="29449"></a>Let's consider a dictation example where the user says "he felt alienated today" but the recognizer hears "he felt alien ate Ted today". The user says four words but the recognizer hears six words. In this example, the boundaries of the spoken words and best-guess align nicely: "alienated" aligns with "alien ate Ted" (incorrect tokens don't always align smoothly with the correct tokens). </p>
   <p><a name="20089"></a>Users are typically better at locating and fixing recognition errors than recognizers or applications - they provided the original speech. In this example, the user will likely identify the words "alien ate Ted" as incorrect (tokens 2 to 4 in the best-guess result). By an application-provided method such as selection by mouse and a pull-down menu, the user will request alternative guesses for the three incorrect tokens. The application calls the <code>getAlternativeTokens</code> method of the <code>FinalDictationResult</code> to obtain the recognizer's guess at the alternatives. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="20101"></a>	// Get 6 alternatives for for tokens 2 through 4.
<a name="20283"></a>	FinalDictationResult r = ...;
<a name="27362"></a>	ResultToken tok2 = r.getBestToken(2);
<a name="27364"></a>	ResultToken tok4 = r.getBestToken(4);
<a name="20111"></a>	String[][] alt = r.getAlternativeTokens(tok2, tok4, 6);


      <hr></pre>
    </dd>
   </dl>
   <p><a name="20070"></a>The return array might look like the following. Each line represents a sequence of alternative tokens to "alien ate Ted". Each word in each alternative sequence represents a <code>ResultToken</code> object in an array. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="20123"></a>	alt[0] = alien ate Ted      // the best guess
<a name="20124"></a>	alt[1] = alienate Ted        // the 1st alternative
<a name="20134"></a>	alt[2] = alienated              // the 2nd alternative
<a name="20333"></a>	alt[3] = alien hated          // the 3rd alternative
<a name="20335"></a>	alt[4] = a lion ate Ted    // the 4th alternative


      <hr></pre>
    </dd>
   </dl>
   <p><a name="20118"></a>The points to note are: </p>
   <ul>
    <li><a name="20145"></a>The first alternative is the best guess. This is usually the case if the <code>toToken</code> and <code>fromToken</code> values are from the best-guess sequence. (From an user perspective it's not really an alternative.) </li>
   </ul> 
   <ul>
    <li><a name="29476"></a>Only five alternative sequences were returned even though six were requested. This is because a recognizer will only return alternatives it considers to reasonable guesses. It is legal for this call to return only the best guess with no alternatives if can't find any reasonable alternatives. </li>
   </ul> 
   <ul>
    <li><a name="20152"></a>The number of tokens is not the same in all the alternative sequences (3, 2, 1, 2, 4 tokens respectively). This return array is known as a <em>ragged array</em>. From a speech perspective is easy to see why different lengths are needed, but application developers do need to be careful processing a ragged array. </li>
   </ul> 
   <ul>
    <li><a name="20153"></a>The best-guess and the alternatives do not always make sense to humans. </li>
   </ul> 
   <p><a name="20027"></a>A complex issue to understand is that the alternatives vary according to how the application (or user) requests them. The 1st alternative to "alien ate Ted" is "alienate Ted". However, the 1st alternative to "alien" might be "a lion", the 1st alternative to "alien ate" might be "alien eight", and the 1st alternative to "alien ate Ted today" might be "align ate Ted to day". </p>
   <p><a name="20353"></a>Fortunately for application developers, users learn to select sequences that are likely to give reasonable alternatives, and recognizers are developed to make the alternatives as useful and accurate as possible. </p> 
   <a name="20512"></a>
   <h4>6.7.10.2 &nbsp; &nbsp; Result Tokens </h4> 
   <p><a name="20513"></a>A <code>ResultToken</code> object represents a single token in a result. A token is most often a single word, but multi-word tokens are possible (e.g., "New York") as well as formatting characters and language-specific constructs. For a <code>DictationGrammar</code> the set of tokens is built into the recognizer. </p>
   <p><a name="37647"></a>Each <code>ResultToken</code> in a <code>FinalDictationResult</code> provides the following information. </p>
   <ul>
    <li><a name="26900"></a>The <em>spoken form</em> of the token which provides a transcript of what the user says (<code>getSpokenText</code> method). In a dictation system, the spoken form is typically used when displaying unfinalized tokens. </li>
   </ul> 
   <ul>
    <li><a name="26932"></a>The <em>written form</em> of the token which indicates how to visually present the token (<code>getWrittenText</code> method). In a dictation system, the written form of finalized tokens is typically placed into the text edit window after applying the following presentation hints. </li>
   </ul> 
   <ul>
    <li><a name="26936"></a>A <em>capitalization hint</em> indicating whether the written form of the following token should be capitalized (first letter only), all uppercase, all lowercase, or left as-is (<code>getCapitalizationHint</code> method). </li>
   </ul> 
   <ul>
    <li><a name="20551"></a>An <em>spacing hint</em> indicating how the written form should be spaced with the previous and following tokens. </li>
   </ul> 
   <p><a name="37665"></a>The presentation hints in a <code>ResultToken</code> are important for the processing of dictation results. Dictation results are typically displayed to the user, so using the written form and the capitalization and spacing hints for formatting is important. For example, when dictation is used in word processing, the user will want the printed text to be correctly formatted. </p>
   <p><a name="26926"></a>The capitalization hint indicates how the written form of the following token should be formatted. The capitalization hint takes one of four mutually exclusive values. <code>CAP_FIRST</code> indicates that the first character of the following token should be capitalized. The <code>UPPERCASE</code> and <code>LOWERCASE</code> values indicate that the following token should be either all uppercase or lowercase. <code>CAP_AS_IS</code> indicates that there should be no change in capitalization of the following token. </p>
   <p><a name="26927"></a>The spacing hint deals with spacing around a token. It is an <code>int</code> value containing three flags which are or'ed together (using the '|' operator). If none of the three spacing hint flags are set true, then <code>getSpacingHint</code> method returns the value <code>SEPARATE</code> which is the value zero. </p>
   <ul>
    <li><a name="26993"></a>The <code>ATTACH_PREVIOUS</code> bit is set if the token should be attached to the previous token: no space between this token and the previous token. In English, some punctuation characters have this flag set true. For example, periods, commas and colons are typically attached to the previous token. </li>
   </ul> 
   <ul>
    <li><a name="26994"></a>The <code>ATTACH_FOLLOWING</code> bit is set if the token should be attached to the following token: no space between this token and the following token. For example, in English, opening quotes, opening parentheses and dollar signs typically attach to the following token. </li>
   </ul> 
   <ul>
    <li><a name="26995"></a>The <code>ATTACH_GROUP</code> bit is set if the token should be attached to previous or following tokens if they also have the <code>ATTACH_GROUP</code> flag set to true. In other words, tokens in an attachment group should be attached together. In English, a common use of the group flag is for numbers, digits and currency amounts. For example, the sequence of four spoken-form tokens, <code>"3" "point" "1" "4"</code>, should have the group flag set true, so the presentation form should not have separating spaces: <code>"3.14"</code>. </li>
   </ul> 
   <p><a name="37689"></a>Every language has conventions for textual representation of a spoken language. Since recognizers are language-specific and understand many of these presentation conventions, they provide the presentation hints (written form, capitalization hint and spacing hint) to simplify applications. However, applications may choose to override the recognizer's hints or may choose to do additional processing. </p>
   <p><a name="20608"></a><a href="Recognition.html#20919">Table 6-6</a> shows examples of tokens in which the spoken and written forms are different:</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="20919"></a>
     <b> Table 6-6 Spoken and written forms for some English tokens </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="20927"></a><b>Spoken Form </b> &nbsp;</td>
      <td><a name="20929"></a><b>Written Form </b> &nbsp;</td>
      <td><a name="20931"></a><b>Capitalization </b> &nbsp;</td>
      <td><a name="20933"></a><b>Spacing </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="20935"></a>twenty &nbsp;</td>
      <td><a name="20937"></a><code>20 </code> &nbsp;</td>
      <td><a name="20939"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="20941"></a><code>SEPARATE </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="20943"></a>new line &nbsp;</td>
      <td><a name="20945"></a><code>'\n' '\u000A' </code> &nbsp;</td>
      <td><a name="20947"></a><code>CAP_FIRST </code> &nbsp;</td>
      <td><a name="20949"></a><code>ATTACH_PREVIOUS &amp; ATTACH_FOLLOWING </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="20951"></a>new paragraph &nbsp;</td>
      <td><a name="20953"></a><code>'\u2029' </code> &nbsp;</td>
      <td><a name="21021"></a><code>CAP_FIRST </code> &nbsp;</td>
      <td><a name="27034"></a><code>ATTACH_PREVIOUS &amp; ATTACH_FOLLOWING </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="20967"></a>no space &nbsp;</td>
      <td><a name="20969"></a><code>null </code> &nbsp;</td>
      <td><a name="20971"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="27038"></a><code>ATTACH_PREVIOUS &amp; ATTACH_FOLLOWING </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21090"></a>Space bar &nbsp;</td>
      <td><a name="21092"></a><code> ' ' '\u0020' </code> &nbsp;</td>
      <td><a name="21094"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="27040"></a><code>ATTACH_PREVIOUS &amp; ATTACH_FOLLOWING </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21098"></a>Capitalize next &nbsp;</td>
      <td><a name="21100"></a><code>null </code> &nbsp;</td>
      <td><a name="21102"></a><code>CAP_FIRST </code> &nbsp;</td>
      <td><a name="21104"></a><code>SEPARATE </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21106"></a>Period &nbsp;</td>
      <td><a name="21108"></a><code>'.' '\u002E' </code> &nbsp;</td>
      <td><a name="21151"></a><code>CAP_FIRST </code> &nbsp;</td>
      <td><a name="21112"></a><code>ATTACH_PREVIOUS </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21082"></a>Comma &nbsp;</td>
      <td><a name="21084"></a><code>',' '\u002C' </code> &nbsp;</td>
      <td><a name="21086"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21088"></a><code>ATTACH_PREVIOUS </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21137"></a>Open parentheses &nbsp;</td>
      <td><a name="21139"></a><code>'(' '\u0028' </code> &nbsp;</td>
      <td><a name="21141"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21143"></a><code>ATTACH_FOLLOWING </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21129"></a>Exclamation mark &nbsp;</td>
      <td><a name="21131"></a><code>'!' '\u0021' </code> &nbsp;</td>
      <td><a name="21133"></a><code>CAP_FIRST </code> &nbsp;</td>
      <td><a name="21135"></a><code>ATTACH_PREVIOUS </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="20975"></a>dollar sign &nbsp;</td>
      <td><a name="20977"></a><code>'$' '\u0024' </code> &nbsp;</td>
      <td><a name="20979"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="20981"></a><code>ATTACH_FOLLOWING &amp; ATTACH_GROUP </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="20983"></a>pound sign &nbsp;</td>
      <td><a name="20985"></a><code>'£' '\u00A3' </code> &nbsp;</td>
      <td><a name="20987"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="20989"></a><code>ATTACH_FOLLOWING &amp; ATTACH_GROUP </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="20991"></a>yen sign &nbsp;</td>
      <td><a name="20993"></a><code>'¥' '\u00A5' </code> &nbsp;</td>
      <td><a name="20995"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="20997"></a><code>ATTACH_PREVIOUS &amp; ATTACH_GROUP </code> &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="21119"></a>"New line", "new paragraph", "space bar", "no space" and "capitalize next" are all examples of conversion of an implicit command (e.g. "start a new paragraph"). For three of these, the written form is a single Unicode character. Most programmers are familiar with the new-line character '\n' and space ' ', but fewer are familiar with the Unicode character for new paragraph '\u2029'. For convenience and consistency, the <code>ResultToken</code> includes static variables called <code>NEW_LINE</code> and <code>NEW_PARAGRAPH</code>. </p>
   <p><a name="37699"></a>Some applications will treat a paragraph boundary as two new-line characters, others will treat it differently. Each of these commands provides hints for capitalization. For example, in English the first letter of the first word of a new paragraph is typically capitalized. </p>
   <p><a name="21127"></a>The punctuation characters, "period", "comma", "open parentheses", "exclamation mark" and the three currency symbols convert to a single Unicode character and have special presentation hints. </p>
   <p><a name="21072"></a>An important feature of the written form for most of the examples is that the application does not need to deal with synonyms (multiple ways of saying the same thing). For example, "open parentheses" may also be spoken as "open paren" or "begin paren" but in all cases the same written form is generated. </p>
   <p><a name="20600"></a>The following is an example sequence of result tokens.</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="21204"></a>
     <b> Table 6-7 Sample sequence of result tokens </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="21212"></a><b>Spoken Form </b> &nbsp;</td>
      <td><a name="21214"></a><b>Written Form </b> &nbsp;</td>
      <td><a name="21216"></a><b>Capitalization </b> &nbsp;</td>
      <td><a name="21218"></a><b>Spacing </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21220"></a>new line &nbsp;</td>
      <td><a name="21222"></a><code>"\n" </code> &nbsp;</td>
      <td><a name="21224"></a><code>CAP_FIRST </code> &nbsp;</td>
      <td><a name="21226"></a><code>ATTACH_PREVIOUS &amp; ATTACH_FOLLOWING </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21228"></a>the &nbsp;</td>
      <td><a name="21230"></a><code>"the" </code> &nbsp;</td>
      <td><a name="21232"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21234"></a><code>SEPARATE </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21236"></a>uppercase next &nbsp;</td>
      <td><a name="21238"></a><code>null </code> &nbsp;</td>
      <td><a name="21240"></a><code>UPPERCASE </code> &nbsp;</td>
      <td><a name="21242"></a><code>SEPARATE </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21244"></a>index &nbsp;</td>
      <td><a name="21246"></a><code>"index" </code> &nbsp;</td>
      <td><a name="21248"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21250"></a><code>SEPARATE </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21252"></a>is &nbsp;</td>
      <td><a name="21254"></a><code>"is" </code> &nbsp;</td>
      <td><a name="21256"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21258"></a><code>SEPARATE </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21260"></a>seven &nbsp;</td>
      <td><a name="21262"></a><code>"7" </code> &nbsp;</td>
      <td><a name="21264"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21266"></a><code>ATTACH_GROUP </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21268"></a>dash &nbsp;</td>
      <td><a name="21270"></a><code>"-" </code> &nbsp;</td>
      <td><a name="21272"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21274"></a><code>ATTACH_GROUP </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21276"></a>two &nbsp;</td>
      <td><a name="21278"></a><code>"2" </code> &nbsp;</td>
      <td><a name="21280"></a><code>CAP_AS_IS </code> &nbsp;</td>
      <td><a name="21282"></a><code>ATTACH_GROUP </code> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="21286"></a>period &nbsp;</td>
      <td><a name="21288"></a><code>"." </code> &nbsp;</td>
      <td><a name="21290"></a><code>CAP_FIRST </code> &nbsp;</td>
      <td><a name="21292"></a><code>ATTACH_PREVIOUS </code> &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="21364"></a>This sequence of tokens should be converted to the following string: </p>
   <dl>
    <dd>
     <pre><a name="21366"></a>	"\nThe INDEX is 7-2."
</pre>
    </dd>
   </dl>
   <p><a name="21676"></a>Conversion of spoken text to a written form is a complex task and is complicated by the different conventions of different languages and often by different conventions for the same language. The spoken form, written form and presentation hints of the <code>ResultToken</code> interface handle most simple conversions. Advanced applications should consider filtering the results to process more complex patterns, particularly cross-token patterns. For example "nineteen twenty eight" is typically converted to "1928" and "twenty eight dollars" to "$28" (note the movement of the dollar sign to before the numbers). </p> 
   <a name="28834"></a>
   <h3>6.7.11 &nbsp; &nbsp; Result Audio </h3> 
   <p><a name="37754"></a>If requested by an application, some recognizers can provide audio data for results. Audio data has a number of uses. In dictation applications, providing audio feedback to users aids correction of text because the audio reminds users of what they said (it's not always easy to remember exactly what you dictate, especially in long sessions). Audio data also allows storage for future evaluation and debugging. </p>
   <p><a name="30193"></a>Audio data is provided for finalized results through the following methods of the <code>FinalResult</code> interface.</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="30146"></a>
     <b> Table 6-8 FinalResult interface: audio methods </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="30150"></a><b>Name </b> &nbsp;</td>
      <td><a name="30152"></a><b>Description </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="30154"></a><code>getAudio </code> &nbsp;</td>
      <td><a name="30156"></a>Get an AudioClip for a token, a sequence of tokens or for an entire result. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="30158"></a><code>isAudioAvailable </code> &nbsp;</td>
      <td><a name="30160"></a>Tests whether audio data is available for a result. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="30162"></a><code>releaseAudio </code> &nbsp;</td>
      <td><a name="30164"></a>Release audio data for a result. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="30196"></a>There are two <code>getAudio</code> methods in the <code>FinalResult</code> interface. One method accepts no parameters and returns an <code>AudioClip</code> for an entire result or <code>null</code> if audio data is not available for this result. The other <code>getAudio</code> method takes a start and end <code>ResultToken</code> as input and returns an <code>AudioClip</code> for the segment of the result including the start and end token or <code>null</code> if audio data is not available. </p>
   <p><a name="30215"></a>In both forms of the <code>getAudio</code> method, the recognizer will attempt to return the specified audio data. However, it is not always possible to exactly determine the start and end of words or even complete results. Sometimes segments are "clipped" and sometimes surrounding audio is included in the <code>AudioClip</code>. </p>
   <p><a name="23036"></a>Not all recognizers provide access to audio for results. For recognizers that do provide audio data, it is not necessarily provided for all results. For example, a recognizer might only provide audio data for dictation results. Thus, applications should always check for a null return value on a <code>getAudio</code> call. </p>
   <p><a name="22988"></a>The storage of audio data for results potentially requires large amounts of memory, particularly for long sessions. Thus, result audio requires special management. An application that wishes to use result audio should: </p>
   <ul>
    <li><a name="23060"></a>Set the <code>ResultAudioProvided</code> parameter of <code>RecognizerProperties</code> to <code>true</code>. Recognizers that do not support audio data ignore this call. </li>
   </ul> 
   <ul>
    <li><a name="23068"></a>Test the availability of audio for a result using the <code>isAudioAvailable</code> method of the <code>FinalResult</code> interface. </li>
   </ul> 
   <ul>
    <li><a name="23084"></a>Use the <code>getAudio</code> methods to obtain audio data. These methods return <code>null</code> if audio data is not available. </li>
   </ul> 
   <ul>
    <li><a name="23069"></a>Once the application has finished use of the audio for a <code>Result</code>, it should call the <code>releaseAudio</code> method of <code>FinalResult</code> to free up resources. </li>
   </ul> 
   <p><a name="30254"></a>A recognizer may choose to release audio data for a result if it is necessary to reclaim memory or other system resources. </p>
   <p><a name="30261"></a>When audio is released by either a call to <code>releaseAudio</code> or by the recognizer a <code>AUDIO_RELEASED</code> event is issued to the <code>audioReleased</code> method of the <code>ResultListener</code>. </p> 
   <a name="37704"></a>
   <h3>6.7.12 &nbsp; &nbsp; Result Correction </h3> 
   <p><a name="37705"></a>Recognition results are not always correct. Some recognizers can be trained by informing of the correct tokens for a result - usually when a user corrects a result. </p>
   <p><a name="37706"></a>Recognizers are not required to support correction capabilities. If a recognizer does support correction, it does not need to support correction for every result. For example, some recognizers support correction only for dictation results. </p>
   <p><a name="37707"></a>Applications are not required to provide recognizers with correction information. However, if the information is available to an application and the recognizer supports correction then it is good practice to inform the recognizer of the correction so that it can improve its future recognition performance. </p>
   <p><a name="37736"></a>The <code>FinalResult</code> interface provides the methods that handle correction.</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="37713"></a>
     <b> Table 6-9 FinalResult interface: correction methods </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="37717"></a><b>Name </b> &nbsp;</td>
      <td><a name="37719"></a><b>Description </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="37721"></a><code>tokenCorrection </code> &nbsp;</td>
      <td><a name="37723"></a>Inform the recognizer of a correction in which zero or more tokens replace a token or sequence of tokens. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="37725"></a><code>MISRECOGNITION<br> USER_CHANGE<br> DONT_KNOW </code> &nbsp;</td>
      <td><a name="37727"></a>Indicate the type of correction. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="37729"></a><code>isTrainingInfoAvailable </code> &nbsp;</td>
      <td><a name="37731"></a>Tests whether the recognizer has information available to allow it to learn from a correction. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="37733"></a><code>releaseTrainingInfo </code> &nbsp;</td>
      <td><a name="37735"></a>Release training information for a result. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p><a name="37737"></a>Often, but certainly not always, a correction is triggered when a user corrects a recognizer by selecting amongst the alternative guesses for a result. Other instances when an application is informed of the correct result are when the user types a correction to dictated text, or when a user corrects a misrecognized command with a follow-up command. </p>
   <p><a name="37738"></a>Once an application has obtained the correct result text, it should inform the recognizer. The correction information is provided by a call to the <code>tokenCorrection</code> method of the <code>FinalResult</code> interface. This method indicates a correction of one token sequence to another token sequence. Either token sequence may contain one or more tokens. Furthermore, the correct token sequence may contain zero tokens to indicate deletion of tokens. </p>
   <p><a name="37739"></a>The <code>tokenCorrection</code> method accepts a <code>correctionType</code> parameter that indicates the reason for the correction. The legal values are defined by constants of the <code>FinalResult</code> interface: </p>
   <ul>
    <li><a name="37740"></a><code>MISRECOGNITION</code> indicates that the new tokens are known to be the tokens actually spoken by the user: a correction of a recognition error. Applications can be confident that a selection of an alternative token sequence implies a <code>MISRECOGNITION</code> correction. </li>
   </ul> 
   <ul>
    <li><a name="37741"></a><code>USER_CHANGE</code> indicates that the new tokens are not the tokens originally spoken by the user but instead the user has changed his/her mind. This is a "speako" (a spoken version of a "typo"). A <code>USER_CHANGE</code> may be indicated if a user types over the recognized result, but sometimes the user may choose to type in the correct result. </li>
   </ul> 
   <ul>
    <li><a name="37742"></a><code>DONT_KNOW</code> the application does not know whether the new tokens are correcting a recognition error or indicating a change by the user. Applications should indicate this type of correction whenever unsure of the type of correction. </li>
   </ul> 
   <p><a name="37743"></a>Why is it useful to tell a recognizer about a <code>USER_CHANGE</code>? Recognizers adapt to both the sounds and the patterns of words of users. A <code>USER_CHANGE</code> correction allows the recognizer to learn about a user's word patterns. A <code>MISRECOGNITION</code> correction allows the recognizer to learn about both the user's voice and the word patterns. In both cases, correcting the recognizer requests it to re-train itself based on the new information. </p>
   <p><a name="37744"></a>Training information needs to be managed because it requires substantial memory and possibly other system resources to maintain it for a result. For example, in long dictation sessions, correction data can begin to use excessive amounts of memory. </p>
   <p><a name="37745"></a>Recognizers maintain training information only when the recognizer's <code>TrainingProvided</code> parameter is set to true through the <code>RecognizerProperties</code> interface. Recognizers that do not support correction will ignore calls to the <code>setTrainingProvided</code> method. </p>
   <p><a name="37746"></a>If the <code>TrainingProvided</code> parameter is set to true, a result may include training information when it is finalized. Once an application believes the training information is no longer required for a specific <code>FinalResult</code>, it should call the <code>releaseTrainingInfo</code> method of <code>FinalResult</code> to indicate the recognizer can release the resources used to store the information. </p>
   <p><a name="37747"></a>At any time, the availability of training information for a result can be tested by calling the <code>isTrainingInfoAvailable</code> method. </p>
   <p><a name="37748"></a>Recognizers can choose to release training information even without a request to do so by the application. This does not substantially affect an application because performing correction on a result which does not have training information is not an error. </p>
   <p><a name="37749"></a>A <code>TRAINING_INFO_RELEASED</code> event is issued to the <code>ResultListener</code> when the training information is released. The event is issued identically whether the application or recognizer initiated the release. </p> 
   <a name="37935"></a>
   <h3>6.7.13 &nbsp; &nbsp; Rejected Results </h3> 
   <p><a name="22233"></a>First, a warning: <em>ignore rejected results unless you really understand them! </em></p>
   <p><a name="24731"></a>Like humans, recognizers don't have perfect hearing and so they make mistakes (recognizers still tend to make more mistakes than people). An application should never completely trust a recognition result. In particular, applications should treat important results carefully, for example, "delete all files". </p>
   <p><a name="21690"></a>Recognizers try to determine whether they have made a mistake. This process is known as <em>rejection</em>. But recognizers also make mistakes in rejection! In short, a recognizer cannot always tell whether or not it has made a mistake. </p>
   <p><a name="21691"></a>A recognizer may reject incoming speech for a number of reasons: </p>
   <ul>
    <li><a name="21692"></a>Detected a non-speech event (e.g. cough, laughter, microphone click). </li>
   </ul> 
   <ul>
    <li><a name="21693"></a>Detected speech that only partially matched an active grammar (e.g. user spoke only half a command). </li>
   </ul> 
   <ul>
    <li><a name="21694"></a>Speech contained "um", "ah", or some other speaking error that the recognizer could not ignore. </li>
   </ul> 
   <ul>
    <li><a name="21695"></a>Speech matched an active grammar but the recognizer was not confident that it was an accurate match. </li>
   </ul> 
   <p><a name="37948"></a>Rejection is controlled by the <code>ConfidenceLevel</code> parameter of <code>RecognizerProperties</code> (see <a href="Recognition.html#15438">Section 6.8</a>). The confidence value is a floating point number between 0.0 and 1.0. A value of 0.0 indicates weak rejection - the recognizer doesn't need to be very confident to accept a result. A value of 1.0 indicates strongest rejection, implying that the recognizer will reject a result unless it is very confident that the result is correct. A value of 0.5 is the recognizer's default. </p> 
   <a name="37964"></a>
   <h4>6.7.13.1 &nbsp; &nbsp; Rejection Timing </h4> 
   <p><a name="37969"></a>A result may be rejected with a <code>RESULT_REJECTED</code> event at any time while it is <code>UNFINALIZED</code>: that is, any time after a <code>RESULT_CREATED</code> event but without a <code>RESULT_ACCEPTED</code> event occurring. (For a description of result events see <a href="Recognition.html#35736">Section 6.7.4</a>.) </p>
   <p><a name="37970"></a>This means that the sequence of result events that produce a <code>REJECTED</code> result: </p>
   <ul>
    <li><a name="37981"></a>A single <code>RESULT_CREATED</code> event to issue a new result in the <code>UNFINALIZED</code> state. </li>
   </ul> 
   <ul>
    <li><a name="37986"></a>While in the <code>UNFINALIZED</code> state, zero or more <code>RESULT_UPDATED</code> events may be issued to update finalized and/or unfinalized tokens. Also, a single optional <code>GRAMMAR_FINALIZED</code> event may be issued to indicate that the matched grammar has been identified. </li>
   </ul> 
   <ul>
    <li><a name="37994"></a>A single <code>RESULT_REJECTED</code> event moves the result to the <code>REJECTED</code> state. </li>
   </ul> 
   <p><a name="38066"></a>When a result is rejected, there is a strong probability that the information about a result normally provided through <code>Result</code>, <code>FinalResult</code>, <code>FinalRuleResult</code> and <code>FinalDictationResult</code> interfaces is inaccurate, or more typically, not available. </p>
   <p><a name="38069"></a>Some possibilities that an application must consider: </p>
   <ul>
    <li><a name="38083"></a>There are no finalized tokens (<code>numTokens</code> returns 0). </li>
   </ul> 
   <ul>
    <li><a name="38085"></a>The <code>GRAMMAR_FINALIZED</code> event was not issued, so the <code>getGrammar</code> method returns <code>null</code>. In this case, all the methods of the <code>FinalRuleResult</code> and <code>FinalDictationResult</code> interfaces throw exceptions. </li>
   </ul> 
   <ul>
    <li><a name="38109"></a>Audio data and training information may be unavailable, even when requested. </li>
   </ul> 
   <ul>
    <li><a name="38110"></a>All tokens provided as best guesses or alternative guesses may be incorrect. </li>
   </ul> 
   <ul>
    <li><a name="38111"></a>If the result does match a <code>RuleGrammar</code>, there is not a guarantee that the tokens can be parsed successfully against the grammar. </li>
   </ul> 
   <p><a name="38125"></a>Finally, a repeat of the warning. Only use rejected results if you really know what you are doing! </p> 
   <a name="12857"></a>
   <h3>6.7.14 &nbsp; &nbsp; Result Timing </h3> 
   <p><a name="23149"></a>Recognition of speech is not an instant process. There are intrinsic delays between the time the user starts or ends speaking a word or sentence and the time at which the corresponding result event is issued by the speech recognizer. </p>
   <p><a name="23173"></a>The most significant delay for most applications is the time between when the user stops speaking and the <code>RESULT_ACCEPTED</code> or <code>RESULT_REJECTED</code> event that indicates the recognizer has finalized the result. </p>
   <p><a name="37876"></a>The minimum finalization time is determined by the <code>CompleteTimeout</code> parameter that is set through the <code>RecognizerProperties</code> interface. This time-out indicates the period of silence after speech that the recognizer should process before finalizing a result. If the time-out is too long, the response of the recognizer (and the application) is unnecessarily delayed. If the time-out is too short, the recognizer may inappropriately break up a result (e.g. finalize a result while the user is taking a quick breath). Typically values are less than a second, but not usually less than 0.3sec. </p>
   <p><a name="23175"></a>There is also an <code>IncompleteTimeout</code> parameter that indicates the period of silence a recognizer should process if the user has said something that may only partially matches an active grammar. This time-out indicates how long a recognizer should wait before rejecting an incomplete sentence. This time-out also indicates how long a recognizer should wait mid-sentence if a result could be accepted, but could also be continued and accepted after more words. The <code>IncompleteTimeout </code>is usually longer than the complete time-out. </p>
   <p><a name="37900"></a>Latency is the overall delay between a user finishing speaking and a result being produced. There are many factors that can affect latency. Some effects are temporary, others reflect the underlying design of speech recognizers. Factors that can increase latency include: </p>
   <ul>
    <li><a name="12864"></a>The <code>CompleteTimeout</code> and <code>IncompleteTimeout</code> properties discussed above. </li>
   </ul> 
   <ul>
    <li><a name="23209"></a><em>Computer power</em> (especially CPU speed and memory): less powerful computers may process speech slower than real-time. Most systems try to catch up while listening to background silence (which is easier to process than real speech). </li>
   </ul> 
   <ul>
    <li><a name="23200"></a><em>Grammar complexity</em>: larger and more complex grammars tend to require more time to process. In most cases, rule grammars are processed more quickly than dictation grammars. </li>
   </ul> 
   <ul>
    <li><a name="23220"></a><em>Suspending</em>: while a recognizer is in the <code>SUSPENDED</code> state, it must buffer of incoming audio. When it returns to the <code>LISTENING</code> state it must catch up by processing the buffered audio. The longer the recognizer is suspended, the longer it can take to catch up to real time and the more latency increases. </li>
   </ul> 
   <ul>
    <li><a name="23223"></a><em>Client/server latencies</em>: in client/server architectures, communication of the audio data, results, and other information between the client and server can introduce delays. </li>
   </ul> 
   <a name="23002"></a>
   <h3>6.7.15 &nbsp; &nbsp; Storing Results </h3> 
   <p><a name="23239"></a>Result objects can be stored for future processing. This is particularly useful for dictation applications in which the correction information, audio data and alternative token information is required in future sessions on the same document because that stored information can assist document editing. </p>
   <p><a name="23004"></a>The <code>Result</code> object is recognizer-specific. This is because each recognizer provides an implementation of the <code>Result</code> interface. The implications are that (a) recognizers do not usually understand each other's results, and (b) a special mechanism is required to store and load result objects (standard Java object serialization is not sufficient). </p>
   <p><a name="23005"></a>The <code>Recognizer</code> interface defines the methods <code>writeVendorResult</code> and <code>readVendorResult</code> to perform this function. These methods write to an <code>OutputStream</code> and read from an <code>InputStream</code> respectively. If the correction information and audio data for a result are available, then they will be stored by this call. Applications that do not need to store this extra data should explicitly release it before storing a result. </p>
   <dl>
    <dd>
     <pre>
      <hr>
<a name="23006"></a>{
<a name="23007"></a>	Recognizer rec;
<a name="23008"></a>	OutputStream stream;
<a name="23009"></a>	Result result;
<a name="23010"></a>	...
<a name="23011"></a>	try {
<a name="30947"></a>		rec.writeVendorResult(stream, result);
<a name="30948"></a>	} catch (Exception e) {
<a name="30951"></a>		e.printStackTrace();
<a name="30949"></a>	}
<a name="23012"></a>}


      <hr></pre>
    </dd>
   </dl>
   <p><a name="26681"></a>A limitation of storing vendor-specific results is that a compatible recognizer must be available to read the file. Applications that need to ensure a file containing a result can be read, even if no recognizer is available, should wrap the result data when storing it to the file. When re-loading the file at a later time, the application will unwrap the result data and provide it to a recognizer only if a suitable recognizer is available. One way to perform the wrapping is to provide the <code>writeVendorResult</code> method with a <code>ByteArrayOutputStream</code> to temporarily place the result in a byte array before storing to a file. </p>
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="15438"></a>
   <h2>6.8 &nbsp; &nbsp; Recognizer Properties </h2> 
   <p><a name="23249"></a>A speech engine has both persistent and run-time adjustable properties. The persistent properties are defined in the <code>RecognizerModeDesc</code> which includes properties inherited from <code>EngineModeDesc</code> (see <a href="SpeechEngine.html#9171">Section 4.2</a>). The persistent properties are used in the selection and creation of a speech recognizer. Once a recognizer has been created, the same property information is available through the <code>getEngineModeDesc</code> method of a <code>Recognizer</code> (inherited from the <code>Engine</code> interface). </p>
   <p><a name="23254"></a>A recognizer also has seven run-time adjustable properties. Applications get and set these properties through <code>RecognizerProperties</code> which extends the <code>EngineProperties</code> interface. The <code>RecognizerProperties</code> for a recognizer are provided by the <code>getEngineProperties</code> method that the <code>Recognizer</code> inherits from the <code>Engine</code> interface. For convenience a <code>getRecognizerProperties</code> method is also provided in the <code>Recognizer</code> interface to return a correctly cast object. </p>
   <p><a name="34221"></a>The get and set methods of <code>EngineProperties</code> and <code>RecognizerProperties</code> follow the JavaBeans conventions with the form: </p>
   <dl>
    <dd>
     <pre><a name="23301"></a>	Type getPropertyName();<br>
	void setPropertyName(Type);
</pre>
    </dd>
   </dl>
   <p><a name="23599"></a>A recognizer can choose to ignore unreasonable values provided to a set method, or can provide upper and lower bounds.</p>
   <p> </p>
   <table border="3" cellspacing="5" cellpadding="6" align="center" bgcolor="#f0f0f0"> 
    <caption>
     <a name="23331"></a>
     <b> Table 6-10 Run-time Properties of a Recognizer </b> 
    </caption> 
    <tbody>
     <tr valign="top">
      <td><a name="23335"></a><b>Property </b> &nbsp;</td>
      <td><a name="23337"></a><b>Description </b> &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="23339"></a><code>ConfidenceLevel </code> &nbsp;</td>
      <td><a name="23341"></a>float value in the range 0.0 to 1.0. Results are rejected if the engine is not confident that it has correctly determined the spoken text. A value of 1.0 requires a recognizer to have maximum confidence in every result so more results are likely to be rejected. A value of 0.0 requires low confidence indicating fewer rejections. 0.5 is the recognizer's default. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="23343"></a><code>Sensitivity </code> &nbsp;</td>
      <td><a name="23345"></a>float value between 0.0 and 1.0. A value of 0.5 is the default for the recognizer. 1.0 gives maximum sensitivity, making the recognizer sensitive to quiet input but more sensitive to noise. 0.0 gives minimum sensitivity, requiring the user to speak loudly and making the recognizer less sensitive to background noise. <a name="23394"></a>Note: some recognizers set the gain automatically during use, or through a setup "Wizard". On these recognizers the sensitivity adjustment should be used only in cases where the automatic settings are not adequate. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="23347"></a><code>SpeedVsAccuracy </code> &nbsp;</td>
      <td><a name="23349"></a>float value between 0.0 and 1.0. 0.0 provides the fastest response. 1.0 maximizes recognition accuracy. 0.5 is the default value for the recognizer which the manufacturer determines as the best compromise between speed and accuracy. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="23351"></a><code>CompleteTimeout </code> &nbsp;</td>
      <td><a name="23494"></a>float value in seconds that indicates the minimum period between when a speaker stops speaking (silence starts) and the recognizer finalizing a result. The complete time-out is applied when the speech prior to the silence matches an active grammar (c.f. IncompleteTimeout). <a name="23525"></a>A long complete time-out value delays the result and makes the response slower. A short time-out may lead to an utterance being broken up inappropriately (e.g. when the user takes a breath). Complete time- out values are typically in the range of 0.3 seconds to 1.0 seconds. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="23355"></a><code>IncompleteTimeout </code> &nbsp;</td>
      <td><a name="23569"></a>float value in seconds that indicates the minimum period between when a speaker stops speaking (silence starts) and the recognizer finalizing a result. The incomplete time-out is applied when the speech prior to the silence does not match an active grammar (c.f. CompleteTimeout). In effect, this is the period the recognizer will wait before rejecting an incomplete utterance. <a name="23580"></a>The IncompleteTimeout is typically longer than the CompleteTimeout. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="27278"></a><code>ResultNumAlternatives </code> &nbsp;</td>
      <td><a name="27270"></a>integer value indicating the preferred maximum number of N-best alternatives in FinalDictationResult and FinalRuleResult objects (see Section 6.7.9). Returning alternatives requires additional computation. <a name="27327"></a>Recognizers do not always produce the maximum number of alternatives (for example, because some alternatives are rejected), and the number of alternatives may vary between results and between tokens. A value of 0 or 1 requests that no alternatives be provided - only a best guess. &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="23359"></a><code>ResultAudioProvided </code> &nbsp;</td>
      <td><a name="23587"></a>boolean value indicating whether the application wants the recognizer to audio with FinalResult objects. Recognizers that do provide result audio can ignore this call. (See Result Audio for details.) &nbsp; </td>
     </tr>
     <tr valign="top">
      <td><a name="23363"></a><code>TrainingProvided </code> &nbsp;</td>
      <td><a name="23682"></a>boolean value indicating whether the application wants the recognizer to support training with FinalResult objects. &nbsp; </td>
     </tr>
    </tbody>
   </table> 
   <table> 
    <tbody>
     <tr>
      <td> </td>
     </tr>
    </tbody>
   </table> 
   <p></p>
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="13004"></a>
   <h2>6.9 &nbsp; &nbsp; Speaker Management </h2> 
   <p><a name="38824"></a>A <code>Recognizer</code> may, optionally, provide a <code>SpeakerManager</code> object. The <code>SpeakerManager</code> allows an application to manage the <code>SpeakerProfiles</code> of that <code>Recognizer</code>. The <code>SpeakerManager</code> for is obtained through <code>getSpeakerManager</code> method of the <code>Recognizer</code> interface. Recognizers that do not maintain speaker profiles - known as speaker-independent recognizers - return <code>null</code> for this method. </p>
   <p><a name="38759"></a>A <code>SpeakerProfile</code> object represents a single enrollment to a recognizer. One user may have multiple <code>SpeakerProfiles</code> in a single recognizer, and one recognizer may store the profiles of multiple users. </p>
   <p><a name="38856"></a>The <code>SpeakerProfile</code> class is a reference to data stored with the recognizer. A profile is identified by three values all of which are <code>String</code> objects: </p>
   <ul>
    <li><a name="38857"></a><code>id</code>: A unique identifier for a profile (per-recognizer unique). The string may be automatically generated but should be printable. </li>
   </ul> 
   <ul>
    <li><a name="38864"></a><code>name</code>: An identifier for a user. This may be an account name or any other name that could be entered by a user. </li>
   </ul> 
   <ul>
    <li><a name="38865"></a><code>variant</code>: The variant identifies a particular enrollment of a user and becomes useful when one user has more than one enrollment. </li>
   </ul> 
   <p><a name="38896"></a>The <code>SpeakerProfile</code> object is a handle to all the stored data the recognizer has about a speaker in a particular enrollment. Except for the three values defined above, the speaker data stored with a profile is internal to the recognizer. </p>
   <p><a name="38916"></a>Typical data stored by a recognizer with the profile might include: </p>
   <ul>
    <li><a name="38882"></a><em>Full speaker data</em>: Full name, age, gender and so on. </li>
   </ul> 
   <ul>
    <li><a name="38934"></a><em>Speaker preferences</em>: Settings such as those provided through the <code>RecognizerProperties</code> (see <a href="Recognition.html#15438">Section 6.8</a>). </li>
   </ul> 
   <ul>
    <li><a name="38943"></a><em>Language models</em>: Data about the words and word patterns of the speaker. </li>
   </ul> 
   <ul>
    <li><a name="38951"></a><em>Word models</em>: Data about the pronunciation of words by the speaker. </li>
   </ul> 
   <ul>
    <li><a name="38959"></a><em>Acoustic models</em>: Data about the speaker's voice and speaking style. </li>
   </ul> 
   <ul>
    <li><a name="38967"></a><em>History</em>: Records of previous training information and usage history </li>
   </ul> 
   <p><a name="38982"></a>The primary role of stored profiles is in maintaining information that enables a recognition to adapt to characteristics of the speaker. The goal of this adaptation is to improve the performance of the speech recognizer including both recognition accuracy and speed. </p>
   <p><a name="38760"></a>The <code>SpeakerManager</code> provides management of all the profiles stored in the recognizer. Most often, the functionality of the <code>SpeakerManager</code> is used as a direct consequence of user actions, typically by providing an enrollment window to the user. The functionality provided includes: </p>
   <ul>
    <li><a name="38995"></a><em>Current speaker</em>: The <code>getCurrentSpeaker</code> and <code>setCurrentSpeaker</code> methods determine which speaker profile is currently being used to recognize incoming speech. </li>
   </ul> 
   <ul>
    <li><a name="38996"></a><em>Listing profiles</em>: The <code>listKnownSpeakers</code> method returns an array of all the <code>SpeakerProfiles</code> known to the recognizer. A common procedure is to display that list to a user to allow the user to select a profile. </li>
   </ul> 
   <ul>
    <li><a name="39010"></a><em>Creation and deletion</em>: The <code>newSpeakerProfile</code> and <code>newSpeakerProfile</code> methods create a new profile or delete a profile in the recognizer. </li>
   </ul> 
   <ul>
    <li><a name="39015"></a><em>Read and write</em>: The <code>readVendorSpeakerProfile</code> and <code>writeVendorSpeakerProfile</code> methods allow a speaker profile and all the recognizer's associated data to be read from or stored to a <code>Stream</code>. The data format will typically be proprietary. </li>
   </ul> 
   <ul>
    <li><a name="39033"></a><em>Save and revert</em>: During normal operation, a recognizer will maintain and update the speaker profile as new information becomes available. Some of the events that may modify the profile include changing the <code>RecognizerProperties</code>, making a correction to a result, producing any result that allows the recognizer to adapt its models, and more many activities. It is normal to save the updated profile at the end of any session by calling <code>saveCurrentSpeakerProfile</code>. In some cases, however, a user's data may be corrupted (e.g., because they loaned their computer to another user). In this case, the application may be requested by a user to revert the profile to the last stored version by calling <code>revertCurrentSpeaker</code>. </li>
   </ul> 
   <ul>
    <li><a name="39043"></a><em>Display component</em>: The <code>getControlComponent</code> method optionally returns an AWT <code>Component</code> object that can be displayed to a user. If supported, this component should expose the vendor's speaker management capabilities which may be more detailed than those provided by the <code>SpeakerManager</code> interface. The vendor functionality may also be proprietary. </li>
   </ul> 
   <p><a name="39053"></a>An individual speaker profile may be large (perhaps several MByte) so storing, loading, creating and otherwise manipulating these objects can be slow. </p>
   <p><a name="38764"></a>The <code>SpeakerManager</code> is one of the capabilities of a <code>Recognizer</code> that is available in the deallocated state. The purpose is to allow an application to indicate the initial speaker profile to be loaded when the recognizer is allocated. To achieve this, the <code>listKnownSpeakers</code>, <code>getCurrentSpeaker</code> and <code>setCurrentSpeaker</code> methods can be called before calling the <code>allocate</code> method. </p>
   <p><a name="39076"></a>To facilitate recognizer selection, the list of speaker profiles is also a property of a recognizer presented through the <code>RecognizerModeDesc</code> class. This allows an application to select a recognizer that has already been trained by a user, if one is available. </p>
   <p><a name="39080"></a>In most cases, a <code>Recognizer</code> persistently restores the last used speaker profile when allocating a recognizer, unless asked to do otherwise. </p>
   <p>&nbsp;</p> 
   <hr width="30%" align="center" noshade> 
   <p>&nbsp;</p> 
   <a name="38507"></a>
   <h2>6.10 &nbsp; &nbsp; Recognizer Audio </h2> 
   <p><a name="39088"></a>The current audio functionality of the Java Speech API is incompletely specified. Once a standard mechanism is established for streaming input and output audio on the Java platform the API will be revised to incorporate that functionality. </p>
   <p><a name="39089"></a>In this release of the API, the only established audio functionality is provided through the <code>RecognizerAudioListener</code> interface and the <code>RecognizerAudioEvent</code> class. Audio events issued by a recognizer are intended to support simple feedback mechanisms for a user. The three types of <code>RecognizerAudioEvent</code> are as follows: </p>
   <ul>
    <li><a name="39099"></a><code>SPEECH_STARTED</code> and <code>SPEECH_STOPPED</code>: These events are issued when possible speech input is detected in the audio input stream. These events are usually based on a crude mechanism for speech detection so a <code>SPEECH_STARTED</code> event is not always followed by output of a result. Furthermore, one <code>SPEECH_STARTED</code> may be followed by multiple results, and one result might cover multiple <code>SPEECH_STARTED</code> events. </li>
   </ul> 
   <ul>
    <li><a name="39116"></a><code>AUDIO_LEVEL</code>: This event is issued periodically to indicate the volume of audio input to the recognizer. The level is a <code>float</code> and varies on a scale from 0.0 to 1.0: silence to maximum volume. The audio level is often displayed visually as a "VU Meter" - the scale on a stereo system that goes up and down with the volume. </li>
   </ul> 
   <p><a name="39119"></a>All the <code>RecognizerAudioEvents</code> are produced as audio reaches the input to the recognizer. Because recognizers use internal buffers between audio input and the recognition process, the audio events can run ahead of the recognition process. </p> 
   <hr size="7" noshade> 
  </blockquote> 
  <!-- This inserts footnotes-->
  <p> </p>
  <table align="center" cellspacing="20" cellpadding="6" bgcolor="#f0f0ff"> 
   <tbody>
    <tr> 
     <td><code> <a href="index.html">Contents</a> </code> </td>
     <td><code> <a href="Synthesis.html">Previous</a> </code> </td>
     <td><code> &nbsp;&nbsp;<a href="">Next</a>&nbsp;&nbsp; </code> </td>
    </tr>
   </tbody>
  </table>  
  <hr width="30%" noshade align="center"> 
  <p align="center"> <font size="-1">Java<sup><font size="-2">TM</font></sup> Speech API Programmer's Guide<br> <a href="Copyright.html">Copyright © 1997-1998</a> Sun Microsystems, Inc. All rights reserved<br> Send comments or corrections to <a href="mailto:javaspeech-comments@sun.com">javaspeech-comments@sun.com</a> </font> </p> 
 </body>
</html>