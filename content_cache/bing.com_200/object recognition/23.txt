<!doctype html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:dc="http://purl.org/dc/terms/" xmlns:doi="http://dx.doi.org/" lang="en" xml:lang="en" itemscope itemtype="http://schema.org/Article" class="no-js">
 <head prefix="og: http://ogp.me/ns#"> 
  <title>Why is Real-World Visual Object Recognition Hard?</title> 
  <link rel="stylesheet" href="/ploscompbiol/resource/compiled/asset_V7OLX4LSII2Y4DADFWUTOT572TJ6EECJ.css"> 
  <!-- allows for  extra head tags --> 
  <!-- hello --> 
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600"> 
  <link media="print" rel="stylesheet" type="text/css" href="/ploscompbiol/resource/css/print.css"> 
  <script type="text/javascript">
        var siteUrlPrefix = "/ploscompbiol/";
    </script> 
  <script src="/ploscompbiol/resource/compiled/asset_SC5JIUGEUPR4P4P6VBUINUVOVUSU3NRY.js"></script> 
  <link rel="shortcut icon" href="/ploscompbiol/resource/img/favicon.ico" type="image/x-icon"> 
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"> 
  <link rel="canonical" href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0040027"> 
  <meta name="description" content="Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist's “null” model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a “simpler” recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation."> 
  <meta name="keywords" content="Object recognition,Human performance,Vision,Principal component analysis,Support vector machines,Grayscale,Imaging techniques,Primates"> 
  <meta name="citation_doi" content="10.1371/journal.pcbi.0040027"> 
  <meta name="citation_author" content="Nicolas Pinto"> 
  <meta name="citation_author_institution" content="McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"> 
  <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"> 
  <meta name="citation_author" content="David D Cox"> 
  <meta name="citation_author_institution" content="McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"> 
  <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"> 
  <meta name="citation_author_institution" content="The Rowland Institute at Harvard, Cambridge, Massachusetts, United States of America"> 
  <meta name="citation_author" content="James J DiCarlo"> 
  <meta name="citation_author_institution" content="McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"> 
  <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"> 
  <meta name="citation_title" content="Why is Real-World Visual Object Recognition Hard?"> 
  <meta itemprop="name" content="Why is Real-World Visual Object Recognition Hard?"> 
  <meta name="citation_journal_title" content="PLOS Computational Biology"> 
  <meta name="citation_journal_abbrev" content="PLOS Computational Biology"> 
  <meta name="citation_date" content="Jan 25, 2008"> 
  <meta name="citation_firstpage" content="e27"> 
  <meta name="citation_issue" content="1"> 
  <meta name="citation_volume" content="4"> 
  <meta name="citation_issn" content="1553-7358"> 
  <meta name="citation_publisher" content="Public Library of Science"> 
  <meta name="citation_pdf_url" content="http://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.0040027&amp;type=printable"> 
  <meta name="dc.identifier" content="10.1371/journal.pcbi.0040027"> 
  <meta name="twitter:card" content="summary"> 
  <meta name="twitter:site" content="@ploscompbiol"> 
  <meta name="twitter:title" content="Why is Real-World Visual Object Recognition Hard?"> 
  <meta property="twitter:description" content="Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist's “null” model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a “simpler” recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation."> 
  <meta property="twitter:image" content="http://journals.plos.org/ploscompbiol/article/figure/image?id=10.1371/journal.pcbi.0040027.g002&amp;size=inline"> 
  <meta property="og:type" content="article"> 
  <meta property="og:url" content="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0040027"> 
  <meta property="og:title" content="Why is Real-World Visual Object Recognition Hard?"> 
  <meta property="og:description" content="Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist's “null” model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a “simpler” recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation."> 
  <meta property="og:image" content="http://journals.plos.org/ploscompbiol/article/figure/image?id=10.1371/journal.pcbi.0040027.g002&amp;size=inline"> 
  <meta name="citation_reference" content="citation_title=Untangling invariant object recognition.;citation_author=JJ DiCarlo;citation_author=DD Cox;citation_journal_title=Trends Cogn Sci;citation_volume=11;citation_number=11;citation_first_page=333;citation_last_page=341;citation_publication_date=2007;"> 
  <meta name="citation_reference" content="citation_title=Object recognition with cortex-like mechanisms.;citation_author=T Serre;citation_author=L Wolf;citation_author=S Bileschi;citation_author=M Riesenhuber;citation_author=T Poggio;citation_journal_title=IEEE PAMI;citation_volume=9;citation_number=9;citation_first_page=411;citation_last_page=426;citation_publication_date=2007;"> 
  <meta name="citation_reference" content="citation_title=Distinctive image features from scale-invariant keypoints.;citation_author=DG Lowe;citation_journal_title=IEEE IJCV;citation_volume=60;citation_number=60;citation_first_page=91;citation_last_page=110;citation_publication_date=2004;"> 
  <meta name="citation_reference" content="citation_title=SVM-KNN: Discriminative nearest neighbor classification for visual category recognition.;citation_author=H Zhang;citation_author=AC Berg;citation_author=M Maire;citation_author=J Malik;citation_journal_title=IEEE CVPR 2006;citation_first_page=2126;citation_last_page=2136;citation_publication_date=2006;"> 
  <meta name="citation_reference" content="citation_title=Unsupervised learning of models for recognition.;citation_author=M Weber;citation_author=M Welling;citation_author=P Perona;citation_journal_title=IEEE ECCV 2000;citation_first_page=18;citation_last_page=32;citation_publication_date=2000;"> 
  <meta name="citation_reference" content="citation_title=Map-seeking circuits in visual cognition: A computational mechanism for biological and machine vision;citation_author=D Arathorn;citation_publication_date=2002;citation_publisher=Stanford University Press"> 
  <meta name="citation_reference" content="citation_title=A natural approach to studying vision.;citation_author=G Felsen;citation_author=Y Dan;citation_journal_title=Nat Neurosci;citation_volume=8;citation_number=8;citation_first_page=1643;citation_last_page=1646;citation_publication_date=2005;"> 
  <meta name="citation_reference" content="citation_title=In praise of artifice.;citation_author=NC Rust;citation_author=JA Movshon;citation_journal_title=Nat Neurosci;citation_volume=8;citation_number=8;citation_first_page=1647;citation_last_page=1649;citation_publication_date=2005;"> 
  <meta name="citation_reference" content="citation_title=Neural activity in areas V1, V2, and V4 during free viewing of natural scenes compared to control images.;citation_author=JL Gallant;citation_author=CE Connor;citation_author=DC Van Essen;citation_journal_title=Neuroreport;citation_volume=9;citation_number=9;citation_first_page=85;citation_last_page=89;citation_publication_date=1998;"> 
  <meta name="citation_reference" content="citation_title=How do visual neurons respond in the real world?;citation_author=P Reinagel;citation_journal_title=Curr Opin Neurobiol;citation_volume=11;citation_number=11;citation_first_page=437;citation_last_page=442;citation_publication_date=2001;"> 
  <meta name="citation_reference" content="citation_title=The “independent components” of natural scenes are edge filters.;citation_author=AJ Bell;citation_author=TJ Sejnowski;citation_journal_title=Vision Res;citation_volume=37;citation_number=37;citation_first_page=3327;citation_last_page=3338;citation_publication_date=1997;"> 
  <meta name="citation_reference" content="citation_title=Natural image statistics and neural representation.;citation_author=EP Simoncelli;citation_author=BA Olshausen;citation_journal_title=Annu Rev Neurosci;citation_volume=24;citation_number=24;citation_first_page=1193;citation_last_page=1216;citation_publication_date=2001;"> 
  <meta name="citation_reference" content="citation_title=Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories.;citation_author=L Fei-Fei;citation_author=R Fergus;citation_author=P Perona;citation_journal_title=IEEE CVPR;citation_volume=2004;citation_number=2,004;citation_first_page=178;citation_publication_date=2004;"> 
  <meta name="citation_reference" content="citation_title=Dataset issues in object recognition.;citation_author=J Ponce;citation_author=TL Berg;citation_author=M Everingham;citation_author=DA Forsyth;citation_author=M Hebert;citation_first_page=29;citation_last_page=48;citation_publication_date=2006;citation_publisher=Lect Notes Comput Sci"> 
  <meta name="citation_reference" content="citation_title=Unsupervised learning of visual features through Spike Timing Dependent Plasticity.;citation_author=T Masquelier;citation_author=SJ Thorpe;citation_journal_title=PLoS Comp Bio;citation_volume=3;citation_number=3;citation_publication_date=2007;"> 
  <meta name="citation_reference" content="citation_title=The duration of the attentional blink in natural scenes depends on stimulus category.;citation_author=W Einhäuser;citation_author=C Koch;citation_author=S Makeig;citation_journal_title=Vision Res;citation_volume=47;citation_number=47;citation_first_page=597;citation_last_page=607;citation_publication_date=2007;"> 
  <meta name="citation_reference" content="citation_title=Caltech-256 Object Category Dataset;citation_author=G Griffin;citation_author=A Holub;citation_author=P Perona;citation_publication_date=2007;citation_publisher=Caltech Technical Report"> 
  <meta name="citation_reference" content="citation_title=Using dependent regions for object categorization in a generative framework.;citation_author=G Wang;citation_author=Y Zhang;citation_author=L Fei-Fei;citation_journal_title=IEEE CVPR;citation_volume=2006;citation_number=2,006;citation_first_page=1597;citation_last_page=1604;citation_publication_date=2006;"> 
  <meta name="citation_reference" content="citation_title=Multiclass object recognition with sparse, localized features.;citation_author=J Mutch;citation_author=DG Lowe;citation_journal_title=IEEE CVPR;citation_volume=2006;citation_number=2,006;citation_first_page=11;citation_last_page=18;citation_publication_date=2006;"> 
  <meta name="citation_reference" content="citation_title=Beyond bags of features: spatial pyramid matching for recognizing natural scene categories.;citation_author=S Lazebnik;citation_author=C Schmid;citation_author=J Ponce;citation_journal_title=IEEE CVPR;citation_volume=2006;citation_number=2,006;citation_first_page=2169;citation_last_page=2178;citation_publication_date=2006;"> 
  <meta name="citation_reference" content="citation_title=Pyramid match kernels: Discriminative classification with sets of image features (version 2);citation_author=K Grauman;citation_author=T Darrell;citation_publication_date=2006;citation_publisher=MIT"> 
  <meta name="citation_reference" content="citation_title=PASCAL Object Recognition Database Collection, Visual Object Classes Challenge;"> 
  <meta name="citation_reference" content="citation_title=Learning the discriminative power-invariance trade-off.;citation_author=M Varma;citation_author=D Ray;citation_publication_date=2007;citation_publisher=IEEE ICCV"> 
  <meta name="citation_reference" content="citation_title=Fast readout of object identity from macaque inferior temporal cortex.;citation_author=CP Hung;citation_author=G Kreiman;citation_author=T Poggio;citation_author=JJ DiCarlo;citation_journal_title=Science;citation_volume=310;citation_number=310;citation_first_page=863;citation_last_page=866;citation_publication_date=2005;"> 
  <meta name="citation_reference" content="citation_title=LabelMe: a database and web-based tool for image annotation;citation_author=B Russell;citation_author=A Torralba;citation_author=K Murphy;citation_author=WT Freeman;citation_publication_date=2005;citation_publisher=MIT Artificial Intelligence Lab Memo AIM-2005–025"> 
  <meta name="citation_reference" content="citation_title=Peekaboom: a game for locating objects in images.;citation_author=L Von Ahn;citation_author=R Liu;citation_author=M Blum;citation_journal_title=ACM SIGCHI;citation_volume=2006;citation_number=2,006;citation_first_page=55;citation_last_page=64;citation_publication_date=2006;"> 
  <meta name="citation_reference" content="citation_title=StreetScenes: Towards scene understanding in still images;citation_author=S Bileschi;citation_publication_date=2006;citation_publisher=MIT EECS"> 
  <meta name="citation_reference" content="citation_title=Learning methods for generic object recognition with invariance to pose and lighting.;citation_author=Y LeCun;citation_author=FJ Huang;citation_author=L Bottou;citation_journal_title=IEEE CVPR;citation_volume=2004;citation_number=2,004;citation_first_page=97;citation_last_page=104;citation_publication_date=2004;"> 
  <meta name="citation_reference" content="citation_title=Receptive fields, binocular interaction and functional architecture in the cat&amp;apos;s visual cortex.;citation_author=DH Hubel;citation_author=TN Wiesel;citation_journal_title=J Physiol;citation_volume=160;citation_number=160;citation_first_page=106;citation_last_page=154;citation_publication_date=1962;"> 
  <meta name="citation_reference" content="citation_title=Receptive fields and functional architecture of monkey striate cortex.;citation_author=DH Hubel;citation_author=TN Wiesel;citation_journal_title=J Physiol;citation_volume=195;citation_number=195;citation_first_page=215;citation_last_page=243;citation_publication_date=1968;"> 
  <meta name="citation_reference" content="citation_title=An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex.;citation_author=JP Jones;citation_author=LA Palmer;citation_journal_title=J Neurophysiol;citation_volume=58;citation_number=58;citation_first_page=1233;citation_last_page=1258;citation_publication_date=1987;"> 
  <meta name="citation_reference" content="citation_title=LIBSVM: a Library for Support Vector Machines;citation_author=CC Chang;citation_author=CJ Lin;citation_publication_date=2001;"> 
  <!-- DoubleClick overall ad setup script --> 
  <script type="text/javascript">
  var googletag = googletag || {};
  googletag.cmd = googletag.cmd || [];
  (function() {
    var gads = document.createElement('script');
    gads.async = true;
    gads.type = 'text/javascript';
    var useSSL = 'https:' == document.location.protocol;
    gads.src = (useSSL ? 'https:' : 'http:') +
        '//www.googletagservices.com/tag/js/gpt.js';
    var node = document.getElementsByTagName('script')[0];
    node.parentNode.insertBefore(gads, node);
  })();
</script> 
  <!-- DoubleClick ad slot setup script --> 
  <script id="doubleClickSetupScript" type="text/javascript">
    googletag.cmd.push(function() {
  googletag.defineSlot('/75507958/PCOMPBIO_728x90_ATF', [728, 90], 'div-gpt-ad-1458247671871-0').addService(googletag.pubads());
  googletag.defineSlot('/75507958/PCOMPBIO_160x600_BTF', [160, 600], 'div-gpt-ad-1458247671871-1').addService(googletag.pubads());
      googletag.pubads().enableSingleRequest();
      googletag.enableServices();
    });
  </script> 
  <script type="text/javascript">
    var WombatConfig = WombatConfig || {};
    WombatConfig.resourcePath = "/ploscompbiol/resource/";
    WombatConfig.imgPath = "/ploscompbiol/resource/img/";
    WombatConfig.journalKey = "PLoSCompBiol";
    WombatConfig.figurePath = "/ploscompbiol/article/figure/image";
    WombatConfig.figShareInstitutionString = "plos";
    WombatConfig.doiResolverPrefix = "http://dx.plos.org/";
</script> 
  <script type="text/javascript">
  var WombatConfig = WombatConfig || {};
  WombatConfig.metrics = WombatConfig.metrics || {};
  WombatConfig.metrics.referenceUrl      = "http://lagotto.io/plos";
  WombatConfig.metrics.googleScholarUrl  = "http://scholar.google.com/scholar";
  WombatConfig.metrics.googleScholarCitationUrl  = WombatConfig.metrics.googleScholarUrl + "?hl=en&lr=&cites=";
  WombatConfig.metrics.crossrefUrl  = "http://www.crossref.org";
</script>
  <script src="//code.jquery.com/jquery-2.1.4.min.js"></script> 
  <script>window.jQuery || document.write('<script src="/ploscompbiol/resource/js/vendor/jquery-2.1.4.min.js""><\/script>')</script> 
  <script type="text/javascript" src="https://widgets.figshare.com/static/figshare.js"></script> 
  <!--For Google Tag manager to be able to track site information  --> 
  <script>

  dataLayer = [{
    'mobileSite': 'false',
    'desktopSite': 'true'
  }];

</script> 
 </head> 
 <body class="article ploscompbiol"> 
  <!-- Google Tag Manager --> 
  <noscript>
   <iframe src="//www.googletagmanager.com/ns.html?id=GTM-TP26BH" height="0" width="0" style="display:none;visibility:hidden"></iframe>
  </noscript> 
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TP26BH');</script> 
  <noscript>
   <iframe src="//www.googletagmanager.com/ns.html?id=GTM-MQQMGF" height="0" width="0" style="display:none;visibility:hidden"></iframe>
  </noscript> 
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MQQMGF');</script> 
  <!-- End Google Tag Manager --> 
  <header> 
   <div id="topslot" class="head-top"> 
    <div class="center"> 
     <div class="title">
      Advertisement
     </div> 
     <!-- DoubleClick Ad Zone --> 
     <div class="advertisement" id="div-gpt-ad-1458247671871-0" style="width:728px; height:90px;"> 
      <script type="text/javascript">
      googletag.cmd.push(function() { googletag.display('div-gpt-ad-1458247671871-0'); });
    </script> 
     </div> 
    </div> 
   </div> 
   <div id="user" class="nav"> 
    <ul class="nav-user"> 
     <li><a href="http://www.plos.org">plos.org</a></li> 
     <li><a href="https://community.plos.org/registration/new">create account</a></li> 
     <li class="highlighted"><a href="/ploscompbiol/user/secure/login?page=%2Fploscompbiol%2Farticle%3Fid%3D10.1371%2Fjournal.pcbi.0040027">sign in</a></li> 
    </ul> 
   </div> 
   <div id="pagehdr"> 
    <nav class="nav-main"> 
     <h1 class="logo"> <a href="/ploscompbiol/.">PLOS Computational Biology</a> </h1> 
     <section class="top-bar-section"> 
      <ul class="nav-elements"> 
       <li class="menu-section-header has-dropdown " id="browse"> <span class="menu-section-header-title"> Browse </span> 
        <ul class="menu-section dropdown " id="browse-dropdown-list"> 
         <li> <a href="/ploscompbiol/issue">Current Issue</a> </li> 
         <li> <a href="/ploscompbiol/volume">Journal Archive</a> </li> 
         <li> <a href="/ploscompbiol/s/collections">Collections</a> </li> 
        </ul> </li> 
       <li class="multi-col-parent menu-section-header has-dropdown" id="publish"> Publish 
        <div class="dropdown mega "> 
         <ul class="multi-col" id="publish-dropdown-list"> 
          <li class="menu-section-header " id="submissions"> <span class="menu-section-header-title"> Submissions </span> 
           <ul class="menu-section " id="submissions-dropdown-list"> 
            <li> <a href="/ploscompbiol/s/getting-started">Getting Started</a> </li> 
            <li> <a href="/ploscompbiol/s/presubmission-inquiries">Presubmission Inquiries</a> </li> 
            <li> <a href="/ploscompbiol/s/submission-guidelines">Submission Guidelines</a> </li> 
            <li> <a href="/ploscompbiol/s/figures">Figures</a> </li> 
            <li> <a href="/ploscompbiol/s/tables">Tables</a> </li> 
            <li> <a href="/ploscompbiol/s/supporting-information">Supporting Information</a> </li> 
            <li> <a href="/ploscompbiol/s/latex">LaTeX</a> </li> 
            <li> <a href="/ploscompbiol/s/other-article-types">Other Article Types</a> </li> 
            <li> <a href="/ploscompbiol/s/revising-your-manuscript">Revising Your Manuscript</a> </li> 
            <li> <a href="/ploscompbiol/s/submit-now">Submit Now</a> </li> 
           </ul> </li> 
          <li class="menu-section-header " id="policies"> <span class="menu-section-header-title"> Policies </span> 
           <ul class="menu-section " id="policies-dropdown-list"> 
            <li> <a href="/ploscompbiol/s/best-practices-in-research-reporting">Best Practices in Research Reporting</a> </li> 
            <li> <a href="/ploscompbiol/s/human-subjects-research">Human Subjects Research</a> </li> 
            <li> <a href="/ploscompbiol/s/animal-research">Animal Research</a> </li> 
            <li> <a href="/ploscompbiol/s/competing-interests">Competing Interests</a> </li> 
            <li> <a href="/ploscompbiol/s/disclosure-of-funding-sources">Disclosure of Funding Sources</a> </li> 
            <li> <a href="/ploscompbiol/s/licenses-and-copyright">Licenses and Copyright</a> </li> 
            <li> <a href="/ploscompbiol/s/data-availability">Data Availability</a> </li> 
            <li> <a href="/ploscompbiol/s/materials-and-software-sharing">Materials and Software Sharing</a> </li> 
            <li> <a href="/ploscompbiol/s/ethical-publishing-practice">Ethical Publishing Practice</a> </li> 
            <li> <a href="/ploscompbiol/s/authorship">Authorship</a> </li> 
            <li> <a href="/ploscompbiol/s/downloads-and-translations">Downloads and Translations</a> </li> 
           </ul> </li> 
          <li class="menu-section-header " id="manuscript-review-and-publication"> <span class="menu-section-header-title"> Manuscript Review and Publication </span> 
           <ul class="menu-section " id="manuscript-review-and-publication-dropdown-list"> 
            <li> <a href="/ploscompbiol/s/editorial-and-peer-review-process">Editorial and Peer Review Process</a> </li> 
            <li> <a href="/ploscompbiol/s/reviewer-guidelines">Reviewer Guidelines</a> </li> 
            <li> <a href="/ploscompbiol/s/accepted-manuscripts">Accepted Manuscripts</a> </li> 
            <li> <a href="/ploscompbiol/s/corrections-and-retractions">Corrections and Retractions</a> </li> 
            <li> <a href="/ploscompbiol/s/comments">Comments</a> </li> 
            <li> <a href="/ploscompbiol/s/article-level-metrics">Article-Level Metrics</a> </li> 
           </ul> </li> 
         </ul> 
        </div> </li> 
       <li class="menu-section-header has-dropdown " id="about"> <span class="menu-section-header-title"> About </span> 
        <ul class="menu-section dropdown " id="about-dropdown-list"> 
         <li> <a href="/ploscompbiol/s/journal-information">Journal Information</a> </li> 
         <li> <a href="/ploscompbiol/s/editors-in-chief">Editors-in-Chief</a> </li> 
         <li> <a href="/ploscompbiol/s/editorial-board">Editorial Board</a> </li> 
         <li> <a href="/ploscompbiol/s/publishing-information">Publishing Information</a> </li> 
         <li> <a href="/ploscompbiol/s/publication-fees">Publication Fees</a> </li> 
         <li> <a href="/ploscompbiol/s/press-and-media">Press and Media</a> </li> 
         <li> <a href="/ploscompbiol/s/contact">Contact</a> </li> 
        </ul> </li> 
       <li id="navsearch" class="head-search"> 
        <form name="searchForm" action="/ploscompbiol/search" method="get"> 
         <fieldset> 
          <legend>Search</legend> 
          <label for="search">Search</label> 
          <input id="search" type="text" name="q" placeholder="Search" required> 
          <button id="headerSearchButton" type="submit"><span class="search-icon"></span></button> 
         </fieldset> 
         <input type="hidden" name="filterJournals" value="PLoSCompBiol"> 
        </form> <a id="advSearch" href="/ploscompbiol/search"> advanced search </a> </li> 
      </ul> 
     </section> 
    </nav> 
   </div> 
  </header>
  <main> 
   <div class="set-grid"> 
    <header class="title-block"> 
     <ul id="almSignposts" class="signposts"> 
      <li id="loadingMetrics"> <p>Loading metrics</p> </li> 
     </ul> 
     <script type="text/template" id="signpostsGeneralErrorTemplate">
  <li id="metricsError">Article metrics are unavailable at this time. Please try again later.</li>
</script> 
     <script type="text/template" id="signpostsNewArticleErrorTemplate">
  <li></li><li></li><li id="tooSoon">Article metrics are unavailable for recently published articles.</li>
</script> 
     <script type="text/template" id="signpostsTemplate">
    <li id="almSaves">
      <%= s.numberFormat(saveCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#savedHeader">Save</a>
        <p class="saves-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#savedHeader">Total Mendeley and CiteULike bookmarks.</a></p>
      </div>
    </li>

    <li id="almCitations">
      <%= s.numberFormat(citationCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#citedHeader">Citation</a>
        <p class="citations-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#citedHeader">Paper's citation count computed by Scopus.</a></p>
      </div>
    </li>

    <li id="almViews">
      <%= s.numberFormat(viewCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#viewedHeader">View</a>
        <p class="views-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#viewedHeader">Sum of PLOS and PubMed Central page views and downloads.</a></p>
      </div>
    </li>

    <li id="almShares">
      <%= s.numberFormat(shareCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#discussedHeader">Share</a>
        <p class="shares-tip" data-js-tooltip-hover="target"><a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027#discussedHeader">Sum of Facebook and Twitter activity.</a></p>
      </div>
    </li>
</script> 
     <div class="article-meta"> 
      <div class="classifications"> 
       <p class="license-short" id="licenseShort">Open Access</p> 
       <p class="peer-reviewed" id="peerReviewed">Peer-reviewed</p> 
       <div class="article-type"> 
        <p class="type-article" id="artType">Research Article</p> 
       </div> 
      </div> 
     </div> 
     <div class="article-title-etc"> 
      <div class="title-authors"> 
       <h1 id="artTitle">
        <!--?xml version="1.0" encoding="UTF-8"?-->Why is Real-World Visual Object Recognition Hard?</h1> 
       <ul class="author-list clearfix" data-js-tooltip="tooltip_container" id="author-list"> 
        <li data-js-tooltip="tooltip_trigger"> <a data-author-id="0" class="author-name"> Nicolas Pinto <span class="contribute"> </span>,</a> 
         <div id="author-meta-0" class="author-info" data-js-tooltip="tooltip_target"> 
          <p> <span class="contribute"> </span> Contributed equally to this work with: Nicolas Pinto, David D Cox </p> 
          <p id="authAffiliations-0"><span class="type">Affiliations</span> McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America , Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America </p> 
          <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose0"> ? </a> 
         </div> </li> 
        <li data-js-tooltip="tooltip_trigger"> <a data-author-id="1" class="author-name"> David D Cox <span class="contribute"> </span>,</a> 
         <div id="author-meta-1" class="author-info" data-js-tooltip="tooltip_target"> 
          <p> <span class="contribute"> </span> Contributed equally to this work with: Nicolas Pinto, David D Cox </p> 
          <p id="authAffiliations-1"><span class="type">Affiliations</span> McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America , Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America , The Rowland Institute at Harvard, Cambridge, Massachusetts, United States of America </p> 
          <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose1"> ? </a> 
         </div> </li> 
        <li data-js-tooltip="tooltip_trigger"> <a data-author-id="2" class="author-name"> James J DiCarlo <span class="email"> </span></a> 
         <div id="author-meta-2" class="author-info" data-js-tooltip="tooltip_target"> 
          <p id="authCorresponding-2"> <span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:dicarlo@mit.edu">dicarlo@mit.edu</a></p> 
          <p id="authAffiliations-2"><span class="type">Affiliations</span> McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America , Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America </p> 
          <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose2"> ? </a> 
         </div> </li> 
       </ul> 
      </div> 
      <div id="floatTitleTop" data-js-floater="title_author" class="float-title"> 
       <div class="set-grid"> 
        <div class="float-title-inner"> 
         <h1>
          <!--?xml version="1.0" encoding="UTF-8"?-->Why is Real-World Visual Object Recognition Hard?</h1> 
         <ul id="floatAuthorList" data-js-floater="floated_authors"> 
          <li data-float-index="1">Nicolas Pinto,&nbsp; </li> 
          <li data-float-index="2">David D Cox,&nbsp; </li> 
          <li data-float-index="3">James J DiCarlo </li> 
         </ul> 
        </div> 
        <div class="logo-close" id="titleTopCloser"> 
         <img src="/ploscompbiol/resource/img/logo.plos.95.png" alt="PLOS"> 
         <div class="close-floater" title="close">
          x
         </div> 
        </div> 
       </div> 
      </div> 
      <ul class="date-doi"> 
       <li id="artPubDate">Published: January 25, 2008</li> 
       <li id="artDoi"> <a href="https://doi.org/10.1371/journal.pcbi.0040027">https://doi.org/10.1371/journal.pcbi.0040027</a> </li> 
      </ul> 
     </div> 
     <div> 
     </div> 
    </header> 
    <section class="article-body"> 
     <ul class="article-tabs"> 
      <li class="tab-title active" id="tabArticle"> <a href="/ploscompbiol/article?id=10.1371/journal.pcbi.0040027" class="article-tab-1">Article</a> </li> 
      <li class="tab-title " id="tabAuthors"> <a href="/ploscompbiol/article/authors?id=10.1371/journal.pcbi.0040027" class="article-tab-2">Authors</a> </li> 
      <li class="tab-title " id="tabMetrics"> <a href="/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.0040027" class="article-tab-3">Metrics</a> </li> 
      <li class="tab-title " id="tabComments"> <a href="/ploscompbiol/article/comments?id=10.1371/journal.pcbi.0040027" class="article-tab-4">Comments</a> </li> 
      <li class="tab-title " id="tabRelated"> <a href="/ploscompbiol/article/related?id=10.1371/journal.pcbi.0040027" class="article-tab-5">Related Content</a> </li> 
     </ul> 
     <div class="article-container"> 
      <div id="nav-article"> 
       <ul class="nav-secondary"> 
        <li class="nav-comments" id="nav-comments"> <a href="article/comments?id=10.1371/journal.pcbi.0040027">Reader Comments (1)</a> </li> 
        <li class="nav-media" id="nav-media" data-doi="10.1371/journal.pcbi.0040027"> <a href="/ploscompbiol/article/related?id=10.1371/journal.pcbi.0040027"> Media Coverage <span id="media-coverage-count"></span> </a> </li> 
        <li id="nav-figures"><a href="#" data-doi="10.1371/journal.pcbi.0040027">Figures</a></li> 
       </ul> 
      </div> 
      <div id="figure-lightbox-container"></div> 
      <script id="figure-lightbox-template" type="text/template">
  <div id="figure-lightbox" class="reveal-modal full" data-reveal aria-hidden="true"
       role="dialog">
    <div class="lb-header">
      <h1 id="lb-title"><%= articleTitle %></h1>

      <div id="lb-authors">
        <% authorList.split(',').forEach(function (author) { %>
        <span><%= author.trim() %></span>
        <% }) %>
      </div>

      <div class="lb-close" title="close">&nbsp;</div>
    </div>
    <div class="img-container">
      <div class="loader"> <i class="fa-spinner"></i> </div>
      <img class="main-lightbox-image" src=""/>
      <aside id="figures-list">
        <% figureList.each(function (ix, figure) { %>
        <div class="change-img" data-doi="<%= figure.getAttribute('data-doi') %>">
          <img class="aside-figure" src="/ploscompbiol/article/figure/image?size=inline&id=<%= figure.getAttribute('data-doi') %>" />
        </div>
        <% }) %>
        <div class="dummy-figure">
        </div>
      </aside>
    </div>
    <div id="lightbox-footer">

      <div id="btns-container" class="lightbox-row <% if(figureList.length <= 1) { print('one-figure-only') } %>">
        <div class="fig-btns-container reset-zoom-wrapper left">
          <span class="fig-btn reset-zoom-btn">Reset zoom</span>
        </div>
        <div class="zoom-slider-container">
          <div class="range-slider-container">
            <span id="lb-zoom-min"></span>
            <div class="range-slider round" data-slider data-options="start: 20; end: 200; initial: 20;">
              <span class="range-slider-handle" role="slider" tabindex="0"></span>
              <span class="range-slider-active-segment"></span>
              <input type="hidden">
            </div>
            <span id="lb-zoom-max"></span>
          </div>
        </div>
        <% if(figureList.length > 1) { %>
        <div class="fig-btns-container">
          <span class="fig-btn all-fig-btn"><i class="icon icon-all"></i> All Figures</span>
          <span class="fig-btn next-fig-btn"><i class="icon icon-next"></i> Next</span>
          <span class="fig-btn prev-fig-btn"><i class="icon icon-prev"></i> Previous</span>
        </div>
        <% } %>
      </div>
      <div id="image-context">
      </div>
    </div>
  </div>
</script> 
      <script id="image-context-template" type="text/template">
  <div class="footer-text">
    <div id="figure-description-wrapper">
      <div id="view-more-wrapper" style="<% descriptionExpanded? print('display:none;') : '' %>">
        <span id="figure-title"><%= title %></span>
        <p id="figure-description">
          <%= description %>&nbsp;&nbsp;
        </p>
        <span id="view-more">show more<i class="icon-arrow-right"></i></span>

      </div>
      <div id="view-less-wrapper" style="<% descriptionExpanded? print('display:inline-block;') : '' %>" >
        <span id="figure-title"><%= title %></span>
        <p id="full-figure-description">
          <%= description %>&nbsp;&nbsp;
          <span id="view-less">show less<i class="icon-arrow-left"></i></span>
        </p>
      </div>
    </div>
  </div>
  <div id="show-context-container">
    <a class="btn show-context" href="<%= showInContext(strippedDoi) %>">Show in Context</a>
  </div>
  <div id="download-buttons">
    <h3>Download:</h3>
    <div class="item">
      <a href="/ploscompbiol/article/figure/image?size=original&download=&id=<%= doi %>" title="original image">
        <span class="download-btn">TIFF</span>
      </a>
      <span class="file-size"><%= fileSizes.original %></span>
    </div>
    <div class="item">
      <a href="/ploscompbiol/article/figure/image?size=large&download=&id=<%= doi %>" title="large image">
        <span class="download-btn">PNG</span>
      </a>
      <span class="file-size"><%= fileSizes.large %></span>
    </div>
    <div class="item">
      <a href="/ploscompbiol/article/figure/powerpoint?id=<%= doi %>" title="PowerPoint slide">
        <span class="download-btn">PPT</span>
      </a>
    </div>

  </div>
</script> 
      <div class="article-content"> 
       <div id="figure-carousel-section"> 
        <h2>Figures</h2> 
        <div id="figure-carousel"> 
         <div class="carousel-wrapper"> 
          <div class="slider"> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pcbi.0040027.g001"> 
            <img src="/ploscompbiol/article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.0040027.g001" alt="Figure 1"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pcbi.0040027.g002"> 
            <img src="/ploscompbiol/article/figure/image?size=inline&amp;id=10.1371/journal.pcbi.0040027.g002" alt="Figure 2"> 
           </div> 
          </div> 
         </div> 
         <div class="carousel-control"> 
          <span class="button previous"></span> 
          <span class="button next"></span> 
         </div> 
         <div class="carousel-page-buttons"> 
         </div> 
        </div> 
       </div> 
       <div class="article-text" id="artText"> 
        <div class="abstract toc-section">
         <a id="abstract0" name="abstract0" data-toc="abstract0" class="link-target" title="Abstract"></a>
         <h2>Abstract</h2>
         <a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1" class="link-target"></a>
         <p>Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist's “null” model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a “simpler” recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation.</p> 
        </div>
        <div class="abstract toc-section">
         <a id="abstract1" name="abstract1" class="link-target" data-toc="abstract1" title="Author Summary"></a> 
         <h2>Author Summary</h2> 
         <a id="article1.front1.article-meta1.abstract2.p1" name="article1.front1.article-meta1.abstract2.p1" class="link-target"></a>
         <p>The ease with which we recognize visual objects belies the computational difficulty of this feat. At the core of this challenge is image variation—any given object can cast an infinite number of different images onto the retina, depending on the object's position, size, orientation, pose, lighting, etc. Recent computational models have sought to match humans' remarkable visual abilities, and, using large databases of “natural” images, have shown apparently impressive progress. Here we show that caution is warranted. In particular, we found that a very simple neuroscience “toy” model, capable only of extracting trivial regularities from a set of images, is able to outperform most state-of-the-art object recognition systems on a standard “natural” test of object recognition. At the same time, we found that this same toy model is easily defeated by a simple recognition test that we generated to better span the range of image variation observed in the real world. Together these results suggest that current “natural” tests are inadequate for judging success or driving forward progress. In addition to tempering claims of success in the machine vision literature, these results point the way forward and call for renewed focus on image variation as a central challenge in object recognition.</p> 
        </div> 
        <div class="articleinfo">
         <p><strong>Citation: </strong>Pinto N, Cox DD, DiCarlo JJ (2008) Why is Real-World Visual Object Recognition Hard? PLoS Comput Biol 4(1): e27. https://doi.org/10.1371/journal.pcbi.0040027</p>
         <p><strong>Editor: </strong>Karl J. Friston, University College London, United Kingdom</p>
         <p><strong>Received: </strong>September 26, 2007; <strong>Accepted: </strong>December 10, 2007; <strong>Published: </strong> January 25, 2008</p>
         <p><strong>Copyright: </strong> © 2008 Pinto et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
         <p><strong>Funding: </strong> This work was supported by The National Eye Institute (NIH-R01-EY014970), The Pew Charitable Trusts (PEW UCSF 2893sc), and The McKnight Foundation.</p>
         <p><strong>Competing interests: </strong> The authors have declared that no competing interests exist.</p>
        </div> 
        <div id="section1" class="section toc-section">
         <a id="s1" name="s1" data-toc="s1" class="link-target" title="Introduction"></a>
         <h2>Introduction</h2>
         <a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1" class="link-target"></a>
         <p>Visual object recognition is an extremely difficult computational problem. The core problem is that each object in the world can cast an infinite number of different 2-D images onto the retina as the object's position, pose, lighting, and background vary relative to the viewer (e.g., [<a href="#pcbi-0040027-b001" class="ref-tip">1</a>]). Yet the brain solves this problem effortlessly. Progress in understanding the brain's solution to object recognition requires the construction of artificial recognition systems that ultimately aim to emulate our own visual abilities, often with biological inspiration (e.g., [<a href="#pcbi-0040027-b002" class="ref-tip">2</a>–<a href="#pcbi-0040027-b006" class="ref-tip">6</a>]). Such computational approaches are critically important because they can provide experimentally testable hypotheses, and because instantiation of a working recognition system represents a particularly effective measure of success in understanding object recognition. However, a major challenge is assessing the recognition performance of such models. Ideally, artificial systems should be able to do what our own visual systems can, but it is unclear how to evaluate progress toward this goal. In practice, this amounts to choosing an image set against which to test performance.</p> 
         <a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2" class="link-target"></a>
         <p>Although controversial ([<a href="#pcbi-0040027-b007" class="ref-tip">7</a>,<a href="#pcbi-0040027-b008" class="ref-tip">8</a>]), a popular recent approach in the study of vision is the use of “natural” images [<a href="#pcbi-0040027-b007" class="ref-tip">7</a>,<a href="#pcbi-0040027-b009" class="ref-tip">9</a>–<a href="#pcbi-0040027-b012" class="ref-tip">12</a>], in part because they ostensibly capture the essence of problems encountered in the real world. For example, in computational vision, the Caltech101 image set has emerged as a gold standard for testing “natural” object recognition performance [<a href="#pcbi-0040027-b013" class="ref-tip">13</a>]. The set consists of a large number of images divided into 101 object categories (e.g., images containing planes, cars, faces, flamingos, etc.; see <a href="#pcbi-0040027-g001">Figure 1</a>A) plus an additional “background” category (for 102 categories total). While a number of specific concerns have been raised with this set (see [<a href="#pcbi-0040027-b014" class="ref-tip">14</a>] for more details), its images are still currently widely used by neuroscientists, both in theoretical (e.g., [<a href="#pcbi-0040027-b002" class="ref-tip">2</a>,<a href="#pcbi-0040027-b015" class="ref-tip">15</a>]) and experimental (e.g., [<a href="#pcbi-0040027-b016" class="ref-tip">16</a>]) contexts. The logic of Caltech101 (and sets like it; e.g., Caltech256 [<a href="#pcbi-0040027-b017" class="ref-tip">17</a>]) is that the sheer number of categories and the diversity of those images place a high bar for object recognition systems and require them to solve the computational crux of object recognition. Because there are 102 object categories, chance performance is less than 1% correct. In recent years, several object recognition models (including biologically inspired approaches) have shown what appears to be impressively high performance on this test—better than 60% correct [<a href="#pcbi-0040027-b004" class="ref-tip">4</a>,<a href="#pcbi-0040027-b018" class="ref-tip">18</a>–<a href="#pcbi-0040027-b021" class="ref-tip">21</a>], suggesting that these approaches, while still well below human performance, are at least heading in the right direction.</p> 
         <a class="link-target" id="pcbi-0040027-g001" name="pcbi-0040027-g001"></a>
         <div class="figure" data-doi="10.1371/journal.pcbi.0040027.g001">
          <div class="img-box">
           <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pcbi.0040027.g001" data-doi="info:doi/10.1371/journal.pcbi.0040027" data-uri="info:doi/10.1371/journal.pcbi.0040027.g001"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pcbi.0040027.g001" alt="thumbnail" class="thumbnail"></a>
           <div class="expand"></div>
          </div>
          <div class="figure-inline-download">
            Download: 
           <ul>
            <li>
             <div class="definition-label">
              <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pcbi.0040027.g001"> PPT </a>
             </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pcbi.0040027.g001"> PowerPoint slide </a></li>
            <li>
             <div class="definition-label">
              <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pcbi.0040027.g001"> PNG </a>
             </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pcbi.0040027.g001"> larger image </a></li>
            <li>
             <div class="definition-label">
              <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pcbi.0040027.g001"> TIFF </a>
             </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pcbi.0040027.g001"> original image </a></li>
           </ul>
          </div>
          <div class="figcaption">
           <span>Figure 1. </span> Performance of a Simple V1-Like Model Relative to Current Performance of State-of-the-Art Artificial Object Recognition Systems (Some Biologically Inspired) on an Ostensibly “Natural” Standard Image Database (Caltech101)
          </div>
          <p class="caption_target"><a id="article1.body1.sec1.fig1.caption1.p1" name="article1.body1.sec1.fig1.caption1.p1" class="link-target"></a></p>
          <p>(A) Example images from the database and their category labels.</p> 
          <a id="article1.body1.sec1.fig1.caption1.p2" name="article1.body1.sec1.fig1.caption1.p2" class="link-target"></a>
          <p>(B) Two example images from the “car” category.</p> 
          <a id="article1.body1.sec1.fig1.caption1.p3" name="article1.body1.sec1.fig1.caption1.p3" class="link-target"></a>
          <p>(C) Reported performance of five state-of-the-art computational object recognition systems on this “natural” database are shown in gray (1 = Wang et al. 2006; 2 = Grauman and Darrell 2006; 3 = Mutch and Lowe 2006; 4 = Lazebnik et al. 2006; 5 = Zhang et al. 2006). In this panel, 15 training examples were used to train each system. Since chance performance on this 102-category task is less than 1%, performance values greater than ?40% have been taken as substantial progress. The performance of the simple V1-like model is shown in black (+ is with “ad hoc” features; see <a href="#s4">Methods</a>). Although the V1-like model is extremely simple and lacks any explicit invariance-building mechanisms, it performs as well as, or better than, state-of-the-art object recognition systems on the “natural” databases (but see Varma and Ray 2007 for a recent hybrid approach, that pools the above methods to achieve higher performance).</p> 
          <a id="article1.body1.sec1.fig1.caption1.p4" name="article1.body1.sec1.fig1.caption1.p4" class="link-target"></a>
          <p>(D) Same as (C) except that 30 training examples were used. The dashed lines indicates performance achieved using an untransformed grayscale pixel space representation and a linear SVM classifier (15 training examples: 16.1%, SD 0.4; 30 training examples: 17.3%, SD 0.8). Error bars (barely visible) represent the standard deviation of the mean performance of the V1-like model over ten random training and testing splits of the images. The authors of the state-of-the-art approaches do not consistently report this variation, but when they do they are in the same range (less than 1%). The V1-model also performed favorably with fewer training examples (see <a href="#pcbi-0040027-sg004">Figure S4</a>).</p> 
          <p></p>
          <p class="caption_object"><a href="https://doi.org/10.1371/journal.pcbi.0040027.g001"> https://doi.org/10.1371/journal.pcbi.0040027.g001</a></p>
         </div>
         <a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3" class="link-target"></a>
         <p>However, we argue here for caution, as it is not clear to what extent such “natural” image tests actually engage the core problem of object recognition. Specifically, while the Caltech101 set certainly contains a large number of images (9,144 images), variations in object view, position, size, etc., between and within object category are poorly defined and are not varied systematically. Furthermore, image backgrounds strongly covary with object category (see <a href="#pcbi-0040027-g001">Figure 1</a>B). The majority of images are also “composed” photographs, in that a human decided how the shot should be framed, and thus the placement of objects within the image is not random and the set may not properly reflect the variation found in the real world. Furthermore, if the Caltech101 object recognition task is hard, it is not easy to know what makes it hard—different kinds of variation (view, lighting, exemplar, etc.) are all inextricably mixed together. Such problems are not unique to the Caltech101 set, but also apply to other uncontrolled “natural” image sets (e.g., Pascal VOC [<a href="#pcbi-0040027-b022" class="ref-tip">22</a>]).</p> 
        </div> 
        <div id="section2" class="section toc-section">
         <a id="s2" name="s2" data-toc="s2" class="link-target" title="Results"></a>
         <h2>Results</h2>
         <a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1" class="link-target"></a>
         <p>To explore this issue, we used the simplest, most obvious starting point for a biologically inspired object recognition system—a “V1-like” model based roughly on the known properties of simple cells of the first primate cortical visual processing stage (area V1). In particular, the model was a population of locally normalized, thresholded Gabor functions spanning a range of orientations and spatial frequencies (see <a href="#s4">Methods</a> for details). This is a neuroscience “null” model because it is only a first-order description of the early visual system, and one would not expect it to be good for real-world object recognition tasks. Specifically, it contains no explicit mechanisms to enable recognition to tolerate variation in object position, size, or pose, nor does it contain a particularly sophisticated representation of shape. Nevertheless, null models are useful for establishing baselines, and we proceeded to test this null model on a gold-standard “natural” object recognition task (i.e., Caltech101 [<a href="#pcbi-0040027-b013" class="ref-tip">13</a>]), using standard, published procedures [<a href="#pcbi-0040027-b021" class="ref-tip">21</a>].</p> 
         <a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2" class="link-target"></a>
         <p>We found that this simple V1-like model performed remarkably well on the Caltech101 object recognition task—indeed, it outperformed reported state-of-the-art computational efforts (biologically inspired or not; see <a href="#pcbi-0040027-g001">Figure 1</a>). <a href="#pcbi-0040027-g001">Figure 1</a> shows the cross-validated performance of two versions of this simple model: one where only the model's outputs are fed into a standard linear classifier, and one where some additional ad-hoc features are also used (e.g., local feature intensity histograms; see <a href="#s4">Methods</a> for details). In both cases, performance is surprisingly good (61% and 67% correct with 15 and 30 training examples), and comparable to, or better than, the current reported performance in the literature ([<a href="#pcbi-0040027-b004" class="ref-tip">4</a>,<a href="#pcbi-0040027-b018" class="ref-tip">18</a>–<a href="#pcbi-0040027-b021" class="ref-tip">21</a>]; but see [<a href="#pcbi-0040027-b023" class="ref-tip">23</a>]).</p> 
         <a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3" class="link-target"></a>
         <p>Given the V1-like model's surprisingly good performance on this “natural” image set (<a href="#pcbi-0040027-g001">Figure 1</a>), there are two possibilities. Either this model is a previously overlooked good model of visual object recognition, or current “natural” tests do not appropriately engage the object recognition problem. Given that our V1-like model contains no special machinery for tolerating image variation (and it would generally be considered a “straw man” model by neuroscientists), we were suspicious that this result had more to do with the test set, than the model itself. Nevertheless, to distinguish between these two possibilities, we designed a second more carefully controlled object recognition test that directly engages the core problem of object recognition.</p> 
         <a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4" class="link-target"></a>
         <p>Specifically, we constructed a series of two-category image sets, consisting of rendered images of plane and car objects. By the logic of the Caltech101 “natural” image test, this task should be substantially easier—there are only two object categories (rather than 102), and only a handful of specific objects per category (<a href="#pcbi-0040027-g002">Figure 2</a>A). In these sets, however, we explicitly and parametrically introduced real-world variation in the image that each object produced (see <a href="#s4">Methods</a>). In spite of the much smaller number of categories that the system was required to identify, the problem proved substantially harder for the V1-like model, exactly as one would expect for an incomplete model of object recognition. <a href="#pcbi-0040027-g002">Figure 2</a> shows how performance rapidly degrades toward chance-level as even modest amounts of real-world object image variation are systematically introduced in this simple two-category problem (see <a href="#pcbi-0040027-sg002">Figure S2</a> for a comparable demonstration with more than two object categories). Given this result, we conclude that the “V1-like” model performed well on the “natural” object recognition test (<a href="#pcbi-0040027-g001">Figure 1</a>), not because it is a good model of object recognition, but because the “natural” image test is inadequate.</p> 
         <a class="link-target" id="pcbi-0040027-g002" name="pcbi-0040027-g002"></a>
         <div class="figure" data-doi="10.1371/journal.pcbi.0040027.g002">
          <div class="img-box">
           <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pcbi.0040027.g002" data-doi="info:doi/10.1371/journal.pcbi.0040027" data-uri="info:doi/10.1371/journal.pcbi.0040027.g002"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pcbi.0040027.g002" alt="thumbnail" class="thumbnail"></a>
           <div class="expand"></div>
          </div>
          <div class="figure-inline-download">
            Download: 
           <ul>
            <li>
             <div class="definition-label">
              <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pcbi.0040027.g002"> PPT </a>
             </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pcbi.0040027.g002"> PowerPoint slide </a></li>
            <li>
             <div class="definition-label">
              <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pcbi.0040027.g002"> PNG </a>
             </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pcbi.0040027.g002"> larger image </a></li>
            <li>
             <div class="definition-label">
              <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pcbi.0040027.g002"> TIFF </a>
             </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pcbi.0040027.g002"> original image </a></li>
           </ul>
          </div>
          <div class="figcaption">
           <span>Figure 2. </span> The Same Simple V1-Like Model That Performed Well in 
           <a href="#pcbi-0040027-g001">Figure 1</a> Is Not a Good Model of Object Recognition—It Fails Badly on a “Simple” Problem That Explicitly Requires Tolerance to Image Variation
          </div>
          <p class="caption_target"><a id="article1.body1.sec2.fig1.caption1.p1" name="article1.body1.sec2.fig1.caption1.p1" class="link-target"></a></p>
          <p>(A) We used 3-D models of cars and planes to generate image sets for performing a cars-versus-planes two-category test. By using 3-D models, we were able to parametrically control the amount of identity-preserving variation that the system was required to tolerate to perform the task (i.e., variation in each object's position, scale, pose). The 3-D models were rendered using ray-tracing software (see <a href="#s4">Methods</a>), and were placed on either a white noise background (shown here), a scene background, or a phase scrambled background (these backgrounds are themselves another form of variation that a recognition system must tolerate; see <a href="#pcbi-0040027-sg001">Figure S1</a>).</p> 
          <a id="article1.body1.sec2.fig1.caption1.p2" name="article1.body1.sec2.fig1.caption1.p2" class="link-target"></a>
          <p>(B) As the amount of variation was increased (<em>x</em>-axis), performance drops off, eventually reaching chance level (50%). Here, we used 100 training and 30 testing images for each object category. However, using substantially more exemplar images (1,530 training, 1,530 testing) yielded only mild performance gains (e.g., 2.7% for the fourth variation level using white noise background), indicating that the failure of this model is not due to under-training. Error bars represent the standard error of the mean computed over ten random splits of training and testing images (see <a href="#s4">Methods</a>). This result highlights a fundamental problem in the current use of “natural” images to test object recognition models. By the logic of the “natural” Caltech101 test set, this task should be easy, because it has just two object categories, while the Caltech101 test should be hard (102 object categories). However, this V1-like model fails badly with this “easy” set, in spite of high performance on the Caltech101 test set (<a href="#pcbi-0040027-g001">Figure 1</a>).</p> 
          <p></p>
          <p class="caption_object"><a href="https://doi.org/10.1371/journal.pcbi.0040027.g002"> https://doi.org/10.1371/journal.pcbi.0040027.g002</a></p>
         </div>
         <a id="article1.body1.sec2.p5" name="article1.body1.sec2.p5" class="link-target"></a>
         <p>These results (re-)emphasize that object recognition is hard, not because images are natural or “complex,” but because each object can produce a very wide range of retinal images. Although the Caltech101 and other such “natural” sets were useful in that they encouraged the use of common performance tests on which all recognition models should compete, the results presented here show that a different direction is needed to create the content of those tests. This question is not simply an academic concern—great effort is now being expended to test object recognition models against a new, larger image set: the “Caltech256.” However, as with its predecessor, it fails to reflect real-world variation, and our “null” V1 model also performs well above chance (24% accuracy with 15 training examples to discriminate 257 categories), and competitively with early published performance estimates on this new set (see <a href="#pcbi-0040027-sg003">Figure S3</a>).</p> 
        </div> 
        <div id="section3" class="section toc-section">
         <a id="s3" name="s3" data-toc="s3" class="link-target" title="Discussion"></a>
         <h2>Discussion</h2>
         <a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1" class="link-target"></a>
         <p>How should we gauge progress in solving object recognition? First, the results presented here underscore that simple chance performance level is far from a good baseline and that our intuitions about “hard” and “easy” recognition problems are often far from correct. Indeed, it is disconcerting how little variation we needed to introduce to break a model that performs quite well according to current “natural' object recognition tests. Thus, simple “null” models (that are able to exploit regularities in the image database) are needed to objectively judge the difficulty of recognition tasks and to establish a baseline for each such task. The V1-like model presented here provides one possible “null” model, and portable code for building and evaluating it is freely available upon request.</p> 
         <a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2" class="link-target"></a>
         <p>Second, the development of appropriate recognition tests is critical to guiding the development of object recognition models and testing performance of neuronal populations that might support recognition [<a href="#pcbi-0040027-b024" class="ref-tip">24</a>]. The construction of such tests is not trivial because the issues cut deeper than simple performance evaluation—this is a question of how we think about the problem of object recognition and why it is hard [<a href="#pcbi-0040027-b001" class="ref-tip">1</a>]. Because the number of images in any practical recognition database will be small relative to the dimensionality of the problem domain, test images must be chosen in a manner that properly samples this domain so as to capture the essence of the recognition problem and thus avoid “solutions” that rely on trivial regularities or heuristics.</p> 
         <a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3" class="link-target"></a>
         <p>One approach would be to generate a very large database of “natural” images, like the Caltech sets, but captured in an unbiased way (i.e., with great care taken to avoid the implicit biases that occur in framing a snapshot). Done correctly, this approach has the advantage of directly sampling the true problem domain. However, annotating such an image set is extremely labor-intensive (but see the LabelMe project [<a href="#pcbi-0040027-b025" class="ref-tip">25</a>], Peekaboom [<a href="#pcbi-0040027-b026" class="ref-tip">26</a>], and the StreetScenes dataset [<a href="#pcbi-0040027-b002" class="ref-tip">2</a>,<a href="#pcbi-0040027-b027" class="ref-tip">27</a>]). More importantly, a set that truly reflects all real-world variation may be too stringent of an assay to guide improvement in recognition models. That is, if the problem is too hard, it is not easy to construct a reduced version that still engages the core problem of object recognition.</p> 
         <a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4" class="link-target"></a>
         <p>Another approach, an extension of the one taken here, would be to use synthetic images, where ground truth is known by design. Paradoxically, such synthetic image sets may in many ways be more natural than an arbitrary collection of ostensibly “natural” photographs, because, for a fixed number of images, they better span the range of possible image transformations observed in the real world (see also the NORB dataset [<a href="#pcbi-0040027-b028" class="ref-tip">28</a>]). The synthetic image approach obviates labor-intensive and error-prone labeling procedures, and can be easily used to isolate performance on different components of the task. Such an approach also has the advantage that it can be parametrically made more difficult as needed (e.g., when a given model has achieved the ability to tolerate a certain amount of variation, a new instantiation of the test set with greater variation can be generated). Given the difficulty of real-world object recognition, this ability to gradually “ratchet” task difficulty, while still engaging the core computational problem, may provide invaluable guidance of computational efforts.</p> 
         <a id="article1.body1.sec3.p5" name="article1.body1.sec3.p5" class="link-target"></a>
         <p>While standardized benchmarks are important for assessing progress, designing benchmarks that properly define what constitutes “progress” is extremely difficult. On one hand, a benchmark that captures too little of the complexity of the real world (no matter how complex it may seem at first glance) invites over-optimization to trivial regularities in the test set (e.g., Caltech101). On the other hand, a benchmark that embraces too much of the “real” problem can be too difficult for any model to gain traction (e.g., the detection challenge in Pascal VOC [<a href="#pcbi-0040027-b022" class="ref-tip">22</a>]), giving little insight on which approaches are most promising. This problem is compounded by the fact that there are many more <em>kinds</em> of image variation in the real world beyond those used in our simple synthetic test set (e.g., lighting, occlusion, deformation, etc.). At the center of this challenge is the need to clearly define what the problem is, why it is difficult, and what results would constitute success. The path forward will not be easy, but it is time for the field to give this problem much more central attention.</p> 
        </div> 
        <div id="section4" class="section toc-section">
         <a id="s4" name="s4" data-toc="s4" class="link-target" title="Methods"></a>
         <h2>Methods</h2> 
         <div id="section1" class="section toc-section">
          <a id="s4a" name="s4a" class="link-target" title="A V1-like recognition system."></a> 
          <h3>A V1-like recognition system.</h3> 
          <a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1" class="link-target"></a>
          <p>Area V1 is the first stage of cortical processing of visual information in the primate and is the gateway of subsequent processing stages. We built a very basic representation inspired by known properties of V1 “simple” cells (a subpopulation of V1 cells). The responses of these cells to visual stimuli are well-described by a spatial linear filter, resembling a Gabor wavelet [<a href="#pcbi-0040027-b029" class="ref-tip">29</a>–<a href="#pcbi-0040027-b031" class="ref-tip">31</a>], with a nonlinear output function (threshold and saturation) and some local normalization (roughly analogous to “contrast gain control”). Operationally, our V1-like model consisted of the following processing steps.</p> 
          <a id="article1.body1.sec4.sec1.p2" name="article1.body1.sec4.sec1.p2" class="link-target"></a>
          <p><em>Image preparation.</em> First we converted the input image to grayscale and resized by bicubic interpolation the largest edge to a fixed size (150 pixels for Caltech datasets) while preserving its aspect ratio. The mean was subtracted from the resulting two-dimensional image and we divided it by its standard deviation. The resulting image had zero mean, unit variance, and a size of H × W. Because images have different aspect ratios, H and W vary from image to image.</p> 
          <a id="article1.body1.sec4.sec1.p3" name="article1.body1.sec4.sec1.p3" class="link-target"></a>
          <p><em>Local input divisive normalization</em>. For each pixel in the input image, we subtracted the mean of the pixel values in a fixed window (3 × 3 pixels, centered on the pixel), and we divided this value by the euclidean norm of the resulting 9-dimensional vector (3 × 3 window) if the norm was greater than 1 (i.e., roughly speaking, the normalization was constrained such that it could reduce responses, but not enhance them).</p> 
          <a id="article1.body1.sec4.sec1.p4" name="article1.body1.sec4.sec1.p4" class="link-target"></a>
          <p><em>Linear filtering with a set of Gabor filters.</em> We convolved the normalized images with a set of two-dimensional Gabor filters of fixed size (43 × 43 pixels), spanning 16 orientations (equally spaced around the clock) and six spatial frequencies (1/2, 1/3, 1/4, 1/6, 1/11, 1/18 cycles/pixel) with a fixed Gaussian envelope (standard deviation of 9 cycles/pixel in both directions) and fixed phase (0) for a total of N = 96 filters. Each filter had zero-mean and euclidean norm of one. This dimensionality expansion approximates the roughly 100-fold increase in the number of primate V1 neurons relative to the number of retinal ganglion cell axons. To speed this step, the Gabor filters were decomposed via singular value decomposition into a form suitable for use in a separable convolution (this is possible because the Gabor filters are of low rank), and the decomposed filters retained at least 90% of their original variation.</p> 
          <a id="article1.body1.sec4.sec1.p5" name="article1.body1.sec4.sec1.p5" class="link-target"></a>
          <p><em>Thresholding and saturation</em>. The output of each Gabor filter was passed through a standard output non-linearity—a threshold and response saturation. Specifically, all negative output values were set to 0 and all values greater than 1 were set to 1.</p> 
          <a id="article1.body1.sec4.sec1.p6" name="article1.body1.sec4.sec1.p6" class="link-target"></a>
          <p><em>Local output divisive normalization.</em> The result of the Gabor filtering was a three-dimensional matrix of size H × W × N where each two-dimensional slice (H × W) is the output of each Gabor filter type. For each filter output, we subtracted the mean of filter outputs in a fixed spatial window (3 × 3 pixels, centered) across all orientations and spatial scales (total of 864 elements). We then divided by the euclidean norm of the values in this window (864 elements), except when the norm was less than 1.</p> 
         </div> 
         <div id="section2" class="section toc-section">
          <a id="s4b" name="s4b" class="link-target" title="Comparison to other biologically inspired recognition models."></a> 
          <h3>Comparison to other biologically inspired recognition models.</h3> 
          <a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1" class="link-target"></a>
          <p>Some of the other models whose performance is shown in <a href="#pcbi-0040027-g001">Figure 1</a> were biologically inspired, and thus also have V1-like stages contained within them, as well as additional machinery intended to allow invariant object recognition (e.g., [<a href="#pcbi-0040027-b002" class="ref-tip">2</a>,<a href="#pcbi-0040027-b019" class="ref-tip">19</a>]). Thus, it might be surprising that the simple V1-like model presented here outperforms those models. Although detailed comparisons are beyond the scope of this study and tangential to our main point, we note that the V1-like model presented here contains a number of differences from the V1-like portions of these other models (higher dimensionality, larger receptive fields, inclusion of threshold nonlinearities, local normalization, etc.) that probably produce better performance than these models.</p> 
         </div> 
         <div id="section3" class="section toc-section">
          <a id="s4c" name="s4c" class="link-target" title="Classification."></a> 
          <h3>Classification.</h3> 
          <a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1" class="link-target"></a>
          <p>To test the utility of our V1-like representation for performing object recognition tasks, we performed a standard cross-validated classification procedure on the high-dimensional output of the model.</p> 
          <a id="article1.body1.sec4.sec3.p2" name="article1.body1.sec4.sec3.p2" class="link-target"></a>
          <p><em>Dimensionality reduction.</em> To speed computation and improve classification performance, we reduced the dimensionality of the model output prior to classification. The output of V1-like model (above) was a stack of 96 output images, one per Gabor filter type. Because the dimensionality of this stack can be very high (up to 2,160,000 output values per input image depending on its size), standard dimensionality reduction techniques were used to prepare the data for classification. Specifically, each of the 96 output images was low-pass filtered (17 × 17 boxcar) and down-sampled to a smaller size (30 × 30). Thus, regardless of the original input image size, the total dimensionality for classification was always 86,400 (30 × 30 × 96). The data were then sphered (i.e., each filter output was standardized by subtraction of its mean and division by its standard deviation across the training image set; see below), and the dimensionality of the representation was further reduced by principal components analysis (PCA), keeping as many dimensions as there were data points in the training set. For the Caltech101 experiments (e.g., <a href="#pcbi-0040027-g001">Figure 1</a>), the dimensionality of the final feature vector was 1530 or 3060 (depending on the number of training examples: 15 or 30, respectively).</p> 
          <a id="article1.body1.sec4.sec3.p3" name="article1.body1.sec4.sec3.p3" class="link-target"></a>
          <p><em>Additional “ad hoc” features.</em> To further explore the utility of this V1-like model, we generated some additional easy-to-obtain features and concatenated these to the final feature vector, prior to PCA dimensionality reduction. These features included: raw grayscale input images (downsampled to 100 × 100 by bicubic interpolation; 10,000 features), and model output histograms for some intermediate stages of the model: pre-normalization (one local histogram per quadrant of the image), post-normalization (full image), and post downsampling (full image)—roughly 30,000 features total. No color information was used in these additional features. Throughout the text, results from the system containing these extra “ad hoc” features are reported separately from those obtained with the system that did not have these extra features. These extra features were added to demonstrate what was possible using additional obvious, “cheap” (but still fair) tricks that improve performance without incurring additional conceptual complexity.</p> 
          <a id="article1.body1.sec4.sec3.p4" name="article1.body1.sec4.sec3.p4" class="link-target"></a>
          <p><em>Training.</em> Training and test images were carefully separated to ensure proper cross-validation. 15 training example images, and 30 testing example images were drawn from the full image set. Sphering parameters and PCA eigenvectors were computed from the training images (see Dimensionality Reduction, above), and the dimensionality-reduced training data were used to train a linear support vector machine (SVM) using libsvm-2.82 [<a href="#pcbi-0040027-b032" class="ref-tip">32</a>]. A standard one-versus-all approach was used to generate the multi-class SVM classifier from the training images.</p> 
          <a id="article1.body1.sec4.sec3.p5" name="article1.body1.sec4.sec3.p5" class="link-target"></a>
          <p><em>Testing protocol.</em> Following training, absolutely no changes to the representation or classifier were made. Each test image was sphered using parameters determined from the training images, projected through the V1-like model onto the eigenvectors computed from the training images, and the trained SVM was used to report the predicted category of the test image</p> 
          <a id="article1.body1.sec4.sec3.p6" name="article1.body1.sec4.sec3.p6" class="link-target"></a>
          <p><em>Number of training and testing images.</em> Classifiers were trained using a fixed number of examples (15 and 30 example images; see <a href="#pcbi-0040027-g001">Figure 1</a>C and <a href="#pcbi-0040027-g001">1</a>D). The performance scores reported here are the average of performances obtained from ten random splits of training and testing sets. For testing, 30 images were classified per category, except in categories where there were not enough images available, in which case the maximum number of available images was used (e.g., “inline_skate”, the smallest category, has only 31 examples; when 30 examples were used for training, then only one example was available for testing). Since the Caltech101 sets contains a different number of images for each category, care must be taken to ensure that per-category performance is normalized by the number of test examples considered in each category—otherwise, average performance can be biased toward the performance obtained from categories with larger numbers of images available. This is a particular problem for the Caltech101 set, because some of the largest categories are also empirically the easiest (e.g., cars, airplanes, faces, motorbikes). For the performance values reported in this paper, average performance was computed per category, and then these performances were averaged together to obtain an overall performance value (reported in the text and figures).</p> 
          <a id="article1.body1.sec4.sec3.p7" name="article1.body1.sec4.sec3.p7" class="link-target"></a>
          <p><em>Further controls.</em> To ensure the validity of our results, we undertook a number of checks to verify that the classification procedure used here was correct. Two different SVM front-ends were used (PyML and libsvm command line tools) and produced identical results. To confirm proper cross-validation, we manually inspected training and test set splits to certify that there were no images in common between the two sets (this control was partially motivated by the fact that an earlier version of the Caltech101 dataset contained duplicates). The classification procedure was also repeated with noise images, and for image sets with category labels scrambled; both tests yielded chance performance, as expected.</p> 
         </div> 
         <div id="section4" class="section toc-section">
          <a id="s4d" name="s4d" class="link-target" title="Synthetic dataset generation."></a> 
          <h3>Synthetic dataset generation.</h3> 
          <a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1" class="link-target"></a>
          <p>Synthetic images of cars and planes were generated using POV-Ray, a free, high-quality ray tracing software package (<a href="http://www.povray.org">http://www.povray.org</a>). 3-D models of cars and planes (purchased from Dosch Design and TurboSquid) were converted to the POV-Ray format. This general approach could be used to generate image sets with arbitrary numbers of different objects, undergoing controlled ranges of variation. For example, in <a href="#pcbi-0040027-g002">Figure 2</a> each “pooled variation” level on the <em>x</em>-axis shows the maximum deviation of each of five object viewing parameters (zero variation is shown in <a href="#pcbi-0040027-g002">Figure 2</a>A assuming centering in the image). Given a “pooled variation” level, a set of images was generated by randomly sampling each viewing parameter uniformly within its specified maximum deviation (e.g., +/?30° in plane rotation). Each image in the set was the result of using one such parameter sample to render the view of the object on a given background (see <a href="#pcbi-0040027-sg001">Figure S1</a>). 100% position variation is a full non-overlapping shift of the object's bounding box; 100% scale variation is one octave of change.</p> 
          <a id="article1.body1.sec4.sec4.p2" name="article1.body1.sec4.sec4.p2" class="link-target"></a>
          <p>While this image set is useful for demonstrating the inadequacy of our V1-like model (in spite of its apparent success at the Caltech101 test), we do not believe it represents any sort of new “standard” against which models of object recognition should be tested. Instead, we believe that the <em>approach</em> is more important—identifying the problem, generating sets that span limited regions of the problem space, building models, and then “ratcheting” the problem to a higher difficulty level once the limited version of the problem has been solved.</p> 
         </div> 
        </div> 
        <div id="section5" class="section toc-section">
         <a id="s5" name="s5" data-toc="s5" class="link-target" title="Supporting Information"></a>
         <h2>Supporting Information</h2>
         <div class="figshare_widget" doi="10.1371/journal.pcbi.0040027"></div>
         <div class="supplementary-material">
          <a name="pcbi-0040027-sg001" id="pcbi-0040027-sg001" class="link-target"></a>
          <h3 class="siTitle title-small"><a href="article/file?type=supplementary&amp;id=info:doi/10.1371/journal.pcbi.0040027.sg001">Figure S1. </a>Backgrounds Used</h3>
          <a id="article1.body1.sec5.supplementary-material1.caption1.p1" name="article1.body1.sec5.supplementary-material1.caption1.p1" class="link-target"></a>
          <p class="preSiDOI">Model performance for our “simple” two class image set was assessed with the 3-D models rendered onto three types of backgrounds—white noise, phase-scrambled scene images (?1/f noise), and intact scene images. Performance with each of these types of background is shown in <a href="#pcbi-0040027-g002">Figure 2</a>.</p> 
          <p class="siDoi"><a href="https://doi.org/10.1371/journal.pcbi.0040027.sg001">https://doi.org/10.1371/journal.pcbi.0040027.sg001</a></p>
          <a id="article1.body1.sec5.supplementary-material1.caption1.p2" name="article1.body1.sec5.supplementary-material1.caption1.p2" class="link-target"></a>
          <p class="postSiDOI">(3.8 MB TIF)</p> 
         </div>
         <div class="supplementary-material">
          <a name="pcbi-0040027-sg002" id="pcbi-0040027-sg002" class="link-target"></a>
          <h3 class="siTitle title-small"><a href="article/file?type=supplementary&amp;id=info:doi/10.1371/journal.pcbi.0040027.sg002">Figure S2. </a>Performance Fall-Off for Increasing Numbers of Object Categories</h3>
          <a id="article1.body1.sec5.supplementary-material2.caption1.p1" name="article1.body1.sec5.supplementary-material2.caption1.p1" class="link-target"></a>
          <p><a href="#pcbi-0040027-g002">Figure 2</a> shows that relatively modest amounts of image transformation push the performance of our simple V1-like model down to chance. Here we show that this fall-off becomes slightly steeper as more categories-to-be-discriminated are added.</p> 
          <a id="article1.body1.sec5.supplementary-material2.caption1.p2" name="article1.body1.sec5.supplementary-material2.caption1.p2" class="link-target"></a>
          <p>(A) Four categories of objects (cars, planes, boats, and animals) were used to measure performance when 2, 3, or 4 categories are considered.</p> 
          <a id="article1.body1.sec5.supplementary-material2.caption1.p3" name="article1.body1.sec5.supplementary-material2.caption1.p3" class="link-target"></a>
          <p class="preSiDOI">(B) Average identification performance (“is object category X present or not”) is plotted as a function of view variation and number of object categories to be discriminated. Chance performance is 50% for all three lines, because average one-versus-all performance is shown here, not n-way recognition performance (i.e., “which object is present”).</p> 
          <p class="siDoi"><a href="https://doi.org/10.1371/journal.pcbi.0040027.sg002">https://doi.org/10.1371/journal.pcbi.0040027.sg002</a></p>
          <a id="article1.body1.sec5.supplementary-material2.caption1.p4" name="article1.body1.sec5.supplementary-material2.caption1.p4" class="link-target"></a>
          <p class="postSiDOI">(1.1 MB TIF)</p> 
         </div>
         <div class="supplementary-material">
          <a name="pcbi-0040027-sg003" id="pcbi-0040027-sg003" class="link-target"></a>
          <h3 class="siTitle title-small"><a href="article/file?type=supplementary&amp;id=info:doi/10.1371/journal.pcbi.0040027.sg003">Figure S3. </a>Performance on the Caltech256</h3>
          <a id="article1.body1.sec5.supplementary-material3.caption1.p1" name="article1.body1.sec5.supplementary-material3.caption1.p1" class="link-target"></a>
          <p class="preSiDOI">1 = Griffin et al. (2007).</p> 
          <p class="siDoi"><a href="https://doi.org/10.1371/journal.pcbi.0040027.sg003">https://doi.org/10.1371/journal.pcbi.0040027.sg003</a></p>
          <a id="article1.body1.sec5.supplementary-material3.caption1.p2" name="article1.body1.sec5.supplementary-material3.caption1.p2" class="link-target"></a>
          <p class="postSiDOI">(366 KB TIF)</p> 
         </div>
         <div class="supplementary-material">
          <a name="pcbi-0040027-sg004" id="pcbi-0040027-sg004" class="link-target"></a>
          <h3 class="siTitle title-small"><a href="article/file?type=supplementary&amp;id=info:doi/10.1371/journal.pcbi.0040027.sg004">Figure S4. </a>Performance on the Caltech101 as a Function of the Number of Training Examples, Including Small Numbers of Training Examples</h3>
          <a id="article1.body1.sec5.supplementary-material4.caption1.p1" name="article1.body1.sec5.supplementary-material4.caption1.p1" class="link-target"></a>
          <p class="preSiDOI">Points marked with asterisks are not exact, but were estimated from published plots.</p> 
          <p class="siDoi"><a href="https://doi.org/10.1371/journal.pcbi.0040027.sg004">https://doi.org/10.1371/journal.pcbi.0040027.sg004</a></p>
          <a id="article1.body1.sec5.supplementary-material4.caption1.p2" name="article1.body1.sec5.supplementary-material4.caption1.p2" class="link-target"></a>
          <p class="postSiDOI">(967 KB TIF)</p> 
         </div>
        </div> 
        <div class="section toc-section">
         <a id="ack" name="ack" data-toc="ack" title="Acknowledgments" class="link-target"></a>
         <h2>Acknowledgments</h2> 
         <a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1" class="link-target"></a>
         <p>We would like to thank J. Maunsell, J. Mutch, T. Poggio, E. Simoncelli, and A. Torralba for helpful comments and discussion.</p> 
        </div>
        <div class="contributions toc-section">
         <a id="authcontrib" name="authcontrib" data-toc="authcontrib" title="Author Contributions"></a>
         <h2>Author Contributions</h2>
         <p> NP, DDC, and JJD conceived and designed the experiments and wrote the paper. NP performed the experiments, and analyzed the data. NP and DDC contributed analysis tools.</p>
        </div>
        <div class="toc-section">
         <a id="references" name="references" class="link-target" data-toc="references" title="References"></a>
         <h2>References</h2>
         <ol class="references">
          <li id="ref1"><span class="order">1. </span><a name="pcbi-0040027-b001" id="pcbi-0040027-b001" class="link-target"></a>DiCarlo JJ, Cox DD (2007) Untangling invariant object recognition. Trends Cogn Sci 11: 333–341.JJ DiCarloDD Cox2007Untangling invariant object recognition.Trends Cogn Sci11333341 
           <ul class="reflinks">
            <li><a href="#" data-author="DiCarlo" data-cit="DiCarloJJCoxDD2007Untangling%20invariant%20object%20recognition.Trends%20Cogn%20Sci11333341" data-title="Untangling%20invariant%20object%20recognition." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=DiCarlo%5Bauthor%5D+AND+Untangling+invariant+object+recognition." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Untangling+invariant+object+recognition.&amp;author=DiCarlo&amp;publication_year=2007" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref2"><span class="order">2. </span><a name="pcbi-0040027-b002" id="pcbi-0040027-b002" class="link-target"></a>Serre T, Wolf L, Bileschi S, Riesenhuber M, Poggio T (2007) Object recognition with cortex-like mechanisms. IEEE PAMI 9: 411–426.T. SerreL. WolfS. BileschiM. RiesenhuberT. Poggio2007Object recognition with cortex-like mechanisms.IEEE PAMI9411426 
           <ul class="reflinks">
            <li><a href="#" data-author="Serre" data-cit="SerreTWolfLBileschiSRiesenhuberMPoggioT2007Object%20recognition%20with%20cortex-like%20mechanisms.IEEE%20PAMI9411426" data-title="Object%20recognition%20with%20cortex-like%20mechanisms." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Serre%5Bauthor%5D+AND+Object+recognition+with+cortex-like+mechanisms." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Object+recognition+with+cortex-like+mechanisms.&amp;author=Serre&amp;publication_year=2007" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref3"><span class="order">3. </span><a name="pcbi-0040027-b003" id="pcbi-0040027-b003" class="link-target"></a>Lowe DG (2004) Distinctive image features from scale-invariant keypoints. IEEE IJCV 60: 91–110.DG Lowe2004Distinctive image features from scale-invariant keypoints.IEEE IJCV6091110 
           <ul class="reflinks">
            <li><a href="#" data-author="Lowe" data-cit="LoweDG2004Distinctive%20image%20features%20from%20scale-invariant%20keypoints.IEEE%20IJCV6091110" data-title="Distinctive%20image%20features%20from%20scale-invariant%20keypoints." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Lowe%5Bauthor%5D+AND+Distinctive+image+features+from+scale-invariant+keypoints." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Distinctive+image+features+from+scale-invariant+keypoints.&amp;author=Lowe&amp;publication_year=2004" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref4"><span class="order">4. </span><a name="pcbi-0040027-b004" id="pcbi-0040027-b004" class="link-target"></a>Zhang H, Berg AC, Maire M, Malik J (2006) SVM-KNN: Discriminative nearest neighbor classification for visual category recognition. IEEE CVPR 2006 2126–2136.H. ZhangAC BergM. MaireJ. Malik2006SVM-KNN: Discriminative nearest neighbor classification for visual category recognition.IEEE CVPR 200621262136 
           <ul class="reflinks">
            <li><a href="#" data-author="Zhang" data-cit="ZhangHBergACMaireMMalikJ2006SVM-KNN%3A%20Discriminative%20nearest%20neighbor%20classification%20for%20visual%20category%20recognition.IEEE%20CVPR%20200621262136" data-title="SVM-KNN%3A%20Discriminative%20nearest%20neighbor%20classification%20for%20visual%20category%20recognition." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Zhang%5Bauthor%5D+AND+SVM-KNN%3A+Discriminative+nearest+neighbor+classification+for+visual+category+recognition." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=SVM-KNN%3A+Discriminative+nearest+neighbor+classification+for+visual+category+recognition.&amp;author=Zhang&amp;publication_year=2006" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref5"><span class="order">5. </span><a name="pcbi-0040027-b005" id="pcbi-0040027-b005" class="link-target"></a>Weber M, Welling M, Perona P (2000) Unsupervised learning of models for recognition. IEEE ECCV 2000 18–32.M. WeberM. WellingP. Perona2000Unsupervised learning of models for recognition.IEEE ECCV 20001832 
           <ul class="reflinks">
            <li><a href="#" data-author="Weber" data-cit="WeberMWellingMPeronaP2000Unsupervised%20learning%20of%20models%20for%20recognition.IEEE%20ECCV%2020001832" data-title="Unsupervised%20learning%20of%20models%20for%20recognition." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Weber%5Bauthor%5D+AND+Unsupervised+learning+of+models+for+recognition." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Unsupervised+learning+of+models+for+recognition.&amp;author=Weber&amp;publication_year=2000" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref6"><span class="order">6. </span><a name="pcbi-0040027-b006" id="pcbi-0040027-b006" class="link-target"></a>Arathorn D (2002) Map-seeking circuits in visual cognition: A computational mechanism for biological and machine vision. Stanford (California): Stanford University Press. 240 p.D. Arathorn2002Map-seeking circuits in visual cognition: A computational mechanism for biological and machine visionStanford (California)Stanford University Press240 
           <ul class="find-nolinks"></ul></li>
          <li id="ref7"><span class="order">7. </span><a name="pcbi-0040027-b007" id="pcbi-0040027-b007" class="link-target"></a>Felsen G, Dan Y (2005) A natural approach to studying vision. Nat Neurosci 8: 1643–1646.G. FelsenY. Dan2005A natural approach to studying vision.Nat Neurosci816431646 
           <ul class="reflinks">
            <li><a href="#" data-author="Felsen" data-cit="FelsenGDanY2005A%20natural%20approach%20to%20studying%20vision.Nat%20Neurosci816431646" data-title="A%20natural%20approach%20to%20studying%20vision." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Felsen%5Bauthor%5D+AND+A+natural+approach+to+studying+vision." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=A+natural+approach+to+studying+vision.&amp;author=Felsen&amp;publication_year=2005" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref8"><span class="order">8. </span><a name="pcbi-0040027-b008" id="pcbi-0040027-b008" class="link-target"></a>Rust NC, Movshon JA (2005) In praise of artifice. Nat Neurosci 8: 1647–1649.NC RustJA Movshon2005In praise of artifice.Nat Neurosci816471649 
           <ul class="reflinks">
            <li><a href="#" data-author="Rust" data-cit="RustNCMovshonJA2005In%20praise%20of%20artifice.Nat%20Neurosci816471649" data-title="In%20praise%20of%20artifice." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Rust%5Bauthor%5D+AND+In+praise+of+artifice." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=In+praise+of+artifice.&amp;author=Rust&amp;publication_year=2005" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref9"><span class="order">9. </span><a name="pcbi-0040027-b009" id="pcbi-0040027-b009" class="link-target"></a>Gallant JL, Connor CE, Van Essen DC (1998) Neural activity in areas V1, V2, and V4 during free viewing of natural scenes compared to control images. Neuroreport 9: 85–89.JL GallantCE ConnorDC Van Essen1998Neural activity in areas V1, V2, and V4 during free viewing of natural scenes compared to control images.Neuroreport98589 
           <ul class="reflinks">
            <li><a href="#" data-author="Gallant" data-cit="GallantJLConnorCEVan%20EssenDC1998Neural%20activity%20in%20areas%20V1%2C%20V2%2C%20and%20V4%20during%20free%20viewing%20of%20natural%20scenes%20compared%20to%20control%20images.Neuroreport98589" data-title="Neural%20activity%20in%20areas%20V1%2C%20V2%2C%20and%20V4%20during%20free%20viewing%20of%20natural%20scenes%20compared%20to%20control%20images." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Gallant%5Bauthor%5D+AND+Neural+activity+in+areas+V1%2C+V2%2C+and+V4+during+free+viewing+of+natural+scenes+compared+to+control+images." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Neural+activity+in+areas+V1%2C+V2%2C+and+V4+during+free+viewing+of+natural+scenes+compared+to+control+images.&amp;author=Gallant&amp;publication_year=1998" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref10"><span class="order">10. </span><a name="pcbi-0040027-b010" id="pcbi-0040027-b010" class="link-target"></a>Reinagel P (2001) How do visual neurons respond in the real world? Curr Opin Neurobiol 11: 437–442.P. Reinagel2001How do visual neurons respond in the real world?Curr Opin Neurobiol11437442 
           <ul class="reflinks">
            <li><a href="#" data-author="Reinagel" data-cit="ReinagelP2001How%20do%20visual%20neurons%20respond%20in%20the%20real%20world%3FCurr%20Opin%20Neurobiol11437442" data-title="How%20do%20visual%20neurons%20respond%20in%20the%20real%20world%3F" target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Reinagel%5Bauthor%5D+AND+How+do+visual+neurons+respond+in+the+real+world%3F" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=How+do+visual+neurons+respond+in+the+real+world%3F&amp;author=Reinagel&amp;publication_year=2001" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref11"><span class="order">11. </span><a name="pcbi-0040027-b011" id="pcbi-0040027-b011" class="link-target"></a>Bell AJ, Sejnowski TJ (1997) The “independent components” of natural scenes are edge filters. Vision Res 37: 3327–3338.AJ BellTJ Sejnowski1997The “independent components” of natural scenes are edge filters.Vision Res3733273338 
           <ul class="reflinks">
            <li><a href="#" data-author="Bell" data-cit="BellAJSejnowskiTJ1997The%20%E2%80%9Cindependent%20components%E2%80%9D%20of%20natural%20scenes%20are%20edge%20filters.Vision%20Res3733273338" data-title="The%20%E2%80%9Cindependent%20components%E2%80%9D%20of%20natural%20scenes%20are%20edge%20filters." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Bell%5Bauthor%5D+AND+The+%E2%80%9Cindependent+components%E2%80%9D+of+natural+scenes+are+edge+filters." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=The+%E2%80%9Cindependent+components%E2%80%9D+of+natural+scenes+are+edge+filters.&amp;author=Bell&amp;publication_year=1997" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref12"><span class="order">12. </span><a name="pcbi-0040027-b012" id="pcbi-0040027-b012" class="link-target"></a>Simoncelli EP, Olshausen BA (2001) Natural image statistics and neural representation. Annu Rev Neurosci 24: 1193–1216.EP SimoncelliBA Olshausen2001Natural image statistics and neural representation.Annu Rev Neurosci2411931216 
           <ul class="reflinks">
            <li><a href="#" data-author="Simoncelli" data-cit="SimoncelliEPOlshausenBA2001Natural%20image%20statistics%20and%20neural%20representation.Annu%20Rev%20Neurosci2411931216" data-title="Natural%20image%20statistics%20and%20neural%20representation." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Simoncelli%5Bauthor%5D+AND+Natural+image+statistics+and+neural+representation." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Natural+image+statistics+and+neural+representation.&amp;author=Simoncelli&amp;publication_year=2001" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref13"><span class="order">13. </span><a name="pcbi-0040027-b013" id="pcbi-0040027-b013" class="link-target"></a>Fei-Fei L, Fergus R, Perona P (2004) Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories. IEEE CVPR 2004: 178.L. Fei-FeiR. FergusP. Perona2004Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories.IEEE CVPR2004178 
           <ul class="reflinks">
            <li><a href="#" data-author="Fei-Fei" data-cit="Fei-FeiLFergusRPeronaP2004Learning%20generative%20visual%20models%20from%20few%20training%20examples%3A%20an%20incremental%20Bayesian%20approach%20tested%20on%20101%20object%20categories.IEEE%20CVPR2004178" data-title="Learning%20generative%20visual%20models%20from%20few%20training%20examples%3A%20an%20incremental%20Bayesian%20approach%20tested%20on%20101%20object%20categories." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Fei-Fei%5Bauthor%5D+AND+Learning+generative+visual+models+from+few+training+examples%3A+an+incremental+Bayesian+approach+tested+on+101+object+categories." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Learning+generative+visual+models+from+few+training+examples%3A+an+incremental+Bayesian+approach+tested+on+101+object+categories.&amp;author=Fei-Fei&amp;publication_year=2004" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref14"><span class="order">14. </span><a name="pcbi-0040027-b014" id="pcbi-0040027-b014" class="link-target"></a>Ponce J, Berg TL, Everingham M, Forsyth DA, Hebert M, et al. (2006) Dataset issues in object recognition. Toward category-level object recognition. Berlin: Springer-Verlag: Lect Notes Comput Sci. pp. 29–48.J. PonceTL BergM. EveringhamDA ForsythM. Hebert2006Dataset issues in object recognition.Toward category-level object recognitionBerlin: Springer-VerlagLect Notes Comput Sci2948 
           <ul class="find-nolinks"></ul></li>
          <li id="ref15"><span class="order">15. </span><a name="pcbi-0040027-b015" id="pcbi-0040027-b015" class="link-target"></a>Masquelier T, Thorpe SJ (2007) Unsupervised learning of visual features through Spike Timing Dependent Plasticity. PLoS Comp Bio 3: e31.. T. MasquelierSJ Thorpe2007Unsupervised learning of visual features through Spike Timing Dependent Plasticity.PLoS Comp Bio3e31. 
           <ul class="reflinks">
            <li><a href="#" data-author="Masquelier" data-cit="MasquelierTThorpeSJ2007Unsupervised%20learning%20of%20visual%20features%20through%20Spike%20Timing%20Dependent%20Plasticity.PLoS%20Comp%20Bio3e31.doi%3A10.1371%2Fjournal.pcbi.0030031." data-title="Unsupervised%20learning%20of%20visual%20features%20through%20Spike%20Timing%20Dependent%20Plasticity." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Masquelier%5Bauthor%5D+AND+Unsupervised+learning+of+visual+features+through+Spike+Timing+Dependent+Plasticity." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Unsupervised+learning+of+visual+features+through+Spike+Timing+Dependent+Plasticity.&amp;author=Masquelier&amp;publication_year=2007" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref16"><span class="order">16. </span><a name="pcbi-0040027-b016" id="pcbi-0040027-b016" class="link-target"></a>Einhäuser W, Koch C, Makeig S (2007) The duration of the attentional blink in natural scenes depends on stimulus category. Vision Res 47: 597–607.W. EinhäuserC. KochS. Makeig2007The duration of the attentional blink in natural scenes depends on stimulus category.Vision Res47597607 
           <ul class="reflinks">
            <li><a href="#" data-author="Einh%C3%A4user" data-cit="Einh%C3%A4userWKochCMakeigS2007The%20duration%20of%20the%20attentional%20blink%20in%20natural%20scenes%20depends%20on%20stimulus%20category.Vision%20Res47597607" data-title="The%20duration%20of%20the%20attentional%20blink%20in%20natural%20scenes%20depends%20on%20stimulus%20category." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Einh%C3%A4user%5Bauthor%5D+AND+The+duration+of+the+attentional+blink+in+natural+scenes+depends+on+stimulus+category." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=The+duration+of+the+attentional+blink+in+natural+scenes+depends+on+stimulus+category.&amp;author=Einh%C3%A4user&amp;publication_year=2007" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref17"><span class="order">17. </span><a name="pcbi-0040027-b017" id="pcbi-0040027-b017" class="link-target"></a>Griffin G, Holub A, Perona P (2007) Caltech-256 Object Category Dataset. Pasadena (California): Caltech Technical Report. G. GriffinA. HolubP. Perona2007Caltech-256 Object Category DatasetPasadena (California)Caltech Technical Report 
           <ul class="find-nolinks"></ul></li>
          <li id="ref18"><span class="order">18. </span><a name="pcbi-0040027-b018" id="pcbi-0040027-b018" class="link-target"></a>Wang G, Zhang Y, Fei-Fei L (2006) Using dependent regions for object categorization in a generative framework. IEEE CVPR 2006: 1597–1604.G. WangY. ZhangL. Fei-Fei2006Using dependent regions for object categorization in a generative framework.IEEE CVPR200615971604 
           <ul class="reflinks">
            <li><a href="#" data-author="Wang" data-cit="WangGZhangYFei-FeiL2006Using%20dependent%20regions%20for%20object%20categorization%20in%20a%20generative%20framework.IEEE%20CVPR200615971604" data-title="Using%20dependent%20regions%20for%20object%20categorization%20in%20a%20generative%20framework." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Wang%5Bauthor%5D+AND+Using+dependent+regions+for+object+categorization+in+a+generative+framework." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Using+dependent+regions+for+object+categorization+in+a+generative+framework.&amp;author=Wang&amp;publication_year=2006" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref19"><span class="order">19. </span><a name="pcbi-0040027-b019" id="pcbi-0040027-b019" class="link-target"></a>Mutch J, Lowe DG (2006) Multiclass object recognition with sparse, localized features. IEEE CVPR 2006: 11–18.J. MutchDG Lowe2006Multiclass object recognition with sparse, localized features.IEEE CVPR20061118 
           <ul class="reflinks">
            <li><a href="#" data-author="Mutch" data-cit="MutchJLoweDG2006Multiclass%20object%20recognition%20with%20sparse%2C%20localized%20features.IEEE%20CVPR20061118" data-title="Multiclass%20object%20recognition%20with%20sparse%2C%20localized%20features." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Mutch%5Bauthor%5D+AND+Multiclass+object+recognition+with+sparse%2C+localized+features." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Multiclass+object+recognition+with+sparse%2C+localized+features.&amp;author=Mutch&amp;publication_year=2006" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref20"><span class="order">20. </span><a name="pcbi-0040027-b020" id="pcbi-0040027-b020" class="link-target"></a>Lazebnik S, Schmid C, Ponce J (2006) Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. IEEE CVPR 2006: 2169–2178.S. LazebnikC. SchmidJ. Ponce2006Beyond bags of features: spatial pyramid matching for recognizing natural scene categories.IEEE CVPR200621692178 
           <ul class="reflinks">
            <li><a href="#" data-author="Lazebnik" data-cit="LazebnikSSchmidCPonceJ2006Beyond%20bags%20of%20features%3A%20spatial%20pyramid%20matching%20for%20recognizing%20natural%20scene%20categories.IEEE%20CVPR200621692178" data-title="Beyond%20bags%20of%20features%3A%20spatial%20pyramid%20matching%20for%20recognizing%20natural%20scene%20categories." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Lazebnik%5Bauthor%5D+AND+Beyond+bags+of+features%3A+spatial+pyramid+matching+for+recognizing+natural+scene+categories." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Beyond+bags+of+features%3A+spatial+pyramid+matching+for+recognizing+natural+scene+categories.&amp;author=Lazebnik&amp;publication_year=2006" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref21"><span class="order">21. </span><a name="pcbi-0040027-b021" id="pcbi-0040027-b021" class="link-target"></a>Grauman K, Darrell T (2006) Pyramid match kernels: Discriminative classification with sets of image features (version 2). Cambridge (Massachusetts): MIT. K. GraumanT. Darrell2006Pyramid match kernels: Discriminative classification with sets of image features (version 2)Cambridge (Massachusetts)MITTechnical Report CSAIL-TR-2006–020. Technical Report CSAIL-TR-2006–020. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref22"><span class="order">22. </span><a name="pcbi-0040027-b022" id="pcbi-0040027-b022" class="link-target"></a>PASCAL Object Recognition Database Collection, Visual Object Classes Challenge. PASCAL Object Recognition Database Collection, Visual Object Classes ChallengeAvailable: <a href="http://www.pascal-network.org/challenges/VOC">http://www.pascal-network.org/challenges/VOC</a>. Accessed 26 December 2007. Available: <a href="http://www.pascal-network.org/challenges/VOC">http://www.pascal-network.org/challenges/VOC</a>. Accessed 26 December 2007. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref23"><span class="order">23. </span><a name="pcbi-0040027-b023" id="pcbi-0040027-b023" class="link-target"></a>Varma M, Ray D (2007) Learning the discriminative power-invariance trade-off. Proceedings of the Eleventh IEEE International Conference on Computer Vision. Rio de Janeiro, Brazil: IEEE ICCV. M. VarmaD. Ray2007Learning the discriminative power-invariance trade-off.In:. Proceedings of the Eleventh IEEE International Conference on Computer Vision14–20 October 2007;Rio de Janeiro, BrazilIEEE ICCV 
           <ul class="find-nolinks"></ul></li>
          <li id="ref24"><span class="order">24. </span><a name="pcbi-0040027-b024" id="pcbi-0040027-b024" class="link-target"></a>Hung CP, Kreiman G, Poggio T, DiCarlo JJ (2005) Fast readout of object identity from macaque inferior temporal cortex. Science 310: 863–866.CP HungG. KreimanT. PoggioJJ DiCarlo2005Fast readout of object identity from macaque inferior temporal cortex.Science310863866 
           <ul class="reflinks">
            <li><a href="#" data-author="Hung" data-cit="HungCPKreimanGPoggioTDiCarloJJ2005Fast%20readout%20of%20object%20identity%20from%20macaque%20inferior%20temporal%20cortex.Science310863866" data-title="Fast%20readout%20of%20object%20identity%20from%20macaque%20inferior%20temporal%20cortex." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Hung%5Bauthor%5D+AND+Fast+readout+of+object+identity+from+macaque+inferior+temporal+cortex." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Fast+readout+of+object+identity+from+macaque+inferior+temporal+cortex.&amp;author=Hung&amp;publication_year=2005" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref25"><span class="order">25. </span><a name="pcbi-0040027-b025" id="pcbi-0040027-b025" class="link-target"></a>Russell B, Torralba A, Murphy K, Freeman WT (2005) LabelMe: a database and web-based tool for image annotation. Cambridge (Massachusetts): MIT Artificial Intelligence Lab Memo AIM-2005–025. B. RussellA. TorralbaK. MurphyWT Freeman2005LabelMe: a database and web-based tool for image annotationCambridge (Massachusetts)MIT Artificial Intelligence Lab Memo AIM-2005–025 
           <ul class="find-nolinks"></ul></li>
          <li id="ref26"><span class="order">26. </span><a name="pcbi-0040027-b026" id="pcbi-0040027-b026" class="link-target"></a>Von Ahn L, Liu R, Blum M (2006) Peekaboom: a game for locating objects in images. ACM SIGCHI 2006: 55–64.L. Von AhnR. LiuM. Blum2006Peekaboom: a game for locating objects in images.ACM SIGCHI20065564 
           <ul class="reflinks">
            <li><a href="#" data-author="Von%20Ahn" data-cit="Von%20AhnLLiuRBlumM2006Peekaboom%3A%20a%20game%20for%20locating%20objects%20in%20images.ACM%20SIGCHI20065564" data-title="Peekaboom%3A%20a%20game%20for%20locating%20objects%20in%20images." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Von+Ahn%5Bauthor%5D+AND+Peekaboom%3A+a+game+for+locating+objects+in+images." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Peekaboom%3A+a+game+for+locating+objects+in+images.&amp;author=Von+Ahn&amp;publication_year=2006" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref27"><span class="order">27. </span><a name="pcbi-0040027-b027" id="pcbi-0040027-b027" class="link-target"></a>Bileschi S (2006) StreetScenes: Towards scene understanding in still images. Cambridge (Massachusetts): MIT EECS. S. Bileschi2006StreetScenes: Towards scene understanding in still images[Ph.D. Thesis].Cambridge (Massachusetts)MIT EECS [Ph.D. Thesis]. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref28"><span class="order">28. </span><a name="pcbi-0040027-b028" id="pcbi-0040027-b028" class="link-target"></a>LeCun Y, Huang FJ, Bottou L (2004) Learning methods for generic object recognition with invariance to pose and lighting. IEEE CVPR 2004: 97–104.Y. LeCunFJ HuangL. Bottou2004Learning methods for generic object recognition with invariance to pose and lighting.IEEE CVPR200497104 
           <ul class="reflinks">
            <li><a href="#" data-author="LeCun" data-cit="LeCunYHuangFJBottouL2004Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting.IEEE%20CVPR200497104" data-title="Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=LeCun%5Bauthor%5D+AND+Learning+methods+for+generic+object+recognition+with+invariance+to+pose+and+lighting." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Learning+methods+for+generic+object+recognition+with+invariance+to+pose+and+lighting.&amp;author=LeCun&amp;publication_year=2004" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref29"><span class="order">29. </span><a name="pcbi-0040027-b029" id="pcbi-0040027-b029" class="link-target"></a>Hubel DH, Wiesel TN (1962) Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. J Physiol 160: 106–154.DH HubelTN Wiesel1962Receptive fields, binocular interaction and functional architecture in the cat's visual cortex.J Physiol160106154 
           <ul class="reflinks">
            <li><a href="#" data-author="Hubel" data-cit="HubelDHWieselTN1962Receptive%20fields%2C%20binocular%20interaction%20and%20functional%20architecture%20in%20the%20cat%27s%20visual%20cortex.J%20Physiol160106154" data-title="Receptive%20fields%2C%20binocular%20interaction%20and%20functional%20architecture%20in%20the%20cat%26apos%3Bs%20visual%20cortex." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Hubel%5Bauthor%5D+AND+Receptive+fields%2C+binocular+interaction+and+functional+architecture+in+the+cat%26apos%3Bs+visual+cortex." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Receptive+fields%2C+binocular+interaction+and+functional+architecture+in+the+cat%26apos%3Bs+visual+cortex.&amp;author=Hubel&amp;publication_year=1962" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref30"><span class="order">30. </span><a name="pcbi-0040027-b030" id="pcbi-0040027-b030" class="link-target"></a>Hubel DH, Wiesel TN (1968) Receptive fields and functional architecture of monkey striate cortex. J Physiol 195: 215–243.DH HubelTN Wiesel1968Receptive fields and functional architecture of monkey striate cortex.J Physiol195215243 
           <ul class="reflinks">
            <li><a href="#" data-author="Hubel" data-cit="HubelDHWieselTN1968Receptive%20fields%20and%20functional%20architecture%20of%20monkey%20striate%20cortex.J%20Physiol195215243" data-title="Receptive%20fields%20and%20functional%20architecture%20of%20monkey%20striate%20cortex." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Hubel%5Bauthor%5D+AND+Receptive+fields+and+functional+architecture+of+monkey+striate+cortex." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Receptive+fields+and+functional+architecture+of+monkey+striate+cortex.&amp;author=Hubel&amp;publication_year=1968" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref31"><span class="order">31. </span><a name="pcbi-0040027-b031" id="pcbi-0040027-b031" class="link-target"></a>Jones JP, Palmer LA (1987) An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex. J Neurophysiol 58: 1233–1258.JP JonesLA Palmer1987An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex.J Neurophysiol5812331258 
           <ul class="reflinks">
            <li><a href="#" data-author="Jones" data-cit="JonesJPPalmerLA1987An%20evaluation%20of%20the%20two-dimensional%20Gabor%20filter%20model%20of%20simple%20receptive%20fields%20in%20cat%20striate%20cortex.J%20Neurophysiol5812331258" data-title="An%20evaluation%20of%20the%20two-dimensional%20Gabor%20filter%20model%20of%20simple%20receptive%20fields%20in%20cat%20striate%20cortex." target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Jones%5Bauthor%5D+AND+An+evaluation+of+the+two-dimensional+Gabor+filter+model+of+simple+receptive+fields+in+cat+striate+cortex." target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=An+evaluation+of+the+two-dimensional+Gabor+filter+model+of+simple+receptive+fields+in+cat+striate+cortex.&amp;author=Jones&amp;publication_year=1987" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref32"><span class="order">32. </span><a name="pcbi-0040027-b032" id="pcbi-0040027-b032" class="link-target"></a>Chang CC, Lin CJ (2001) LIBSVM: a Library for Support Vector Machines. CC ChangCJ Lin2001LIBSVM: a Library for Support Vector MachinesAvailable: <a href="http://www.csie.ntu.edu.tw/%E2%88%BCcjlin/libsvm">http://www.csie.ntu.edu.tw/?cjlin/libsvm</a>. Accessed 26 December 2007. Available: <a href="http://www.csie.ntu.edu.tw/%E2%88%BCcjlin/libsvm">http://www.csie.ntu.edu.tw/?cjlin/libsvm</a>. Accessed 26 December 2007. 
           <ul class="find-nolinks"></ul></li>
         </ol>
        </div> 
        <div class="ref-tooltip"> 
         <div class="ref_tooltip-content"> 
         </div> 
        </div> 
       </div> 
      </div> 
     </div> 
    </section> 
    <aside class="article-aside"> 
     <!--[if IE 9]>
<style>
.dload-xml {margin-top: 38px}
</style>
<![endif]--> 
     <div class="dload-menu"> 
      <div class="dload-pdf"> 
       <a href="/ploscompbiol/article/file?id=10.1371/journal.pcbi.0040027&amp;type=printable" id="downloadPdf" target="_blank">Download PDF</a> 
      </div> 
      <div data-js-tooltip-hover="trigger" class="dload-hover">
       &nbsp; 
       <ul class="dload-xml" data-js-tooltip-hover="target"> 
        <li><a href="/ploscompbiol/article/citation?id=10.1371/journal.pcbi.0040027" id="downloadCitation">Citation</a></li> 
        <li><a href="/ploscompbiol/article/file?id=10.1371/journal.pcbi.0040027&amp;type=manuscript" id="downloadXml">XML</a> </li> 
       </ul> 
      </div> 
     </div> 
     <div class="aside-container"> 
      <div class="print-article" id="printArticle" data-js-tooltip-hover="trigger">
        Print 
       <ul class="print-options" data-js-tooltip-hover="target"> 
        <li> <a href="#" onclick="window.print(); return false;" class="preventDefault" id="printBrowser" title="Print
        Article">Print article</a> </li> 
        <li> <a title="Odyssey Press" href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&amp;page=0&amp;journal=3&amp;doi=10.1371%2Fjournal.pcbi.0040027&amp;volume=&amp;issue=&amp;title=Why%20is%20Real-World%20Visual%20Object%20Recognition%20Hard%3F&amp;author_name=Nicolas%20Pinto%2C%20David%20D%20Cox%2C%20James%20J%20DiCarlo&amp;start_page=1&amp;end_page=6">EzReprint </a> </li> 
       </ul> 
      </div> 
      <div class="share-article" id="shareArticle" data-js-tooltip-hover="trigger">
        Share 
       <ul data-js-tooltip-hover="target" class="share-options"> 
        <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027" id="shareReddit" target="_blank" title="Submit to Reddit"><img src="/ploscompbiol/resource/img/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li> 
        <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027" id="shareGoogle" target="_blank" title="Share on Google+"><img src="/ploscompbiol/resource/img/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li> 
        <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027" id="shareStumble" target="_blank" title="Add to StumbleUpon"><img src="/ploscompbiol/resource/img/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li> 
        <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027&amp;t=Why is Real-World Visual Object Recognition Hard?" id="shareFacebook" target="_blank" title="Share on Facebook"><img src="/ploscompbiol/resource/img/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li> 
        <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027&amp;title=Why is Real-World Visual Object Recognition Hard?&amp;summary=Checkout this article I found at PLOS" id="shareLinkedIn" target="_blank" title="Add to LinkedIn"><img src="/ploscompbiol/resource/img/icon.linkedin.16.png" width="16" height="16" alt="LinkedIn">LinkedIn</a></li> 
        <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027&amp;title=Why is Real-World Visual Object Recognition Hard?" id="shareCiteULike" target="_blank" title="Add to CiteULike"><img src="/ploscompbiol/resource/img/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li> 
        <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027" id="shareMendeley" target="_blank" title="Add to Mendeley"><img src="/ploscompbiol/resource/img/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li> 
        <li><a href="https://www.pubchase.com/library?add_aid=10.1371/journal.pcbi.0040027&amp;source=plos" id="sharePubChase" target="_blank" title="Add to PubChase"><img src="/ploscompbiol/resource/img/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li> 
        <li><a href="http://twitter.com/intent/tweet?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.0040027&amp;text=%23PLOSCompBio%3A%20Why is Real-World Visual Object Recognition Hard?" target="_blank" title="share on Twitter" id="twitter-share-link"><img src="/ploscompbiol/resource/img/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li> 
        <li><a href="/ploscompbiol/article/email?id=10.1371/journal.pcbi.0040027" id="shareEmail" title="Email this article"><img src="/ploscompbiol/resource/img/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li> 
       </ul> 
      </div>
     </div> ?
     <!-- Crossmark 2.0 widget --> 
     <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script> 
     <a data-target="crossmark"><img width="150" src="http://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_BW_horizontal.svg"></a> 
     <!-- End Crossmark 2.0 widget --> 
     <div class="skyscraper-container"> 
      <div class="title">
       Advertisement
      </div> 
      <!-- DoubleClick Ad Zone --> 
      <div class="advertisement" id="div-gpt-ad-1458247671871-1" style="width:160px; height:600px;"> 
       <script type="text/javascript">
      googletag.cmd.push(function() { googletag.display('div-gpt-ad-1458247671871-1'); });
    </script> 
      </div> 
     </div> 
     <div class="subject-areas-container"> 
      <h3>Subject Areas 
       <div id="subjInfo">
        ?
       </div> 
       <div id="subjInfoText"> 
        <p>For more information about PLOS Subject Areas, click <a href="/ploscompbiol/s/help-using-this-site#loc-subject-areas">here</a>.</p> 
        <span class="inline-intro">We want your feedback.</span> Do these Subject Areas make sense for this article? Click the target next to the incorrect Subject Area and let us know. Thanks for your help! 
       </div> </h3> 
      <ul id="subjectList"> 
       <li> <a class="taxo-term" title="Search for articles about Object recognition" href="/ploscompbiol/search?filterSubjects=Object+recognition&amp;filterJournals=PLoSCompBiol&amp;q=">Object recognition</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Object recognition">
         <p class="taxo-explain">Is the Subject Area <strong>"Object recognition"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Human performance" href="/ploscompbiol/search?filterSubjects=Human+performance&amp;filterJournals=PLoSCompBiol&amp;q=">Human performance</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Human performance">
         <p class="taxo-explain">Is the Subject Area <strong>"Human performance"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Vision" href="/ploscompbiol/search?filterSubjects=Vision&amp;filterJournals=PLoSCompBiol&amp;q=">Vision</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Vision">
         <p class="taxo-explain">Is the Subject Area <strong>"Vision"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Principal component analysis" href="/ploscompbiol/search?filterSubjects=Principal+component+analysis&amp;filterJournals=PLoSCompBiol&amp;q=">Principal component analysis</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Principal component analysis">
         <p class="taxo-explain">Is the Subject Area <strong>"Principal component analysis"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Support vector machines" href="/ploscompbiol/search?filterSubjects=Support+vector+machines&amp;filterJournals=PLoSCompBiol&amp;q=">Support vector machines</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Support vector machines">
         <p class="taxo-explain">Is the Subject Area <strong>"Support vector machines"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Grayscale" href="/ploscompbiol/search?filterSubjects=Grayscale&amp;filterJournals=PLoSCompBiol&amp;q=">Grayscale</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Grayscale">
         <p class="taxo-explain">Is the Subject Area <strong>"Grayscale"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Imaging techniques" href="/ploscompbiol/search?filterSubjects=Imaging+techniques&amp;filterJournals=PLoSCompBiol&amp;q=">Imaging techniques</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Imaging techniques">
         <p class="taxo-explain">Is the Subject Area <strong>"Imaging techniques"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Primates" href="/ploscompbiol/search?filterSubjects=Primates&amp;filterJournals=PLoSCompBiol&amp;q=">Primates</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Primates">
         <p class="taxo-explain">Is the Subject Area <strong>"Primates"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
      </ul> 
     </div> 
     <div id="subjectErrors"></div> 
     <div class="twitter-container"> 
      <h3>Archived Tweets</h3> 
      <ul id="tweetList"> 
      </ul> 
      <div class="load-more">
       Load more 
       <span></span>
      </div> 
      <div class="view-all">
       <a href="http://alm.plos.org/works/doi.org/10.1371/journal.pcbi.0040027?source_id=twitter">View all tweets</a> 
      </div> 
     </div> 
     <script type="text/template" id="twitterModuleItemTemplate">
  <% _.each(items, function(item) { %>
    <li>
      <div class="tweet-info">
        <a href="http://twitter.com/<%= item.user %>">
          <span class="imgholder">
            <img class="imgLoad" src="<%= item.user_profile_image %>">
          </span>
          <div class="tweetDate"><%= item.created_at %></div>
          <div class="tweetUser">
            <strong><%= item.user_name %></strong>
            <span>@<%= item.user %></span>
          </div>
        </a>
      </div>
      <div class="tweetText">
        <%= item.text %>
      </div>
      <div id="tweetActions">
        <a class="tweet-reply" href="https://twitter.com/intent/tweet?in_reply_to<%= item.id %>&amp;text=@<%= item.user %>">
          <div>&nbsp;</div> Reply
        </a>
        <a class="tweet-retweet" href="https://twitter.com/intent/retweet?tweet_id=<%= item.id %>">
          <div>&nbsp;</div> Retweet
        </a>
        <a class="tweet-favorite" href="https://twitter.com/intent/favorite?tweet_id=<%= item.id %>">
          <div>&nbsp;</div> Favorite
        </a>
      </div>
    </li>
  <% }); %>
</script> 
    </aside> 
   </div> 
  </main> 
  <footer id="pageftr"> 
   <div class="row"> 
    <div class="brand-column"> 
     <img src="/ploscompbiol/resource/img/logo-plos-footer.png" alt="PLOS" class="logo-footer"> 
     <p class="nav-special"> &nbsp; 
      <!--
  Webapp build:  3.3.5 at 20170725 by teamcity, commit: 
  Service build: 2.2.3 at 20170626 by teamcity, commit: 
  Enabled dev features: []
  --> </p> 
     <ul class="nav-aux"> 
      <li><a href="https://www.plos.org/privacy-policy" id="ftr-privacy">Privacy Policy</a></li> 
      <li><a href="https://www.plos.org/terms-of-use" id="ftr-terms">Terms of Use</a></li> 
      <li><a href="https://www.plos.org/advertise/" id="ftr-advertise">Advertise</a></li> 
      <li><a href="https://www.plos.org/media-inquiries" id="ftr-media">Media Inquiries</a></li> 
     </ul> 
    </div> 
    <div class="link-column"> 
     <p class="nav-special"><a href="https://www.plos.org/publications/journals/">Publications</a></p> 
     <div class="nav"> 
      <ul> 
       <li><a href="/plosbiology/" id="ftr-bio">PLOS Biology</a></li> 
       <li><a href="/plosmedicine/" id="ftr-med">PLOS Medicine</a></li> 
       <li><a href="/ploscompbiol/" id="ftr-compbio">PLOS Computational Biology</a></li> 
       <li><a href="http://currents.plos.org" id="ftr-cur">PLOS Currents</a></li> 
       <li><a href="/plosgenetics/" id="ftr-gen">PLOS Genetics</a></li> 
       <li><a href="/plospathogens/" id="ftr-path">PLOS Pathogens</a></li> 
       <li><a href="/plosone/" id="ftr-one">PLOS ONE</a></li> 
       <li><a href="/plosntds/" id="ftr-ntds">PLOS Neglected Tropical Diseases</a></li> 
      </ul> 
     </div> 
    </div> 
    <div class="link-column"> 
     <div class="nav nav-special"> 
      <p><a href="https://www.plos.org" id="ftr-home">plos.org</a></p> 
      <p><a href="http://blogs.plos.org" id="ftr-blog">Blogs</a></p> 
      <p><a href="http://collections.plos.org" id="ftr-collections">Collections</a></p> 
      <p><a href="/ploscompbiol/feedback" id="ftr-feedback">Send us feedback</a></p> 
      <p><a href="/ploscompbiol/s/help-using-this-site" id="ftr-help">Help using this site</a></p> 
      <p><a href="/ploscompbiol/lockss-manifest" id="ftr-lockss">LOCKSS</a></p> 
      <p class="footer-non-profit-statement">PLOS is a nonprofit 501(c)(3) corporation, #C2354500, and is based in San Francisco, California, US</p>
     </div> 
    </div> 
   </div> 
  </footer> 
  <script type="text/javascript">
  var ArticleData = {
    doi: '10.1371/journal.pcbi.0040027',
    title: '<article-title xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Why is Real-World Visual Object Recognition Hard?<\/article-title>',
    date: 'Jan 25, 2008'
  };
</script> 
  <script type="text/javascript" async src="//platform.twitter.com/widgets.js"></script> 
  <!-- This file should be loaded before the renderJs, to avoid conflicts with the FigShare, that implements the MathJax also. --> 
  <!--  mathjax configuration options  --> 
  <!-- more can be found at http://docs.mathjax.org/en/latest/ --> 
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  "HTML-CSS": {
    scale: 100,
    availableFonts: ["STIX","TeX"],
    preferredFont: "STIX",
    webFont: "STIX-Web",
    linebreaks: { automatic: false }
  },
  jax: ["input/MathML", "output/HTML-CSS"]
});
</script> 
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=MML_HTMLorMML"></script> 
  <script src="/ploscompbiol/resource/compiled/asset_7ICN7WUC3M5RIDTJZA36SPT4TFQKFMRT.js"></script> 
  <div class="reveal-modal-bg"></div>   
 </body>
</html>