<!doctype html>
<html lang="en">
 <head>
  <meta charset="utf-8">
  <title>Basic web scraping with Node.js and Cheerio.js · GitBook</title>
  <link rel="icon" href="https://www.gitbook.com/assets/images/logo/favicon.ico?version=19.7.21">
  <link rel="stylesheet" href="https://www.gitbook.com/assets/style.css?version=19.7.21">
  <link rel="stylesheet" href="https://www.gitbook.com/assets/fonts/socicon/socicon.css?version=19.7.21">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/images/logo/apple-touch-icon-152.png">
  <meta name="description" content="basic-web-scraping-with-node-js-and-cheerio-js: ">
  <meta property="og:description" content="basic-web-scraping-with-node-js-and-cheerio-js: ">
  <meta name="keywords" content="gitbook,book,git,markdown,publish,open source">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta property="og:site_name" content="GitBook">
  <meta property="twitter:creator" content="@GitBookIO">
  <meta property="robots" content="index, follow">
 </head>
 <body>
  <div id="application">
   <div class="PJAXWrapper" data-reactroot="" data-reactid="1" data-react-checksum="-1971026258">
    <div class="gb-page-wrapper with-pagehead" data-reactid="2">
     <div class="gb-page-header" data-reactid="3">
      <div class="container" data-reactid="4">
       <a href="/" class="logo pull-left" data-reactid="5"><span class="Icon SVGIcon size-sm LogoText" data-reactid="6">
         <!--?xml version="1.0" encoding="UTF-8"?--> 
         <svg width="1275px" height="261px" viewbox="0 0 1275 261" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"> 
          <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"> 
           <g id="Desktop-HD" transform="translate(-82.000000, -381.000000)"> 
            <g id="Page-1" transform="translate(81.000000, 381.000000)"> 
             <path d="M164.072025,217.262237 C169.917345,217.262237 174.67264,222.009129 174.67264,227.841541 C174.67264,233.671375 169.917345,238.418267 164.072025,238.418267 C158.229288,238.418267 153.473993,233.671375 153.473993,227.841541 C153.473993,222.009129 158.229288,217.262237 164.072025,217.262237 M330.509952,151.736561 C324.664632,151.736561 319.909337,146.989669 319.909337,141.157257 C319.909337,135.324845 324.664632,130.577953 330.509952,130.577953 C336.350106,130.577953 341.105402,135.324845 341.105402,141.157257 C341.105402,146.989669 336.350106,151.736561 330.509952,151.736561 M330.509952,108.395708 C312.413483,108.395708 297.687823,123.095345 297.687823,141.157257 C297.687823,144.676816 298.274163,148.178325 299.428762,151.581855 L191.012673,209.191747 C184.852228,200.332257 174.868948,195.077414 164.072025,195.077414 C151.562576,195.077414 140.158649,202.222247 134.641369,213.392012 L37.2391749,162.132796 C26.951102,156.730982 19.2486081,139.813894 20.0648348,124.415502 C20.4884462,116.383688 23.2703328,110.149041 27.5038632,107.740786 C30.1927619,106.219512 33.4266729,106.351012 36.8543085,108.1456 L37.5052235,108.485953 C63.309353,122.058816 147.775904,166.469718 151.330106,168.117335 C156.818972,170.651933 159.872074,171.678149 169.219936,167.253561 L343.82013,76.6088059 C346.377296,75.6444725 349.360657,73.1975412 349.360657,69.4820216 C349.360657,64.3251588 344.021604,62.2907765 344.006106,62.2907765 C334.079653,57.5387275 318.811564,50.4042078 303.925758,43.4476 C272.103248,28.5726294 236.034292,11.7148451 220.195361,3.43550196 C206.520981,-3.70933137 195.512252,2.31388431 193.549175,3.53090392 L189.736673,5.41573725 C118.448568,40.6061686 23.0456121,87.7708353 17.6161547,91.0712275 C7.8885919,96.9784137 1.86504534,108.746375 1.08498057,123.358345 C-0.131610526,146.520394 11.7062599,170.667404 28.6248834,179.526894 L131.619264,232.549757 C133.938794,248.590178 147.770738,260.60309 164.072025,260.60309 C182.0006,260.60309 196.620357,246.174188 196.888989,228.339178 L310.323839,166.972512 C316.076171,171.458982 323.20524,173.918806 330.509952,173.918806 C348.603839,173.918806 363.329499,159.221747 363.329499,141.157257 C363.329499,123.095345 348.603839,108.395708 330.509952,108.395708" id="Fill-1" fill="#3681FC"></path> 
             <path d="M579.646628,195.850943 C573.517178,202.650267 564.549016,208.126855 552.744725,212.280708 C540.940433,216.434561 527.999623,218.512776 513.924879,218.512776 C492.281955,218.512776 474.988798,211.904257 462.050571,198.684639 C449.109761,185.465022 442.184749,167.070492 441.278117,143.501051 L441.164466,129.224276 C441.164466,112.982737 444.03934,98.8013647 449.791672,86.6775804 C455.541421,74.5537961 463.770846,65.2224529 474.479947,58.6887078 C485.186466,52.1575412 497.57968,48.8880902 511.654425,48.8880902 C532.23832,48.8880902 548.221899,53.591149 559.612911,62.9946882 C570.998757,72.4008059 577.642223,86.4326294 579.532976,105.090159 L541.167737,105.090159 C539.803915,95.8748451 536.854134,89.3024235 532.313227,85.3728941 C527.77232,81.4459431 521.34066,79.4811784 513.015664,79.4811784 C503.027219,79.4811784 495.309227,83.7123843 489.859105,92.1722176 C484.411567,100.632051 481.650344,112.719737 481.572854,128.432698 L481.572854,138.403492 C481.572854,154.871933 484.390903,167.240669 490.029583,175.512276 C495.66568,183.783884 504.540854,187.919688 516.647356,187.919688 C527.015502,187.919688 534.733494,185.614571 539.803915,181.006914 L539.803915,155.400512 L512.109032,155.400512 L512.109032,128.092345 L579.646628,128.092345 L579.646628,195.850943 Z" id="Fill-4" fill="#000000"></path> 
             <path d="M603.029198,216.247882 L641.39702,216.247882 L641.39702,93.646049 L603.029198,93.646049 L603.029198,216.247882 Z M600.758745,61.918451 C600.758745,56.4470196 602.726988,51.9631275 606.663474,48.4667745 C610.597377,44.9704216 615.706543,43.2222451 621.985806,43.2222451 C628.267652,43.2222451 633.374235,44.9704216 637.310721,48.4667745 C641.244623,51.9631275 643.212866,56.4470196 643.212866,61.918451 C643.212866,67.3924608 641.244623,71.8763529 637.310721,75.3701275 C633.374235,78.8664804 628.267652,80.6146569 621.985806,80.6146569 C615.706543,80.6146569 610.597377,78.8664804 606.663474,75.3701275 C602.726988,71.8763529 600.758745,67.3924608 600.758745,61.918451 L600.758745,61.918451 Z" id="Fill-6" fill="#000000"></path> 
             <path d="M709.502874,63.1648647 L709.502874,93.644502 L729.707069,93.644502 L729.707069,120.158512 L709.502874,120.158512 L709.502874,176.247129 C709.502874,180.857365 710.334599,184.067512 712.000632,185.880149 C713.664081,187.692786 716.957401,188.597816 721.875425,188.597816 C725.659514,188.597816 728.836599,188.373492 731.409263,187.919688 L731.409263,215.227855 C724.522996,217.416943 717.334518,218.512776 709.84383,218.512776 C696.675717,218.512776 686.95332,215.41608 680.671474,209.222688 C674.389628,203.029296 671.248704,193.623178 671.248704,181.006914 L671.248704,120.158512 L655.585417,120.158512 L655.585417,93.644502 L671.248704,93.644502 L671.248704,63.1648647 L709.502874,63.1648647 Z" id="Fill-8" fill="#000000"></path> 
             <path d="M786.349587,146.107845 L786.349587,185.653247 L812.455927,185.653247 C819.646988,185.653247 825.169433,184.010786 829.028429,180.723286 C832.890008,177.438365 834.819506,172.81008 834.819506,166.84359 C834.819506,153.095394 827.969401,146.18262 814.274356,146.107845 L786.349587,146.107845 Z M786.349587,120.047639 L807.12204,120.047639 C815.824154,119.972865 822.02851,118.381973 825.737692,115.274963 C829.446874,112.167953 831.298883,107.583502 831.298883,101.52161 C831.298883,94.5521098 829.294478,89.5319039 825.283085,86.4635706 C821.274275,83.3952373 814.80387,81.8610706 805.874453,81.8610706 L786.349587,81.8610706 L786.349587,120.047639 Z M746.506874,216.246335 L746.506874,51.2679824 L805.874453,51.2679824 C827.137676,51.2679824 843.312397,55.1588353 854.398615,62.9379627 C865.484834,70.7196686 871.027943,82.013198 871.027943,96.818551 C871.027943,105.355737 869.0597,112.642384 865.125798,118.686227 C861.191895,124.730071 855.400818,129.188178 847.757733,132.057973 C856.384939,134.324414 863.007741,138.55562 867.623555,144.749012 C872.239368,150.942404 874.548567,158.497208 874.548567,167.410845 C874.548567,183.575031 869.418737,195.719443 859.164243,203.838924 C848.912332,211.960982 833.721733,216.096786 813.592445,216.246335 L746.506874,216.246335 Z" id="Fill-9" fill="#000000"></path> 
             <path d="M928.124294,156.192348 C928.124294,167.524554 929.901395,175.834838 933.458181,181.120623 C937.014966,186.408985 942.276529,189.051877 949.237703,189.051877 C962.85784,189.051877 969.821598,178.591181 970.123808,157.664632 L970.123808,153.812456 C970.123808,131.831328 963.085144,120.839475 949.010399,120.839475 C936.221986,120.839475 929.296974,130.320368 928.237946,149.279574 L928.124294,156.192348 Z M889.870124,153.812456 C889.870124,141.575221 892.254229,130.67877 897.02244,121.123103 C901.79065,111.567436 908.638172,104.221485 917.567589,99.08525 C926.497006,93.9464363 936.978804,91.3783186 949.010399,91.3783186 C967.398747,91.3783186 981.889355,97.0637598 992.484804,108.432064 C1003.08025,119.802946 1008.3754,135.268377 1008.3754,154.833515 L1008.3754,156.192348 C1008.3754,175.303681 1003.05959,190.467436 992.430561,201.686191 C981.793784,212.904946 967.398747,218.513034 949.237703,218.513034 C931.755986,218.513034 917.719986,213.283975 907.124537,202.818123 C896.53167,192.357426 890.818083,178.173475 889.983776,160.271426 L889.870124,153.812456 Z" id="Fill-10" fill="#000000"></path> 
             <path d="M1058.66142,156.192348 C1058.66142,167.524554 1060.43852,175.834838 1063.99531,181.120623 C1067.55209,186.408985 1072.81365,189.051877 1079.77483,189.051877 C1093.39497,189.051877 1100.35872,178.591181 1100.66093,157.664632 L1100.66093,153.812456 C1100.66093,131.831328 1093.62227,120.839475 1079.54752,120.839475 C1066.75911,120.839475 1059.8341,130.320368 1058.77507,149.279574 L1058.66142,156.192348 Z M1020.40725,153.812456 C1020.40725,141.575221 1022.79135,130.67877 1027.55956,121.123103 C1032.32777,111.567436 1039.1753,104.221485 1048.10471,99.08525 C1057.03413,93.9464363 1067.51593,91.3783186 1079.54752,91.3783186 C1097.93587,91.3783186 1112.42648,97.0637598 1123.02193,108.432064 C1133.61738,119.802946 1138.91252,135.268377 1138.91252,154.833515 L1138.91252,156.192348 C1138.91252,175.303681 1133.59671,190.467436 1122.96769,201.686191 C1112.33091,212.904946 1097.93587,218.513034 1079.77483,218.513034 C1062.29311,218.513034 1048.25711,213.283975 1037.66166,202.818123 C1027.0688,192.357426 1021.35521,178.173475 1020.5209,160.271426 L1020.40725,153.812456 Z" id="Fill-11" fill="#000000"></path> 
             <polygon id="Fill-12" fill="#000000" points="1205.08991 170.582831 1194.3059 181.347782 1194.3059 216.246851 1156.05431 216.246851 1156.05431 42.0892824 1194.3059 42.0892824 1194.3059 134.549253 1198.1649 129.451694 1226.9963 93.6450176 1272.85481 93.6450176 1229.83501 144.068822 1275.69352 216.246851 1231.87816 216.246851"></polygon> 
            </g> 
           </g> 
          </g> 
         </svg> </span></a>
       <a class="WeAreHiring" href="/about" data-reactid="7">We are hiring!</a>
       <div data-reactid="8">
        <a href="/join" class="btn btn-primary btn-md pull-right hidden-xs btn-signup" role="button" data-reactid="9">
         <!-- react-text: 10 -->
         <!-- /react-text -->
         <!-- react-text: 11 --> 
         <!-- /react-text -->
         <!-- react-text: 12 -->Sign Up
         <!-- /react-text --></a>
        <a href="/login" class="btn btn-link btn-md pull-right hidden-xs btn-login" role="button" data-reactid="13">
         <!-- react-text: 14 -->
         <!-- /react-text -->
         <!-- react-text: 15 --> 
         <!-- /react-text -->
         <!-- react-text: 16 -->Sign In
         <!-- /react-text --></a>
        <a href="/pricing" class="btn btn-link btn-md" role="button" data-reactid="17">
         <!-- react-text: 18 -->
         <!-- /react-text -->
         <!-- react-text: 19 --> 
         <!-- /react-text -->
         <!-- react-text: 20 -->Pricing
         <!-- /react-text --></a>
        <a href="/explore" class="btn btn-link btn-md" role="button" data-reactid="21">
         <!-- react-text: 22 -->
         <!-- /react-text -->
         <!-- react-text: 23 --> 
         <!-- /react-text -->
         <!-- react-text: 24 -->Explore
         <!-- /react-text --></a>
        <a href="/about" class="btn btn-link btn-md" role="button" data-reactid="25">
         <!-- react-text: 26 -->
         <!-- /react-text -->
         <!-- react-text: 27 --> 
         <!-- /react-text -->
         <!-- react-text: 28 -->About
         <!-- /react-text --></a>
        <a href="/blog" class="btn btn-link btn-md" role="button" data-reactid="29">
         <!-- react-text: 30 -->
         <!-- /react-text -->
         <!-- react-text: 31 --> 
         <!-- /react-text -->
         <!-- react-text: 32 -->Blog
         <!-- /react-text --></a>
       </div>
      </div>
     </div>
     <div class="gb-page-body" data-reactid="33">
      <div id="BookHome" class="BookPage" data-reactid="34">
       <div class="pagehead" data-reactid="35">
        <div class="container clearfix" data-reactid="36">
         <div class="overview" data-reactid="37">
          <h1 class="overview-title" data-reactid="38"><a href="https://www.gitbook.com/@kevinchisholm" class="overview-steptitle" data-reactid="39">kevinchisholm</a>
           <div class="overview-stepdivider" data-reactid="40">
            <i class="Icon octicon octicon-chevron-right" data-reactid="41"></i>
           </div>
           <div class="overview-steptitle primary" data-reactid="42">
            <!-- react-text: 43 -->Basic web scraping with Node.js and Cheerio.js
            <!-- /react-text -->
           </div></h1>
          <p class="overview-note" data-reactid="44">
           <!-- react-text: 45 -->Updated
           <!-- /react-text -->
           <!-- react-text: 46 --> 
           <!-- /react-text --><span data-reactid="47">a year ago</span></p>
         </div>
        </div>
        <div class="container clearfix" data-reactid="48">
         <ul class="menu pull-left" data-reactid="49">
          <li class="active" data-reactid="50"><a href="/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js/details" data-reactid="51">About</a></li>
          <li class="" data-reactid="52"><a href="/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js/discussions" data-reactid="53">
            <!-- react-text: 54 -->0
            <!-- /react-text -->
            <!-- react-text: 55 --> 
            <!-- /react-text -->
            <!-- react-text: 56 -->Discussions
            <!-- /react-text --></a></li>
          <li class="" data-reactid="57"><a href="/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js/changes" data-reactid="58">
            <!-- react-text: 59 -->0
            <!-- /react-text -->
            <!-- react-text: 60 --> 
            <!-- /react-text -->
            <!-- react-text: 61 -->Change Requests
            <!-- /react-text --></a></li>
          <!-- react-text: 62 -->
          <!-- /react-text -->
          <!-- react-text: 63 -->
          <!-- /react-text -->
          <!-- react-text: 64 -->
          <!-- /react-text -->
         </ul>
         <div class="btn-toolbar pull-right head-toolbar" data-reactid="65">
          <div class="btn-group" data-reactid="66">
           <button type="button" class="btn btn-default btn-sm" role="button" data-reactid="67">
            <!-- react-text: 68 -->
            <!-- /react-text -->
            <!-- react-text: 69 --> 
            <!-- /react-text --><i class="Icon octicon octicon-star" data-reactid="70"></i>
            <!-- react-text: 71 --> Star
            <!-- /react-text --></button>
           <a href="/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js/stars" class="btn btn-count btn-sm" role="button" data-reactid="72">
            <!-- react-text: 73 -->
            <!-- /react-text -->
            <!-- react-text: 74 --> 
            <!-- /react-text -->
            <!-- react-text: 75 -->1
            <!-- /react-text --></a>
          </div>
          <div class="btn-group" data-reactid="76">
           <button type="button" class="btn btn-default btn-sm" role="button" data-reactid="77">
            <!-- react-text: 78 -->
            <!-- /react-text -->
            <!-- react-text: 79 --> 
            <!-- /react-text --><i class="Icon octicon octicon-rss" data-reactid="80"></i>
            <!-- react-text: 81 --> 
            <!-- /react-text -->
            <!-- react-text: 82 -->Subscribe
            <!-- /react-text --></button>
           <a href="/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js/subscriptions" class="btn btn-count btn-sm" role="button" data-reactid="83">
            <!-- react-text: 84 -->
            <!-- /react-text -->
            <!-- react-text: 85 --> 
            <!-- /react-text -->
            <!-- react-text: 86 -->1
            <!-- /react-text --></a>
          </div>
          <!-- react-text: 87 -->
          <!-- /react-text -->
         </div>
        </div>
       </div>
       <div class="gb-page-inner" data-reactid="88">
        <div class="container" data-reactid="89">
         <!-- react-empty: 90 -->
         <div data-reactid="91">
          <div class="panel panel-default" data-reactid="92">
           <div data-reactid="93">
            <div style="padding-bottom:0;" data-reactid="94"></div>
            <div class="" style="transform:translateZ(0);" data-reactid="95">
             <div class="panel-heading" data-reactid="96">
              <!-- react-text: 97 -->
              <!-- /react-text -->
              <div class="btn-toolbar " data-reactid="98">
               <div class="btn-group pull-right" data-reactid="99">
                <a data-nopjax="true" href="https://www.gitbook.com/read/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js" class="btn btn-success btn-md" role="button" data-reactid="100">
                 <!-- react-text: 101 -->
                 <!-- /react-text -->
                 <!-- react-text: 102 --> 
                 <!-- /react-text --><b data-reactid="103">Read</b></a>
               </div>
               <div class="btn-group dropdown pull-right" data-reactid="104">
                <a href="https://www.gitbook.com/download/pdf/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js" data-nopjax="true" class="btn btn-default btn-md" role="button" data-reactid="105">
                 <!-- react-text: 106 -->
                 <!-- /react-text -->
                 <!-- react-text: 107 --> 
                 <!-- /react-text --><i class="Icon octicon octicon-file-pdf" data-reactid="108"></i>
                 <!-- react-text: 109 --> Download PDF
                 <!-- /react-text --></a>
                <button type="button" class="btn btn-default btn-md dropdown-toggle" role="button" data-reactid="110">
                 <!-- react-text: 111 -->
                 <!-- /react-text -->
                 <!-- react-text: 112 --> 
                 <!-- /react-text --><span class="caret" data-reactid="113"></span></button>
               </div>
              </div>
             </div>
            </div>
           </div>
           <div class="panel-body" data-reactid="114">
            <div class="gb-markdown BookAbout" data-reactid="115">
             <p>It makes perfect sense that there are security rules that limit the reach of client-side JavaScript. If you relax any of these rules, the user is open to malicious activity. On the server side, JavaScript is not subject to these kinds of limitations. With this freedom comes a great deal of power. Web scraping is one of the cool upsides to this freedom.</p> 
             <p>To get started, clone the following git hub repository:</p> 
             <p><a href="https://github.com/kevinchisholm/basic-web-scraping-with-node-and-cheeriojs" target="_blank">https://github.com/kevinchisholm/basic-web-scraping-with-node-and-cheeriojs</a></p> 
             <p>(Instructions on how to run the code are available in the Git hub page.)</p> 
             <h2>The page we will scrape</h2> 
             <p>Lets' take a moment to look at the example web page that we will scrape: <a href="http://output.jsbin.com/xavuga" target="_blank">http://output.jsbin.com/xavuga</a>. If you use your web developer tools to inspect the DOM, you'll see that there are three main sections to the page. There is a HEADER element, a SECTION element, and a FOOTER element. We will target those three sections of the page later in some of the code examples.</p> 
             <h2>The request NPM module</h2> 
             <p>A key tool we'll need is the <a href="https://www.npmjs.com/package/request" target="_blank">request</a> NPM module. This module allows you to make an HTTP request and use the return value as you wish.</p> 
             <h2>The cheerio NPM module</h2> 
             <p>The <a href="https://www.npmjs.com/package/cheerio" target="_blank">cheerio</a> NPM module provides a server-side jQuery implementation. There is not a 1:1 method replication; that was not their goal. Cheerio's functionality mirrors the most common tasks associated with jQuery. They key point is: you can parse HTML with JavaScript on the server-side.</p> 
             <h3>Caching an entire web page</h3> 
             <p><strong>Example # 1</strong></p> 
             <pre><code>var fs = require('fs'),
    request = require('request'),
    cheerio = require('cheerio'),
    pageURL = 'http://output.jsbin.com/xavuga';

function scrapePage () {
    //make an HTTP request for the page to be scraped
    request(pageURL, function(error, response, responseHtml){        

        //write the entire scraped page to the local file system
        fs.writeFile(__dirname + '/HTML/entire-page.html', responseHtml, function(err){
            console.log('entire-page.html successfully written to HTML folder');
        })
    }) ;
}

//scrape the page
scrapePage();
</code></pre>
             <p>In <strong>example # 1</strong>, we set some variables. The <strong>fs</strong> variable references the <strong>file system</strong> node module. This module provides access to the local file system. We'll need this to write files to disk. The <strong>request</strong> variable refers to the request node module, which we discussed earlier. The <strong>cheerio</strong> variable refers to the cheerio node module, which we also discussed earlier. The <strong>pageUrl</strong> variable is the URL of the web page that we will scrape.</p> 
             <p>At the highest level, there are two things that happen in this code. We defined a function named <strong>scrapePage</strong>, and then we execute that function. Let's look at what happens inside of this function.</p> 
             <p>First, we call the request function, passing it two arguments. The first argument is the URL of the request. The second argument is a callback function. This callback function takes three arguments. The first argument is an error object. This "error first" pattern is common in Node.js. The second object is the response object. The third argument is the contents of the request. In this case, the contents of the request is HTML.</p> 
             <p>Inside of the request callback, we leverage the <strong>file-system</strong> module's <strong>writeFile</strong> method. The first argument we pass is the full path of the file name. This tells the <strong>fs</strong> module the what file to write. The second argument is the content that we want to write to the file. Notice that we pass the <strong>responseHtml</strong> variable. Again, this is the content returned by the request function. The third argument is a callback function. In this case, we are using the callback function to log a message indicating that the file write to disk was successful.</p> 
             <p>When you run <strong>example # 1</strong>, you should see a new file in the HTML folder: <strong>content.html</strong>. This file contains the entire contents of the web page that we make a request to.</p> 
             <h3>Caching only a part of a web page</h3> 
             <p><strong>Example # 2</strong></p> 
             <pre><code>function scrapePage () {
    //make an HTTP request for the page to be scraped
    request(pageURL, function(error, response, responseHtml){        

        //write the entire scraped page to the local file system
        fs.writeFile(__dirname + '/HTML/entire-page.html', responseHtml, function(err){
            console.log('entire-page.html successfully written to HTML folder');
        })

        //write isolated sections of the entire scraped page to the local file system

        //create the cheerio object
        var $ = cheerio.load(responseHtml),
            //create a reference to the header element
            $header = $('header').html();

        //write the header to the local file system
        fs.writeFile(__dirname + '/HTML/header.html', $header, function(err){
            console.log('header.html successfully written to HTML folder');
        });
    });
}
</code></pre>
             <p>In <strong>example # 2</strong>, we have an updated version of the <strong>scrapePage</strong> function. For brevity sake, I have omitted the parts of the code that have not changed.</p> 
             <p>The first change to the <strong>scrapePage</strong> function is the use of the <strong>cheerio.load</strong> method, and assigned it to the <strong>$</strong> variable. Now. we can use the <strong>$</strong> variable much the same way we would jQuery. We created the <strong>$header </strong> variable, which contains the HTML of the HTML <strong>header</strong> element. We then use the <strong>file-system</strong> module's <strong>writeFile</strong> metho to write the HTML <strong>header</strong> element to the file: <strong>header.html</strong>. </p> 
             <p>When you run <strong>example # 2</strong>, you should see another new file in the HTML folder: <strong>header.html</strong>. This file contains the entire contents of the web page that we make a request to.</p> 
             <p><strong>Example # 3</strong></p> 
             <pre><code>function scrapePage () {
    //make an HTTP request for the page to be scraped
    request(pageURL, function(error, response, responseHtml){        

        //write the entire scraped page to the local file system
        fs.writeFile(__dirname + '/HTML/entire-page.html', responseHtml, function(err){
            console.log('entire-page.html successfully written to HTML folder');
        })

        //write isolated sections of the entire scraped page to the local file system

        //create the cheerio object
        var $ = cheerio.load(responseHtml),
            //create a reference to the header element
            $header = $('header').html(),
            $content = $('#mainContent').html(),
            $footer = $('footer').html();

        //write the header to the local file system
        fs.writeFile(__dirname + '/HTML/header.html', $header, function(err){
            console.log('header.html successfully written to HTML folder');
        });

        //write the content to the local file system
        fs.writeFile(__dirname + '/HTML/content.html', $content, function(err){
            console.log('content.html successfully written to HTML folder');
        })

        //write the footer to the local file system
        fs.writeFile(__dirname + '/HTML/footer.html', $footer, function(err){
            console.log('footer.html successfully written to HTML folder');
        });
    });
}
</code></pre>
             <p>In <strong>example # 3</strong>, we have updated the <strong>scrapePage</strong> function again. The new code follows the same pattern as <strong>example # 2</strong>. The difference is that we have also scraped the content and footer sections. In both cases, we write the associated HTML file to disk.</p> 
             <p>When you run <strong>example # 3</strong>, you should see four files in the HTML folder. They are <strong>entire-page.html</strong>, <strong>header.html</strong>, <strong>conten.html</strong> and <strong>footer.html</strong>.</p> 
             <h2>Summary</h2> 
             <p>In this article, we scratched the surface of what is possible when scraping web pages. The high-level areas we focused on are: making a request and then parsing the HTML of the request. We used the <strong>request</strong> module to make the HTTP request, and the <strong>cheerio</strong> module to parse the returned HTML. We also used the <strong>fs</strong> (file-system) module to write our scraped HTML to disk. Hopefully this article has pointed you in the right direction. Happy web page scraping!</p> 
             <h2>Author:</h2> 
             <p>Kevin Chisholm <a href="http://blog.kevinchisholm.com/" target="_blank">blog.kevinchisholm.com</a></p> 
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="gb-page-footer" data-reactid="116">
      <div class="Footer" data-reactid="117">
       <div class="container" data-reactid="118">
        <div class="footer-logo" data-reactid="119">
         <span class="Icon SVGIcon size-sm Logo" data-reactid="120">
          <!--?xml version="1.0" encoding="UTF-8"?--> 
          <svg width="1067px" height="769px" viewbox="0 0 1067 769" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"> 
           <defs></defs> 
           <g id="Page-1" stroke="none" stroke-width="1" fill-rule="evenodd" fill="#3884ff"> 
            <g id="GitBook" transform="translate(-186.000000, -128.000000)"> 
             <path d="M666.026437,768.67745 C683.230722,768.67745 697.226738,782.67354 697.226738,799.870648 C697.226738,817.067756 683.230722,831.063847 666.026437,831.063847 C648.829421,831.063847 634.833405,817.067756 634.833405,799.870648 C634.833405,782.67354 648.829421,768.67745 666.026437,768.67745 M1155.95679,575.450573 C1138.7525,575.450573 1124.75649,561.453028 1124.75649,544.25592 C1124.75649,527.051544 1138.7525,513.055453 1155.95679,513.055453 C1173.1538,513.055453 1187.14982,527.051544 1187.14982,544.25592 C1187.14982,561.453028 1173.1538,575.450573 1155.95679,575.450573 M1155.95679,447.646843 C1102.68819,447.646843 1059.34822,490.987043 1059.34822,544.25592 C1059.34822,554.629425 1061.07083,564.957866 1064.47099,574.997022 L745.327505,744.879153 C727.195774,718.750597 697.806756,703.261571 666.026437,703.261571 C629.204762,703.261571 595.634965,724.326937 579.395922,757.264559 L292.679649,606.108815 C262.392257,590.182229 239.714873,540.291718 242.120711,494.88583 C243.367967,471.199467 251.557992,452.814697 264.01747,445.716348 C271.932749,441.223004 281.45289,441.616953 291.542872,446.904009 L293.45882,447.914321 C369.419335,487.935756 618.059409,618.895438 628.521533,623.758019 C644.679169,631.228512 653.662902,634.257994 681.181035,621.21116 L1195.13924,353.910249 C1202.66929,351.066839 1211.45387,343.849287 1211.45387,332.888493 C1211.45387,317.682936 1195.73379,311.689387 1195.68873,311.689387 C1166.47124,297.671491 1121.52641,276.635198 1077.70237,256.119325 C984.03314,212.258705 877.860826,162.546998 831.229728,138.132338 C790.977371,117.059703 758.574877,134.820841 752.797959,138.414353 L741.5712,143.968888 C531.726898,247.743185 250.898022,386.826066 234.910466,396.55268 C206.278815,413.973656 188.548298,448.67896 186.25294,491.767673 C182.666715,560.072026 217.514296,631.273576 267.321681,657.402132 L570.493595,813.756259 C577.324431,861.062115 618.044872,896.481179 666.026437,896.481179 C718.805142,896.481179 761.834025,853.934692 762.629187,801.341779 L1096.54059,620.379652 C1113.47013,633.612557 1134.46124,640.866452 1155.95679,640.866452 C1209.22538,640.866452 1252.5668,597.524798 1252.5668,544.25592 C1252.5668,490.987043 1209.22538,447.646843 1155.95679,447.646843" id="GitBook_logo_blue"></path> 
            </g> 
           </g> 
          </svg> </span>
        </div>
        <ul class="menu" data-reactid="121">
         <li class="" data-reactid="122"><a href="/about" data-reactid="123">About</a></li>
         <li class="" data-reactid="124"><a href="https://help.gitbook.com" data-reactid="125">Help</a></li>
         <li class="" data-reactid="126"><a href="/explore" data-reactid="127">Explore</a></li>
         <li class="" data-reactid="128"><a href="/editor" data-reactid="129">Editor</a></li>
         <li class="" data-reactid="130"><a href="/blog" data-reactid="131">Blog</a></li>
         <li class="" data-reactid="132"><a href="/pricing" data-reactid="133">Pricing</a></li>
         <li class="" data-reactid="134"><a href="/contact" data-reactid="135">Contact</a></li>
         <li class="footer-copyright" data-reactid="136"><a href="https://www.gitbook.com" data-reactid="137">© GitBook.com</a></li>
        </ul>
       </div>
      </div>
     </div>
     <!-- react-empty: 138 -->
    </div>
    <div class="LoadingBar" data-reactid="139">
     <div class="bar" style="width:0%;display:none;" data-reactid="140">
      <div class="LoadingBar-shadow" style="display:none;" data-reactid="141"></div>
     </div>
    </div>
   </div>
  </div>
  <script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
              ga('create', 'UA-57505611-6', 'auto');
              ga('send', 'pageview');
            </script>
  <script src="https://www.gitbook.com/assets/js/vendor.js?version=19.7.21"></script>
  <script src="https://www.gitbook.com/assets/js/BookHome.js?version=19.7.21"></script>
  <script>(function() { var ReactDOM = require("react-dom"); var React = require("react"); var PJAXWrapper = require("react-pjax");window.appPayloadSize = 14531;ReactDOM.render(React.createElement(PJAXWrapper, { Component: window.Pages.BookHome, props: {"book":{"id":"kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js","status":"published","name":"basic-web-scraping-with-node-js-and-cheerio-js","title":"Basic web scraping with Node.js and Cheerio.js","description":"","public":true,"template":"base","topics":[],"license":"nolicense","language":"en","locked":false,"cover":{"large":"/cover/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js.jpg?build=1466975419167","small":"/cover/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js.jpg?build=1466975419167\u0026size=small"},"urls":{"git":"https://git.gitbook.com/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js.git","access":"https://www.gitbook.com/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js","homepage":"https://kevinchisholm.gitbooks.io/basic-web-scraping-with-node-js-and-cheerio-js/","read":"https://www.gitbook.com/read/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js","edit":"https://www.gitbook.com/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js/edit","content":"https://kevinchisholm.gitbooks.io/basic-web-scraping-with-node-js-and-cheerio-js/content/","download":{"epub":"https://www.gitbook.com/download/epub/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js","mobi":"https://www.gitbook.com/download/mobi/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js","pdf":"https://www.gitbook.com/download/pdf/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js"}},"counts":{"stars":1,"subscriptions":1,"updates":15,"discussions":0,"changeRequests":0},"dates":{"build":"2016-06-26T21:10:19.167Z","created":"2016-06-20T23:25:45.078Z"},"permissions":{"edit":false,"admin":false,"important":false},"publish":{"ebooks":true,"defaultBranch":null,"builder":null},"author":{"id":"572f5b546bc7010f005daa43","type":"User","username":"kevinchisholm","name":"kevinchisholm","location":"","website":"","bio":"","verified":true,"locked":false,"site_admin":false,"urls":{"profile":"https://www.gitbook.com/@kevinchisholm","stars":"https://www.gitbook.com/@kevinchisholm/starred","avatar":"https://s.gravatar.com/avatar/1e5e1048e7215903c4224f58dde04997?s=220\u0026d=https%3A%2F%2Fwww.gitbook.com%2Fassets%2Fimages%2Favatars%2Fuser.png"},"permissions":{},"dates":{"created":"2016-05-08T15:29:24.907Z"},"counts":{}},"features":{"ebooks":true,"discussions":true,"comments":true}},"summary":[],"readme":{"content":[{"type":"normal","content":"\u003cp\u003eIt makes perfect sense that there are security rules that limit the reach of client-side JavaScript. If you relax any of these rules, the user is open to malicious activity. On the server side, JavaScript is not subject to these kinds of limitations. With this freedom comes a great deal of power. Web scraping is one of the cool upsides to this freedom.\u003c/p\u003e\n\u003cp\u003eTo get started, clone the following git hub repository:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/kevinchisholm/basic-web-scraping-with-node-and-cheeriojs\" target=\"_blank\"\u003ehttps://github.com/kevinchisholm/basic-web-scraping-with-node-and-cheeriojs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e(Instructions on how to run the code are available in the Git hub page.)\u003c/p\u003e\n\u003ch2\u003eThe page we will scrape\u003c/h2\u003e\n\u003cp\u003eLets\u0026apos; take a moment to look at the example web page that we will scrape:  \u003ca href=\"http://output.jsbin.com/xavuga\" target=\"_blank\"\u003ehttp://output.jsbin.com/xavuga\u003c/a\u003e. If you use your web developer tools to inspect the DOM, you\u0026apos;ll see that there are three main sections to the page. There is a HEADER element, a SECTION element, and a FOOTER element. We will target those three sections of the page later in some of the code examples.\u003c/p\u003e\n\u003ch2\u003eThe request NPM module\u003c/h2\u003e\n\u003cp\u003eA key tool we\u0026apos;ll need is the \u003ca href=\"https://www.npmjs.com/package/request\" target=\"_blank\"\u003erequest\u003c/a\u003e NPM module. This module allows you to make an HTTP request and use the return value as you wish.\u003c/p\u003e\n\u003ch2\u003eThe cheerio NPM module\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.npmjs.com/package/cheerio\" target=\"_blank\"\u003echeerio\u003c/a\u003e NPM module provides a server-side jQuery implementation. There is not a 1:1 method replication; that was not their goal. Cheerio\u0026apos;s functionality mirrors the most common tasks associated with jQuery. They key point is: you can parse HTML with JavaScript on the server-side.\u003c/p\u003e\n\u003ch3\u003eCaching an entire web page\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eExample # 1\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evar fs = require(\u0026apos;fs\u0026apos;),\n    request = require(\u0026apos;request\u0026apos;),\n    cheerio = require(\u0026apos;cheerio\u0026apos;),\n    pageURL = \u0026apos;http://output.jsbin.com/xavuga\u0026apos;;\n\nfunction scrapePage () {\n    //make an HTTP request for the page to be scraped\n    request(pageURL, function(error, response, responseHtml){        \n\n        //write the entire scraped page to the local file system\n        fs.writeFile(__dirname + \u0026apos;/HTML/entire-page.html\u0026apos;, responseHtml, function(err){\n            console.log(\u0026apos;entire-page.html successfully written to HTML folder\u0026apos;);\n        })\n    }) ;\n}\n\n//scrape the page\nscrapePage();\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn \u003cstrong\u003eexample # 1\u003c/strong\u003e, we set some variables. The \u003cstrong\u003efs\u003c/strong\u003e variable references the \u003cstrong\u003efile system\u003c/strong\u003e node module. This module provides access to the local file system. We\u0026apos;ll need this to write files to disk. The \u003cstrong\u003erequest\u003c/strong\u003e variable refers to the request node module, which we discussed earlier. The \u003cstrong\u003echeerio\u003c/strong\u003e variable refers to the cheerio node module, which we also discussed earlier. The \u003cstrong\u003epageUrl\u003c/strong\u003e variable is the URL of the web page that we will scrape.\u003c/p\u003e\n\u003cp\u003eAt the highest level, there are two things that happen in this code. We defined a function named \u003cstrong\u003escrapePage\u003c/strong\u003e, and then we execute that function. Let\u0026apos;s look at what happens inside of this function.\u003c/p\u003e\n\u003cp\u003eFirst, we call the request function, passing it two arguments. The first argument is the URL of the request. The second argument is a callback function. This callback function takes three arguments. The first argument is an error object. This \u0026quot;error first\u0026quot; pattern is common in Node.js. The second object is the response object. The third argument is the contents of the request. In this case, the contents of the request is HTML.\u003c/p\u003e\n\u003cp\u003eInside of the request callback, we leverage the \u003cstrong\u003efile-system\u003c/strong\u003e module\u0026apos;s \u003cstrong\u003ewriteFile\u003c/strong\u003e method. The first argument we pass is the full path of the file name. This tells the \u003cstrong\u003efs\u003c/strong\u003e module the what file to write. The second argument is the content that we want to write to the file. Notice that we pass the \u003cstrong\u003eresponseHtml\u003c/strong\u003e variable. Again, this is the content returned by the request function. The third argument is a callback function. In this case, we are using the callback function to log a message indicating that the file write to disk was successful.\u003c/p\u003e\n\u003cp\u003eWhen you run \u003cstrong\u003eexample # 1\u003c/strong\u003e, you should see a new file in the HTML folder: \u003cstrong\u003econtent.html\u003c/strong\u003e. This file contains the entire contents of the web page that we make a request to.\u003c/p\u003e\n\u003ch3\u003eCaching only a part of a web page\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eExample # 2\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efunction scrapePage () {\n    //make an HTTP request for the page to be scraped\n    request(pageURL, function(error, response, responseHtml){        \n\n        //write the entire scraped page to the local file system\n        fs.writeFile(__dirname + \u0026apos;/HTML/entire-page.html\u0026apos;, responseHtml, function(err){\n            console.log(\u0026apos;entire-page.html successfully written to HTML folder\u0026apos;);\n        })\n\n        //write isolated sections of the entire scraped page to the local file system\n\n        //create the cheerio object\n        var $ = cheerio.load(responseHtml),\n            //create a reference to the header element\n            $header = $(\u0026apos;header\u0026apos;).html();\n\n        //write the header to the local file system\n        fs.writeFile(__dirname + \u0026apos;/HTML/header.html\u0026apos;, $header, function(err){\n            console.log(\u0026apos;header.html successfully written to HTML folder\u0026apos;);\n        });\n    });\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn \u003cstrong\u003eexample # 2\u003c/strong\u003e, we have an updated version of the \u003cstrong\u003escrapePage\u003c/strong\u003e function. For brevity sake, I have omitted the parts of the code that have not changed.\u003c/p\u003e\n\u003cp\u003eThe first change to the \u003cstrong\u003escrapePage\u003c/strong\u003e function is the use of the \u003cstrong\u003echeerio.load\u003c/strong\u003e method, and assigned it to the \u003cstrong\u003e$\u003c/strong\u003e variable. Now. we can use the \u003cstrong\u003e$\u003c/strong\u003e variable much the same way we would jQuery. We created the \u003cstrong\u003e$header \u003c/strong\u003e variable, which contains the HTML of the HTML \u003cstrong\u003eheader\u003c/strong\u003e element. We then use the \u003cstrong\u003efile-system\u003c/strong\u003e module\u0026apos;s \u003cstrong\u003ewriteFile\u003c/strong\u003e metho to write the HTML \u003cstrong\u003eheader\u003c/strong\u003e element to the file: \u003cstrong\u003eheader.html\u003c/strong\u003e. \u003c/p\u003e\n\u003cp\u003eWhen you run \u003cstrong\u003eexample # 2\u003c/strong\u003e, you should see another new file in the HTML folder: \u003cstrong\u003eheader.html\u003c/strong\u003e. This file contains the entire contents of the web page that we make a request to.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExample # 3\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efunction scrapePage () {\n    //make an HTTP request for the page to be scraped\n    request(pageURL, function(error, response, responseHtml){        \n\n        //write the entire scraped page to the local file system\n        fs.writeFile(__dirname + \u0026apos;/HTML/entire-page.html\u0026apos;, responseHtml, function(err){\n            console.log(\u0026apos;entire-page.html successfully written to HTML folder\u0026apos;);\n        })\n\n        //write isolated sections of the entire scraped page to the local file system\n\n        //create the cheerio object\n        var $ = cheerio.load(responseHtml),\n            //create a reference to the header element\n            $header = $(\u0026apos;header\u0026apos;).html(),\n            $content = $(\u0026apos;#mainContent\u0026apos;).html(),\n            $footer = $(\u0026apos;footer\u0026apos;).html();\n\n        //write the header to the local file system\n        fs.writeFile(__dirname + \u0026apos;/HTML/header.html\u0026apos;, $header, function(err){\n            console.log(\u0026apos;header.html successfully written to HTML folder\u0026apos;);\n        });\n\n        //write the content to the local file system\n        fs.writeFile(__dirname + \u0026apos;/HTML/content.html\u0026apos;, $content, function(err){\n            console.log(\u0026apos;content.html successfully written to HTML folder\u0026apos;);\n        })\n\n        //write the footer to the local file system\n        fs.writeFile(__dirname + \u0026apos;/HTML/footer.html\u0026apos;, $footer, function(err){\n            console.log(\u0026apos;footer.html successfully written to HTML folder\u0026apos;);\n        });\n    });\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn \u003cstrong\u003eexample # 3\u003c/strong\u003e, we have updated the \u003cstrong\u003escrapePage\u003c/strong\u003e function again. The new code follows the same pattern as \u003cstrong\u003eexample # 2\u003c/strong\u003e. The difference is that we have also scraped the content and footer sections. In both cases, we write the associated HTML file to disk.\u003c/p\u003e\n\u003cp\u003eWhen you run \u003cstrong\u003eexample # 3\u003c/strong\u003e, you should see four files in the HTML folder. They are \u003cstrong\u003eentire-page.html\u003c/strong\u003e, \u003cstrong\u003eheader.html\u003c/strong\u003e, \u003cstrong\u003econten.html\u003c/strong\u003e and \u003cstrong\u003efooter.html\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this article, we scratched the surface of what is possible when scraping web pages. The high-level areas we focused on are: making a request and then parsing the HTML of the request. We used the \u003cstrong\u003erequest\u003c/strong\u003e module to make the HTTP request, and the \u003cstrong\u003echeerio\u003c/strong\u003e module to parse the returned HTML. We also used the \u003cstrong\u003efs\u003c/strong\u003e (file-system) module to write our scraped HTML to disk. Hopefully this article has pointed you in the right direction. Happy web page scraping!\u003c/p\u003e\n\u003ch2\u003eAuthor:\u003c/h2\u003e\n\u003cp\u003eKevin Chisholm \u003ca href=\"http://blog.kevinchisholm.com/\" target=\"_blank\"\u003eblog.kevinchisholm.com\u003c/a\u003e\u003c/p\u003e\n"}],"progress":{"chapters":[]}},"languages":[],"hasEbooksVersion":true,"config":{"version":"19.7.21","hasBlog":true,"isEnterprise":false,"apiHost":"https://api.gitbook.com","mainHost":"https://www.gitbook.com","stripePublicKey":"pk_live_iPkbPsG28bjTrkbfHXUq0cA0","gaToken":"UA-57505611-6","mixpanelToken":"aa7ba0807002f1cbd33391bdd14d5ad9","github":{"integrationUrl":"https://github.com/integrations/gitbook/installations/new"}},"req":{"date":"2017-10-08T00:26:34.973Z","csrfToken":"GNlhIHoV-xF4GKyAsNGrQa9YIbLRuKdmfkRQ","query":{},"url":"/book/kevinchisholm/basic-web-scraping-with-node-js-and-cheerio-js/details","duration":108}} }),document.getElementById("application"));})();</script>
 </body>
</html>