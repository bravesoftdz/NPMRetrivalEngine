<!--?xml version="1.0" encoding="utf-8"?--><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"> 
  <!-- AppResources meta begin --> 
  <script type="text/javascript">var ncbi_startTime = new Date();</script> 
  <!-- AppResources meta end --> 
  <!-- TemplateResources meta begin --> 
  <meta name="paf_template" content=""> 
  <!-- TemplateResources meta end --> 
  <!-- Logger begin --> 
  <meta name="ncbi_db" content="pmc">
  <meta name="ncbi_pdid" content="article">
  <meta name="ncbi_acc" content="">
  <meta name="ncbi_domain" content="tswj">
  <meta name="ncbi_report" content="record">
  <meta name="ncbi_type" content="fulltext">
  <meta name="ncbi_objectid" content="">
  <meta name="ncbi_pcid" content="/articles/PMC4005080/">
  <meta name="ncbi_app" content="pmc"> 
  <!-- Logger end --> 
  <title>A Grammar-Based Semantic Similarity Algorithm for Natural Language Sentences</title> 
  <!-- AppResources external_resources begin --> 
  <link rel="stylesheet" href="/core/jig/1.14.8/css/jig.min.css">
  <script type="text/javascript" src="/core/jig/1.14.8/js/jig.min.js"></script> 
  <!-- AppResources external_resources end --> 
  <!-- Page meta begin --> 
  <meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
  <link rel="canonical" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4005080/">
  <link rel="schema.DC" href="http://purl.org/DC/elements/1.0/">
  <meta name="citation_journal_title" content="The Scientific World Journal">
  <meta name="citation_title" content="A Grammar-Based Semantic Similarity Algorithm for Natural Language Sentences">
  <meta name="citation_authors" content="Ming Che Lee, Jia Wei Chang, Tung Cheng Hsieh">
  <meta name="citation_date" content="2014">
  <meta name="citation_volume" content="2014">
  <meta name="citation_doi" content="10.1155/2014/437162">
  <meta name="citation_abstract_html_url" content="/pmc/articles/PMC4005080/?report=abstract">
  <meta name="citation_pmid" content="24982952">
  <meta name="DC.Title" content="A Grammar-Based Semantic Similarity Algorithm for Natural Language Sentences">
  <meta name="DC.Type" content="Text">
  <meta name="DC.Publisher" content="Hindawi">
  <meta name="DC.Contributor" content="Ming Che Lee">
  <meta name="DC.Contributor" content="Jia Wei Chang">
  <meta name="DC.Contributor" content="Tung Cheng Hsieh">
  <meta name="DC.Date" content="2014">
  <meta name="DC.Identifier" content="10.1155/2014/437162">
  <meta name="DC.Language" content="en">
  <meta property="og:title" content="A Grammar-Based Semantic Similarity Algorithm for Natural Language Sentences">
  <meta property="og:type" content="article">
  <meta property="og:description" content="This paper presents a grammar and semantic corpus based similarity algorithm for natural language sentences. Natural language, in opposition to “artificial language”, such as computer programming languages, is the language used by the ...">
  <meta property="og:url" content="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4005080/">
  <meta property="og:site_name" content="PubMed Central (PMC)">
  <meta property="og:image" content="https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-logo-share.png">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@ncbi">
  <link rel="stylesheet" href="/corehtml/pmc/css/3.8/pmc.min.css" type="text/css">
  <link rel="stylesheet" href="/corehtml/pmc/css/3.8/pmc_extras_prnt.min.css" type="text/css" media="print">
  <script type="text/javascript" src="/corehtml/pmc/js/common.min.js">//</script>
  <script type="text/javascript" src="/corehtml/pmc/js/NcbiTagServer.min.js">//</script>
  <meta name="citationexporter" content="backend:'/pmc/utils/ctxp/'">
  <script type="text/javascript" src="/corehtml/pmc/ctxp/jquery.citationexporter.js">//</script>
  <link rel="stylesheet" href="/corehtml/pmc/ctxp/citationexporter.css" type="text/css">
  <script type="text/javascript" src="/core/mathjax/2.6.1/MathJax.js?config=/corehtml/pmc/js/mathjax-config-classic.3.4.js"></script>
  <script type="text/javascript" src="/corehtml/pmc/js/large-obj-scrollbars.min.js"></script>
  <script type="text/javascript">window.name="mainwindow";</script>
  <style type="text/css">.pmc-wm {background:transparent repeat-y top left;background-image:url(/corehtml/pmc/pmcgifs/wm-tswj.gif);background-size: auto, contain}</style>
  <style type="text/css">.print-view{display:block}</style>
  <style type="text/css">
        div.pmc_para_cit li.highlight,
        div.pmc_para_cit li.highlight .one_line_source
        { background: #E0E0E0; }
        a.bibr.highlight { background: #E0E0E0; } 
      </style>
  <meta name="cited_in_systematic_reviews" content="">
  <link rel="alternate" type="application/epub+zip" href="/pmc/articles/PMC4005080/epub/">
  <link rel="alternate" type="application/pdf" href="/pmc/articles/PMC4005080/pdf/TSWJ2014-437162.pdf"> 
  <!-- Page meta end --> 
  <link rel="shortcut icon" href="//www.ncbi.nlm.nih.gov/favicon.ico">
  <meta name="ncbi_phid" content="F4FB5C579D37DCA10000000000080008"> 
  <meta name="referrer" content="origin-when-cross-origin">
  <link type="text/css" rel="stylesheet" href="//static.pubmed.gov/portal/portal3rc.fcgi/4158155/css/3852956/3985586/3808861/4121862/3974050/3917732/251717/4048120/3846471/14534/45193/4113719/3849091/3984811/3751656/4033350/3840896/3577051/3852958/3881636/3579733/4062871/12930/3964959/3855473/4047625/3854974/3854955/4076335/4128070/9685/3549676/3609192/3609193/3609213/3395586/4143404.css">
  <link type="text/css" rel="stylesheet" href="//static.pubmed.gov/portal/portal3rc.fcgi/4158155/css/3411343/3882866/4157116.css" media="print">
 </head> 
 <body class="article"> 
  <div class="grid"> 
   <div class="col twelve_col nomargin shadow"> 
    <!-- System messages like service outage or JS required; this is handled by the TemplateResources portlet --> 
    <div class="sysmessages"> 
     <noscript> 
      <p class="nojs"> <strong>Warning:</strong> The NCBI web site requires JavaScript to function. <a href="/guide/browsers/#enablejs" title="Learn how to enable JavaScript" target="_blank">more...</a> </p> 
     </noscript> 
    </div> 
    <!--/.sysmessage--> 
    <div class="wrap"> 
     <div class="page"> 
      <div class="top"> 
       <div class="universal_header" id="universal_header">
        <ul class="inline_list jig-ncbimenu ui-ncbimenu resources_list" id="navcontent">
         <li class="ui-ncbimenu-item-leaf ui-ncbimenu-item-first ui-helper-reset ui-ncbimenu-item-no-hlt"><a class="ui-ncbimenu-link-first" href="/" role="banner" title="NCBI Home" id="ncbihome" accesskey="1"><span class="offscreen_noflow">NCBI</span><img src="//static.pubmed.gov/portal/portal3rc.fcgi/4158155/img/28977" class="ncbi_logo" title="NCBI" alt="NCBI Logo"></a></li>
         <li class="offscreen_noflow ui-ncbimenu-item-skip access"><a href="#maincontent" title="Skip to the content" tabindex="0" accesskey="3">Skip to main content</a></li>
         <li class="offscreen_noflow ui-ncbimenu-item-skip access"><a href="#navcontent" title="Skip to the navigation" tabindex="0" accesskey="4">Skip to navigation</a></li>
         <li id="resource-menu" class="topmenu ui-helper-reset ui-ncbimenu-item-first ui-helper-reset"><a class="ui-ncbimenu-first-link-has-submenu ui-ncbimenu-link-first topanchor expandDown" href="/static/header_footer_ajax/submenu/#resources">Resources</a></li>
         <li id="all-howtos-menu" class="topmenu ui-helper-reset ui-ncbimenu-item-first"><a class="ui-ncbimenu-first-link-has-submenu ui-ncbimenu-link-first topanchor expandDown" href="/static/header_footer_ajax/submenu/#howto">How To</a></li>
         <li class="offscreen_noflow ui-ncbimenu-item-skip access"><a href="/guide/browsers/#accesskeys" title="About My NCBI Accesskeys" tabindex="0" accesskey="0">About NCBI Accesskeys</a></li>
        </ul>
        <div class="myncbi">
         <span id="myncbiusername" style="display:none"><a href="/account/settings/" id="mnu" title="Edit account settings"></a></span>
         <a accesskey="2" href="/myncbi/" id="myncbi" style="display:none">My NCBI</a>
         <a href="/account/" id="sign_in">Sign in to NCBI</a>
         <a href="/account/signout/" id="sign_out" style="display:none">Sign Out</a>
        </div>
       </div> 
       <div class="header"> 
        <div class="res_logo"> 
         <h1 class="img_logo"><a href="/pmc/" class="pmc_logo offscreen">PMC</a></h1> 
         <div class="NLMLogo"> 
          <a href="https://www.nlm.nih.gov/" title="US National Library of Medicine">US National Library of Medicine</a> 
          <br> 
          <a href="https://www.nih.gov/" title="National Institutes of Health">National Institutes of Health</a> 
         </div> 
        </div> 
        <div class="search">
         <form method="get" action="/pmc/">
          <div class="search_form">
           <label for="database" class="offscreen_noflow">Search database</label>
           <select id="database"><optgroup label="Recent"><option value="pmc" selected class="last" data-ac_dict="pmc-search-autocomplete">PMC</option></optgroup><optgroup label="All"><option value="gquery">All Databases</option><option value="assembly">Assembly</option><option value="biocollections">Biocollections</option><option value="bioproject">BioProject</option><option value="biosample">BioSample</option><option value="biosystems">BioSystems</option><option value="books">Books</option><option value="clinvar">ClinVar</option><option value="clone">Clone</option><option value="cdd">Conserved Domains</option><option value="gap">dbGaP</option><option value="dbvar">dbVar</option><option value="nucest">EST</option><option value="gene">Gene</option><option value="genome">Genome</option><option value="gds">GEO DataSets</option><option value="geoprofiles">GEO Profiles</option><option value="nucgss">GSS</option><option value="gtr">GTR</option><option value="homologene">HomoloGene</option><option value="ipg">Identical Protein Groups</option><option value="medgen">MedGen</option><option value="mesh">MeSH</option><option value="ncbisearch">NCBI Web Site</option><option value="nlmcatalog">NLM Catalog</option><option value="nuccore">Nucleotide</option><option value="omim">OMIM</option><option value="pmc" data-ac_dict="pmc-search-autocomplete">PMC</option><option value="popset">PopSet</option><option value="probe">Probe</option><option value="protein">Protein</option><option value="proteinclusters">Protein Clusters</option><option value="pcassay">PubChem BioAssay</option><option value="pccompound">PubChem Compound</option><option value="pcsubstance">PubChem Substance</option><option value="pubmed">PubMed</option><option value="pubmedhealth">PubMed Health</option><option value="snp">SNP</option><option value="sparcle">Sparcle</option><option value="sra">SRA</option><option value="structure">Structure</option><option value="taxonomy">Taxonomy</option><option value="toolkit">ToolKit</option><option value="toolkitall">ToolKitAll</option><option value="toolkitbook">ToolKitBook</option><option value="toolkitbookgh">ToolKitBookgh</option><option value="unigene">UniGene</option></optgroup></select>
           <div class="nowrap">
            <label for="term" class="offscreen_noflow" accesskey="/">Search term</label>
            <div class="nowrap">
             <input type="text" name="term" id="term" title="Search PMC. Use up and down arrows to choose an item from the autocomplete." value="" class="jig-ncbiclearbutton jig-ncbiautocomplete" data-jigconfig="dictionary:'pmc-search-autocomplete',disableUrl:'NcbiSearchBarAutoComplCtrl'" autocomplete="off" data-sbconfig="ds:'no',pjs:'no',afs:'yes'">
            </div>
            <button id="search" type="submit" class="button_search nowrap" cmd="go">Search</button>
           </div>
          </div>
         </form>
         <ul class="searchlinks inline_list">
          <li> <a href="/pmc/advanced/">Advanced</a> </li>
          <li> <a href="/pmc/journals/">Journal list</a> </li>
          <li class="help"> <a target="_blank" href="/books/NBK3825/">Help</a> </li>
         </ul>
        </div> 
       </div> 
       <!--<component id="Page" label="headcontent"/>--> 
      </div> 
      <div class="content"> 
       <!-- site messages --> 
       <div class="container"> 
        <div id="maincontent" class="content eight_col col"> 
         <div class="navlink-box"> 
          <ul class="page-breadcrumbs inline_list small">
           <li class="journal-list"><a href="/pmc/journals/" class="navlink">Journal List</a></li>
           <li class="archive"><a class="navlink" href="/pmc/journals/1623/">ScientificWorldJournal</a></li>
           <li class="issue-page"><a class="navlink" href="/pmc/issues/233213/">v.2014; 2014</a></li>
           <li class="accid">PMC4005080</li>
          </ul> 
         </div> 
         <!-- Journal banner --> 
         <div class="pmc-page-banner whole_rhythm">
          <div>
           <img src="/corehtml/pmc/pmcgifs/logo-tswj.gif" alt="Logo of tswj" usemap="#logo-imagemap">
           <map id="logo-imagemap" name="logo-imagemap"><area shape="rect" coords="0,0,500,75" alt="The Scientific World Journal" title="The Scientific World Journal" href="http://www.tswj.com" ref="reftype=publisher&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CBanner&amp;TO=Publisher%7COther%7CN/A&amp;rendering-type=normal" target="pmc_ext"></map>
          </div> 
         </div> 
         <!--component id='MainPortlet' label='search-reference'/--> 
         <!-- Book content --> 
         <div class=""> 
          <div class="hide-overflow article lit-style content pmc-wm slang-all page-box">
           <!--main-content-->
           <div class="jig-ncbiinpagenav" data-jigconfig="smoothScroll: false, allHeadingLevels: ['h2'], headingExclude: ':hidden'">
            <div class="fm-sec half_rhythm no_top_margin">
             <div class="fm-citation half_rhythm no_top_margin clearfix">
              <div class="small">
               <div class="inline_block eight_col va_top">
                <div>
                 <div>
                  <span class="cit"><span id="pmcmata">ScientificWorldJournal</span>. 2014; 2014: 437162. </span>
                 </div>
                 <div>
                  <span class="fm-vol-iss-date">Published online 2014 Apr 10. </span> 
                  <span class="doi">doi:&nbsp; <a href="//dx.doi.org/10.1155%2F2014%2F437162" target="pmc_ext" ref="reftype=other&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CFront%20Matter&amp;TO=Content%20Provider%7CCrosslink%7CDOI&amp;rendering-type=normal">10.1155/2014/437162</a></span>
                 </div>
                </div>
               </div>
               <div class="inline_block four_col va_top show-overflow align_right">
                <div class="fm-citation-ids">
                 <div class="fm-citation-pmcid">
                  <span class="fm-citation-ids-label">PMCID: </span>
                  <span>PMC4005080</span>
                 </div>
                </div>
               </div>
              </div>
             </div>
             <h1 class="content-title">A Grammar-Based Semantic Similarity Algorithm for Natural Language Sentences</h1>
             <div class="half_rhythm">
              <div class="contrib-group fm-author">
               <a href="/pubmed/?term=Lee%20MC%5BAuthor%5D&amp;cauthor=true&amp;cauthor_uid=24982952">Ming Che Lee</a>,
               <sup> 1 </sup> 
               <a href="/pubmed/?term=Chang%20JW%5BAuthor%5D&amp;cauthor=true&amp;cauthor_uid=24982952">Jia Wei Chang</a>,
               <sup> 2 ,</sup>
               <sup>*</sup> and 
               <a href="/pubmed/?term=Hsieh%20TC%5BAuthor%5D&amp;cauthor=true&amp;cauthor_uid=24982952">Tung Cheng Hsieh</a>
               <sup> 3 </sup>
              </div>
             </div>
             <div class="fm-panel small half_rhythm">
              <div class="fm-authors-info fm-panel hide half_rhythm" id="idm140112003537392_ai" style="display:none">
               <div class="fm-affl" lang="en"> 
                <sup>1</sup>Department of Computer and Communication Engineering, Ming Chuan University, Taoyuan 333, Taiwan
               </div>
               <div class="fm-affl" lang="en"> 
                <sup>2</sup>Department of Engineering Science, National Cheng Kung University, Tainan 701, Taiwan
               </div>
               <div class="fm-affl" lang="en"> 
                <sup>3</sup>Department of Visual Communication Design, Hsuan Chuang University, Hsinchu 300, Taiwan
               </div>
               <div id="cor1">
                *Jia Wei Chang: 
                <a href="mailto:dev@null" data-email="wt.ude.ukcn.liam@31011089n" class="oemail">wt.ude.ukcn.liam@31011089n</a>
               </div>
               <div id="__fnidm140111971384096">
                Academic Editors: J. G. Duque, J. T. Fernandez-Breis, and P. Melin
               </div>
              </div>
              <div class="togglers">
               <a href="#" class="pmctoggle" rid="idm140112003537392_ai">Author information <span>?</span></a> 
               <a href="#" class="pmctoggle" rid="idm140112003537392_an">Article notes <span>?</span></a> 
               <a href="#" class="pmctoggle" rid="idm140112003537392_cpl">Copyright and License information <span>?</span></a>
              </div>
              <div class="fm-article-notes fm-panel hide half_rhythm" id="idm140112003537392_an" style="display:none">
               <div class="fm-pubdate half_rhythm">
                Received 2013 Dec 17; Accepted 2014 Mar 10.
               </div>
              </div>
              <div class="fm-cpl-info fm-panel hide half_rhythm" id="idm140112003537392_cpl" style="display:none">
               <div class="fm-copyright half_rhythm">
                <a href="/pmc/about/copyright/">Copyright</a> © 2014 Ming Che Lee et al.
               </div>
               <div class="fm-copyright half_rhythm">
                <div class="license">
                 This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.
                </div>
               </div>
              </div>
             </div>
             <div id="pmclinksbox" class="links-box whole_rhythm hidden"></div>
            </div>
            <div class="sec"></div>
            <div id="__abstractidm140111971376928" lang="en" class="tsec sec">
             <h2 class="head no_bottom_margin" id="__abstractidm140111971376928title">Abstract</h2>
             <!--article-meta-->
             <div>
              <p id="__p2" class="p p-first-last">This paper presents a grammar and semantic corpus based similarity algorithm for natural language sentences. Natural language, in opposition to “artificial language”, such as computer programming languages, is the language used by the general public for daily communication. Traditional information retrieval approaches, such as vector models, LSA, HAL, or even the ontology-based approaches that extend to include concept similarity comparison instead of cooccurrence terms/words, may not always determine the perfect matching while there is no obvious relation or concept overlap between two natural language sentences. This paper proposes a sentence similarity algorithm that takes advantage of corpus-based ontology and grammatical rules to overcome the addressed problems. Experiments on two famous benchmarks demonstrate that the proposed algorithm has a significant performance improvement in sentences/short-texts with arbitrary syntax and structure.</p>
             </div>
            </div>
            <div id="sec1" class="tsec sec">
             <h2 class="head no_bottom_margin" id="sec1title">1. Introduction</h2>
             <p id="__p3" class="p p-first"> Natural language, a term in opposition to artificial language, is the language used by the general public for daily communication. An artificial language is often characterized by self-created vocabularies, strict grammar, and a limited ideographic range and therefore belongs to a linguistic category that is less easy to be accustomed to, yet not difficult to be mastered by the general public. A natural language is inseparable from the entire social culture and varies constantly over time; individuals can easily develop a sense of this first language while growing up. In addition, the syntactic and semantic flexibility of a natural language enables this type of language to be natural to human beings. However, due to its endless exceptions, changes, and indications, a natural language also becomes the type of language that is the most difficult to be mastered.</p>
             <p id="__p4">Natural language processing (NLP) studies how to enable a computer to process and understand the language used by human beings in their daily lives, to comprehend human knowledge, and to communicate with human beings in a natural language. Applications of NLP include information retrieval (IR), knowledge extraction, question-answering (QA) systems, text categorization, machine translation, writing assistance, voice identification, composition, and so on. The development of the Internet and the large production of digital documents have resulted in an urgent need for intelligent text processing, and the theory as well as the skill of NLP has therefore become more important.</p>
             <p id="__p5">Traditionally, techniques for detecting similarity between texts have centered on developing document models. In recent years, several types of document models have been established, such as the Boolean model, the vector-based model, and the statistical probability model. The Boolean model achieves the coverage of keywords using the intersection and union of sets. The Boolean algorithm is prone to be misused and thus, a retrieval method that approximates a natural language is a direction for further improvement. Salton and Lesk first proposed the retrieval system of a vector space model (VSM) [<a href="#B49" rid="B49" class=" bibr popnode">1</a>–<a href="#B51" rid="B51" class=" bibr popnode">3</a>], which was not only a binary comparison method. The primary contribution of this method was in suggesting the concepts of partial comparison and similarity, so that the system can calculate the similarity between a document and a query based on the different weights of index terms, and further output the result of retrieval ranking. Concerning the actualization of a vector model, first users' queries and documents in a database should be transformed into vectors in the same dimension. While both the documents and queries are represented by the same vector space dimension, the most common evaluation on semantic similarity in a high dimensional space is to calculate the similarity between two vectors using cosine, whose value should fall between 0 and 1. Overall, the advantages of a vector space model include the following. (1) With given weights, VSM can better select characteristics, and the retrieval efficacy is largely improved compared to the Boolean model. (2) VSM provides the mechanism of partial comparison, which enables the retrieval of documents with the most similar distribution. Wu et al. present a VSM-based FAQ retrieval system. The vector elements are composited by the question category segment and the keyword segment [<a href="#B65" rid="B65" class=" bibr popnode">4</a>]. A phrase-based document similarity measure is proposed by Chim and Deng [<a href="#B7" rid="B7" class=" bibr popnode">5</a>]. In [<a href="#B7" rid="B7" class=" bibr popnode">5</a>], the TF-IDF weighted phases in Suffix Tree [<a href="#B67" rid="B67" class=" bibr popnode">6</a>, <a href="#B68" rid="B68" class=" bibr popnode">7</a>] are mapped into a high dimensional term space of the VSM. Very recently, Li et al. [<a href="#B33" rid="B33" class=" bibr popnode">8</a>] presented a novel sentence similarity computation measure. Their measure, taking the semantic information and word order into account, which acquired good performance in measuring, is basically a VSM-based model.</p>
             <p id="__p6">A need for a method of semantic analysis on shorter documents or sentences has gradually occurred in the fields of NLP applications in recent years [<a href="#B36" rid="B36" class=" bibr popnode">9</a>]. With regard to the applications in text mining, the technique of semantic analysis of short texts/sentences can also be applied in databases as a certain assessment standard to look for undiscovered knowledge [<a href="#B3" rid="B3" class=" bibr popnode">10</a>]. Furthermore, the technique of semantic analysis of short texts/sentences can be employed in other fields, such as text summarization [<a href="#B11" rid="B11" class=" bibr popnode">11</a>], text categorization [<a href="#B26" rid="B26" class=" bibr popnode">12</a>], and machine translation [<a href="#B35" rid="B35" class=" bibr popnode">13</a>]. Recently, a concept under development emphasizes that the similarity between texts is the “latent semantic analysis (LSA), which is based on the statistical data of vocabulary in a large corpus. LSA and the hyperspace analog to language (HAL) are both famous corpus-based algorithms [<a href="#B12" rid="B12" class=" bibr popnode">14</a>–<a href="#B28" rid="B28" class=" bibr popnode">16</a>]. LSA, also known as latent semantic indexing (LSI), is a fully automatic mathematical/statistical technique that analyzes a large corpus of natural language text and a similarity representation of words and text passages. In LSA, a group of terms representing an article was extracted by judging from among many contexts, and a term-document matrix was built to describe the frequency of occurrence of terms in documents. Let <em>M</em> be a term-document matrix where element (<em>x</em>, <em>y</em>) normally describes the<em> TF-IDF</em> weight of term <em>x</em> in document<em>y</em>. Then, the matrix representing the article is divided by singular value decomposition (SVD) into three matrices, including a diagonal matrix of SVD [<a href="#B27" rid="B27" class=" bibr popnode">15</a>]. Through the SVD procedure, smaller singular values can be eliminated, and the dimension of the diagonal matrix can also be reduced. The dimension of the terms included in the original matrix can be decreased through the reconstruction of SVD. Through the processes of decomposition and reconstruction, LSA is capable of acquiring the knowledge of terms expressed by the article. When the LSA is applied to calculating the similarity between texts, the vector of each text is transformed into a reduced dimensional space, while the similarity between two texts is obtained from calculating the two vectors of the reduced dimension [<a href="#B12" rid="B12" class=" bibr popnode">14</a>]. The difference between vector-based model and LSA lies in that LSA transforms terms and documents into a latent semantic space and eliminates some noise in the original vector space.</p>
             <p id="__p7">One of the standard probabilistic models of LSA is the probabilistic latent semantic analysis (PLSA), which is also known as probabilistic latent semantic indexing (PLSI) [<a href="#B19" rid="B19" class=" bibr popnode">17</a>]. PLSA uses mixture decomposition to model the cooccurrence words and documents, where the probabilities are obtained by a convex combination of the aspects. LSA and PLSA have been widely applied in information processing systems and other applications [<a href="#B4" rid="B4" class=" bibr popnode">18</a>–<a href="#B72" rid="B72" class=" bibr popnode">24</a>].</p>
             <p id="__p8">The other important study based on a corpus is the hyperspace analog to language (HAL) [<a href="#B5" rid="B5" class=" bibr popnode">25</a>]. HAL and LSA share very similar attributes: they both use concurrent vocabularies to retrieve the meaning of a term. In contrast to LSA, HAL uses a paragraph or document as a unit of the document to establish the information matrix of a term. HAL establishes a window matrix of a shared term as a basis and shifts the window width without exceeding the original definition of the window matrix. The window scans through an entire corpus, using <em>N</em> terms as the width of the term window (normally a width of 10 terms), and further forms a matrix of <em>N</em> × <em>N</em>. When the window shifts and scans the documents in the entire corpus, elements in the matrix may record the weight of each shared term (number of occurrence/frequency). A 2<em>N</em> dimensional vector of a term can be acquired by combining the lines and rows of the matrix corresponding to the term, and the similarity between two texts can be calculated by the approximate Euclidean distance. However, HAL has less satisfactory results than LSA when calculating short texts.</p>
             <p id="__p9" class="p p-last">To conclude, the aforementioned approaches calculate the similarity based on the number of shared terms in articles, instead of overlook the syntactic structure of sentences. If one applies the conventional methods to calculate the similarity between short texts/sentences directly, some disadvantages may arise.</p>
             <ol class="enumerated" style="list-style-type:decimal">
              <!--
list-behavior=enumerated
prefix-word=
mark-type=decimal
max-label-size=0
-->
              <li>
               <div>
                The conventional methods assume that a document has hundreds or thousands of dimensions, transferring the short texts/sentences into a very high dimensional space and extremely sparse vectors may lead to a less accurate calculation result.
               </div></li>
              <li>
               <div>
                Algorithms based on shared terms are suitable to be applied to the retrieval of medium and longer texts that contain more information. In contrast, information of shared terms in short texts or sentences is rare and even inaccessible. This may cause the system to generate a very low score on semantic similarity, and this result cannot be adjusted by a general smoothing function.
               </div></li>
              <li>
               <div>
                Stopwords are usually not taken into consideration in the indexing of normal IR systems. Stopwords do not have much meaning when calculating the similarity between longer texts. However, they are unavoidable parts with regard to the similarity between sentences, for that they deliver information concerning the structure of sentences, which has a certain degree of impact on explaining the meanings of sentences.
               </div></li>
              <li>
               <div>
                Similar sentences may be composed of synonyms; abundant shared terms are not necessary. Current studies evaluate similarity according to the cooccurring terms in the texts and ignore syntactic information.
               </div></li>
             </ol>
             <p>The proposed semantic similarity algorithm addresses the limitations of these existing approaches by using grammatical rules and the WordNet ontology. A set of grammar matrices is built for representing the relationships between pairs of sentences. The size of the set is limited to the maximum number of selected grammar links. The latent semantic of words is calculated via a WordNet similarity measure. The rest of this paper is organized as follows. <a href="#sec2" rid="sec2" class=" sec"> Section 2</a> introduces related technologies adopted in our algorithm. <a href="#sec3" rid="sec3" class=" sec"> Section 3</a> outlines the proposed algorithm and core functions. <a href="#sec4" rid="sec4" class=" sec"> Section 4</a> gives some examples to illustrate our method. Experimental results on two famous benchmarks are shown in <a href="#sec5" rid="sec5" class=" sec">Section 5</a>, and the final gives the conclusion.</p>
            </div>
            <div id="sec2" class="tsec sec">
             <h2 class="head no_bottom_margin" id="sec2title">2. Background</h2>
             <div id="sec2.1" class="sec sec-first">
              <h3>2.1. Ontology and the WordNet</h3>
              <p id="__p14" class="p p-first">The issue of semantic aware among texts/natural-languages is increasingly pointing towards Semantic Web technologies in general and ontology in particular as a solution. Ontology is a philosophical theory about the nature of being. Artificial intelligence researchers, especially the knowledge acquisition and representation, reincarnate the term to express “<em>a shared and common understanding of some domain that can be communicated between people and application systems</em>” [<a href="#B16" rid="B16" class=" bibr popnode">26</a>, <a href="#B17" rid="B17" class=" bibr popnode">27</a>]. A typical ontology is a taxonomy defining the classes in a specific domain and their relationships as well as a set of inference rules powering its reasoning functions [<a href="#B31" rid="B31" class=" bibr popnode">28</a>]. Ontology is now recognized in the semantic web community as a term that refers to the shared understanding of knowledge in some domains of interest [<a href="#B13" rid="B13" class=" bibr popnode">29</a>–<a href="#B23" rid="B23" class=" bibr popnode">31</a>], which is often conceived as a set of concepts, relations, functions, axioms, and instances. Guarino conducted a comprehensive survey for the definition of ontology from various highly cited works in the knowledge sharing community [<a href="#B2" rid="B2" class=" bibr popnode">32</a>–<a href="#B59" rid="B59" class=" bibr popnode">37</a>]. The semantic web is an evolving extension of the World Wide Web in which web content can be expressed in natural languages and in a form that can be understood, interpreted, and used by software agents. Elements of the semantic web are expressed in formal specifications, which include the resource description framework [<a href="#B43" rid="B43" class=" bibr popnode">38</a>], a variety of data interchange formats (such as RDF/XML, N3, Turtle, and N-Triples) [<a href="#B44" rid="B44" class=" bibr popnode">39</a>, <a href="#B66" rid="B66" class=" bibr popnode">40</a>], and notations such as web ontology language [<a href="#B40" rid="B40" class=" bibr popnode">41</a>] and the RDF schema.</p>
              <p id="__p15" class="p p-last">In recent years, the WordNet [<a href="#B62" rid="B62" class=" bibr popnode">42</a>] has become the most widely used lexical ontology of English. The WordNet was developed and has been maintained by the Cognitive Science Laboratory at Princeton University in the 1990s. Nouns, verbs, adjectives, and adverbs are grouped into cognitive synonyms called “synsets,” and each synonym expresses a distinct concept. As an ordinary online dictionary, WordNet lists subjects along with explanation alphabetically. Additionally, it also shows semantic relations among words and concepts. The latest version of WordNet is 3.0, which contains more than 150,000 words and 110,000 synsets. In WordNet, the lexicalized synsets of nouns and verbs are organized hierarchically by means of hypernym/hypernymy and hyponym/hyponymy. Hyponyms are concepts that describe things more specifically, and hypernyms refer to concepts that describe things more general. In other words, <em>x</em> is a hypernym of <em>y</em> if every <em>y</em> is a kind of <em>x</em>, and <em>y</em> is a hyponym of <em>x</em> if every <em>y</em> is a kind of <em>x</em>. For example,<em> bird</em> is a hyponymy of<em> vertebrate</em>, and<em> vertebrate</em> is a hypernym of<em> bird</em>. The concept hierarchy of WordNet has emerged as a useful framework for knowledge discovery and extraction [<a href="#B6" rid="B6" class=" bibr popnode">43</a>–<a href="#B71" rid="B71" class=" bibr popnode">49</a>]. In this research, we adopt Wu and Palmer's similarity measure [<a href="#B64" rid="B64" class=" bibr popnode">50</a>], which has become somewhat of a standard for measuring similarity between words in a lexical ontology. As shown in </p>
              <div class="disp-formula" id="EEq1">
               <div class="f">
                <math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M1" overflow="scroll">
                 <mtable>
                  <mtr>
                   <mtd>
                    <malignmark></malignmark>
                    <mtext>
                     Similarity
                    </mtext>
                    <mrow>
                     <mo>
                      (
                     </mo>
                     <msub>
                      <mrow>
                       <mi>
                        w
                       </mi>
                      </mrow>
                      <mrow>
                       <mn>
                        1
                       </mn>
                      </mrow>
                     </msub>
                     <mo>
                      ,
                     </mo>
                     <mi></mi>
                     <msub>
                      <mrow>
                       <mi>
                        w
                       </mi>
                      </mrow>
                      <mrow>
                       <mn>
                        2
                       </mn>
                      </mrow>
                     </msub>
                     <mo>
                      )
                     </mo>
                    </mrow>
                   </mtd>
                  </mtr>
                  <mtr>
                   <mtd>
                    <maligngroup></maligngroup>
                    <malignmark></malignmark>
                    <mo>
                     =
                    </mo>
                    <mfrac>
                     <mrow>
                      <mn mathvariant="normal">
                       2
                      </mn>
                      <mo>
                       ×
                      </mo>
                      <mtext>
                       depth
                      </mtext>
                      <mrow>
                       <mo>
                        (
                       </mo>
                       <msub>
                        <mrow>
                         <mi>
                          h
                         </mi>
                        </mrow>
                        <mrow>
                         <msub>
                          <mrow>
                           <mi>
                            w
                           </mi>
                          </mrow>
                          <mrow>
                           <mn mathvariant="normal">
                            1
                           </mn>
                          </mrow>
                         </msub>
                         <mo>
                          ,
                         </mo>
                         <msub>
                          <mrow>
                           <mi>
                            w
                           </mi>
                          </mrow>
                          <mrow>
                           <mn mathvariant="normal">
                            2
                           </mn>
                          </mrow>
                         </msub>
                        </mrow>
                       </msub>
                       <mo>
                        )
                       </mo>
                      </mrow>
                     </mrow>
                     <mrow>
                      <msub>
                       <mrow>
                        <mtext>
                         dep
                        </mtext>
                       </mrow>
                       <mrow>
                        <mtext>
                         pl
                        </mtext>
                       </mrow>
                      </msub>
                      <mrow>
                       <mo>
                        (
                       </mo>
                       <mrow>
                        <msub>
                         <mrow>
                          <mi>
                           w
                          </mi>
                         </mrow>
                         <mrow>
                          <mn mathvariant="normal">
                           1
                          </mn>
                         </mrow>
                        </msub>
                        <mo>
                         ,
                        </mo>
                        <msub>
                         <mrow>
                          <mi>
                           h
                          </mi>
                         </mrow>
                         <mrow>
                          <msub>
                           <mrow>
                            <mi>
                             w
                            </mi>
                           </mrow>
                           <mrow>
                            <mn mathvariant="normal">
                             1
                            </mn>
                           </mrow>
                          </msub>
                          <mo>
                           ,
                          </mo>
                          <msub>
                           <mrow>
                            <mi>
                             w
                            </mi>
                           </mrow>
                           <mrow>
                            <mn mathvariant="normal">
                             2
                            </mn>
                           </mrow>
                          </msub>
                         </mrow>
                        </msub>
                       </mrow>
                       <mo>
                        )
                       </mo>
                      </mrow>
                      <mo>
                       +
                      </mo>
                      <msub>
                       <mrow>
                        <mtext>
                         dep
                        </mtext>
                       </mrow>
                       <mrow>
                        <mtext>
                         pl
                        </mtext>
                       </mrow>
                      </msub>
                      <mrow>
                       <mo>
                        (
                       </mo>
                       <mrow>
                        <msub>
                         <mrow>
                          <mi>
                           w
                          </mi>
                         </mrow>
                         <mrow>
                          <mn mathvariant="normal">
                           2
                          </mn>
                         </mrow>
                        </msub>
                        <mo>
                         ,
                        </mo>
                        <msub>
                         <mrow>
                          <mi>
                           h
                          </mi>
                         </mrow>
                         <mrow>
                          <msub>
                           <mrow>
                            <mi>
                             w
                            </mi>
                           </mrow>
                           <mrow>
                            <mn mathvariant="normal">
                             1
                            </mn>
                           </mrow>
                          </msub>
                          <mo>
                           ,
                          </mo>
                          <msub>
                           <mrow>
                            <mi>
                             w
                            </mi>
                           </mrow>
                           <mrow>
                            <mn mathvariant="normal">
                             2
                            </mn>
                           </mrow>
                          </msub>
                         </mrow>
                        </msub>
                       </mrow>
                       <mo>
                        )
                       </mo>
                      </mrow>
                      <mo>
                       +
                      </mo>
                      <mn mathvariant="normal">
                       2
                      </mn>
                      <mo>
                       ×
                      </mo>
                      <mtext>
                       depth
                      </mtext>
                      <mrow>
                       <mo>
                        (
                       </mo>
                       <mrow>
                        <msub>
                         <mrow>
                          <mi>
                           h
                          </mi>
                         </mrow>
                         <mrow>
                          <msub>
                           <mrow>
                            <mi>
                             w
                            </mi>
                           </mrow>
                           <mrow>
                            <mn mathvariant="normal">
                             1
                            </mn>
                           </mrow>
                          </msub>
                          <mo>
                           ,
                          </mo>
                          <msub>
                           <mrow>
                            <mi>
                             w
                            </mi>
                           </mrow>
                           <mrow>
                            <mn mathvariant="normal">
                             2
                            </mn>
                           </mrow>
                          </msub>
                         </mrow>
                        </msub>
                       </mrow>
                       <mo>
                        )
                       </mo>
                      </mrow>
                     </mrow>
                    </mfrac>
                    <mo>
                     ,
                    </mo>
                   </mtd>
                  </mtr>
                 </mtable>
                </math>
               </div>
               <div class="l">
                (1)
               </div>
              </div>
              <p> where depth(<em>h</em> <sub><em>w</em><sub>1</sub>,<em>w</em><sub>2</sub></sub>) is the depth of the lowest common hypernym (<em>h</em> <sub><em>w</em><sub>1</sub>,<em>w</em><sub>2</sub></sub>) in a lexical taxonomy, dep<sub>pl</sub>(<em>w</em> <sub>1</sub>, <em>h</em> <sub><em>w</em><sub>1</sub>,<em>w</em><sub>2</sub></sub>) and dep<sub>pl</sub>(<em>w</em> <sub>2</sub>, <em>h</em> <sub><em>w</em><sub>1</sub>,<em>w</em><sub>2</sub></sub>) denote the number of hops from <em>h</em> <sub><em>w</em><sub>1</sub>,<em>w</em><sub>2</sub></sub> to <em>w</em> <sub>1</sub> and <em>w</em> <sub>2</sub>, respectively.</p>
             </div>
             <div id="sec2.2" class="sec sec-last">
              <h3>2.2. The Link Grammar</h3>
              <p id="__p16" class="p p-first"> <em>Link grammar</em> (LG) [<a href="#B55" rid="B55" class=" bibr popnode">51</a>], designed by Davy Temperley, John Lafferty, and Daniel Sleator, is a syntactic parser of English which builds relations between pairs of words. Given a sentence, LG produces a corresponding syntactic structure, which consists of a set of labeled links connecting pairs of words. The latest version of LG also produces a “constituent representation” (Penn tree-bank style phrase tree) of a sentence (noun phrases, verb phrases, etc.). The parser uses a dictionary of more than 6,000 word forms and has coverage of a wide variety of syntactic constructions. LG is now being maintained under the auspices of the Abiword project [<a href="#B1" rid="B1" class=" bibr popnode">52</a>]. The basic idea of LG is thinking of words as blocks with connectors which form the relations, or called links. These links are used not only to identify the part-of-speech of words but also to describe functions of those words in a sentence in detail. LG can explain the modification relations between different parts of speech and treats a sentence as a sequence of words and consists of a set of labeled links connecting pairs of words. All of the words in the LG dictionary have been defined to describe the way they are used in sentences, and such a system is termed a “lexical system.”</p>
              <p id="__p17">A lexical system can easily construct a large grammar structure, as changing the definition of a word only affects the grammar of the sentence that the word is in. Additionally, expressing the grammar of irregular verbs is simple as the system individually defines each one. As to the grammar of different phrase structures, links that are smooth and conform to semantic structure can be established for every word by using link grammar words to analyze the grammar of a sentence.</p>
              <p id="__p18">All produced links among words obey three basic rules [<a href="#B55" rid="B55" class=" bibr popnode">51</a>].</p>
              <ol class="enumerated" style="list-style-type:decimal">
               <!--
list-behavior=enumerated
prefix-word=
mark-type=decimal
max-label-size=0
-->
               <li>
                <div>
                 Planarity: the links do not cross to each other.
                </div></li>
               <li>
                <div>
                 Connectivity: the links suffice to connect all the words of the sequence together.
                </div></li>
               <li>
                <div>
                 Satisfaction: the links satisfy the linking requirements of each word in the sequence.
                </div></li>
              </ol>
              <p> </p>
              <p id="__p22">In the sentence “<em>Canadian officials have agreed to run a complementary threat response exercise</em>.”, for example, there are<strong> AN</strong> links connect noun-modifiers “<em>official</em>” to noun “<em>Canadian,</em>” “<em>exercise</em>” to “<em>response,</em>” and “<em>exercise</em>” to “<em>threat</em>” as shown in <a href="/pmc/articles/PMC4005080/figure/fig1/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig1" rid-ob="ob-fig1" co-legend-rid="lgnd_fig1"><span>Figure 1</span></a>. The main words are marked with “<em>.n</em>”, “<em>.v</em>”, “<em>.a</em>” to indicate nouns, verbs, and adjectives. The<strong> A</strong> link connects prenoun (attributive) adjectives to nouns. The link<strong> D</strong> connects determiners to nouns. There are many words that can act as either determiners or noun-phrases such as “<em>a</em>” (labeled as “<strong>Ds</strong>”), “<em>many</em>” (“<strong>DmC</strong>”), and “<em>some</em>” (“<strong>Dm</strong>”), and each of them is corresponding to the subtype of the linking type<strong> D</strong>. The link<strong> O </strong>connects transitive verbs to direct or indirect objects, in which<strong> Os</strong> is a subtype of<strong> O</strong> that connectors mark nouns as being singular.<strong> PP </strong>connects forms of “have” with past participles (“<em>have agreed</em>”),<strong> Sp</strong> is a subtype of<strong> S</strong> that connects plural nouns to plural verb forms (<strong>S</strong> connects subject-nouns to finite verbs), and so on.</p>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig1" co-legend-rid="lgnd_fig1">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig1/" target="figure" rid-figpopup="fig1" rid-ob="ob-fig1"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.001.gif" class="small-thumb" alt="Figure 1" title="Figure 1" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.001.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig1">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig1/" target="figure" rid-figpopup="fig1" rid-ob="ob-fig1">Figure 1</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Linkage structures produced by link grammar.</span>
                </div>
               </div>
              </div>
              <p id="__p23" class="p p-last">This simple example illustrates that the linkages imply a certain degree of semantic correlations in the sentence. LG defines more than 100 links; however, in our design, the semantic similarity is extracted from a specific designed linkage-matrix and is evaluated by the WordNet similarity measure; thus, only the connectors contain nonspecific nouns and verbs are reserved. Others links, such as<strong> AL</strong> (which connects a few determiners to following determiners, such as “<em>both the</em>” and “<em>all the</em>”) and<strong> EC</strong> (which connects adverbs and comparative adjectives, like “<em>much more</em>”), are ignored.</p>
             </div>
            </div>
            <div id="sec3" class="tsec sec">
             <h2 class="head no_bottom_margin" id="sec3title">3. The Grammatical Semantic Similarity Algorithm</h2>
             <p id="__p24" class="p p-first">This section shows the proposed grammatical similarity algorithm in detail. This algorithm can be a plug-in of normal English natural language processing systems and expert systems. Our approach obtains similarity from semantic and syntactic information contained in the compared natural language sentences. A natural language sentence is considered as a sequence of links instead of separated words and each of which contains a specific meaning. Unlike existing approaches use fixed term set of vocabulary, cooccurrence terms [<a href="#B49" rid="B49" class=" bibr popnode">1</a>–<a href="#B51" rid="B51" class=" bibr popnode">3</a>], or even word orders [<a href="#B33" rid="B33" class=" bibr popnode">8</a>], the proposed approach directly extracts the latent semantics from the same or similar links.</p>
             <div id="sec3.1" class="sec">
              <h3>3.1. Linking Types</h3>
              <p id="__p25" class="p p-first">The proposed algorithm determines the similarity of two natural language sentences from the grammar information and the semantic similarity of words that the links contain. <a href="/pmc/articles/PMC4005080/table/tab1/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab1" rid-ob="ob-tab1" co-legend-rid=""><span> Table 1</span></a> shows the selected links, subtypes of links, and the corresponding descriptions used in our approach. The first column is the selected major linking types of<em> LG</em>. The second column shows the selected subtypes of the major linking types. If all subtypes of a specific link were selected, it is denoted by “?.” The dash line identifies that there is no any subtype been selected or exists. This method is divided into three functions. The first part is the linking type extraction. Algorithm <a href="/pmc/articles/PMC4005080/figure/alg1/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg1" rid-ob="ob-alg1" co-legend-rid="lgnd_alg1"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -4.5em;">Algorithm1</span></span><span>1</span></a> accepts a sentence <em>S</em> and a set of selected linking types <em>?</em> and returns the set of remained linking types and the corresponding information of each link. This is the preprocessing phase; the elements of the returned set are structures that record the links, subtypes of links, and the nouns or verbs of each link.</p>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="alg1" co-legend-rid="lgnd_alg1">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/alg1/" target="figure" rid-figpopup="alg1" rid-ob="ob-alg1"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.alg.001.gif" class="small-thumb" alt="Algorithm 1" title="Algorithm 1" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.alg.001.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_alg1">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/alg1/" target="figure" rid-figpopup="alg1" rid-ob="ob-alg1">Algorithm 1</a>
                </div>
                <!--caption a4-->
                <div>
                 <span> <em>Linking types</em>.</span>
                </div>
               </div>
              </div>
              <!--table ft1-->
              <!--table-wrap mode=article t1-->
              <div class="table-wrap iconblock ten_col whole_rhythm clearfix" id="tab1">
               <a href="/pmc/articles/PMC4005080/table/tab1/" target="table" rid-ob="ob-tab1" rid-figpopup="tab1" class="table img_link icnblk_img figpopup"><img alt="Table 1" title="Table 1" class="small-thumb" src="/pmc/articles/PMC4005080/table/tab1/?report=thumb" src-large="/pmc/articles/PMC4005080/table/tab1/?report=previmg"></a>
               <div class="icnblk_cntnt">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/table/tab1/" target="table" rid-figpopup="tab1" rid-ob="ob-tab1">Table 1</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Selected linkages used in the algorithm; superscript <em>?</em> denotes the optional linking types. </span>
                </div>
               </div>
              </div>
              <p id="__p26">After preprocessing, <a href="/pmc/articles/PMC4005080/figure/alg2/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg2" rid-ob="ob-alg2" co-legend-rid="lgnd_alg2"><span>Algorithm 2</span></a> computes the semantic similarity score of the input sentences. The algorithm accepts two sentences and a set of selected linking types and returns the semantic similarity score, which is formalized to 0~1. In <a href="/pmc/articles/PMC4005080/figure/alg2/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg2" rid-ob="ob-alg2" co-legend-rid="lgnd_alg2"><span>Algorithm 2</span></a>, lines 1 and 2 call <a href="/pmc/articles/PMC4005080/figure/alg1/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg1" rid-ob="ob-alg1" co-legend-rid="lgnd_alg1"><span>Algorithm 1</span></a> to record the links and information of words of sentences <em>S</em> <sub><em>A</em></sub> and <em>S</em> <sub><em>B</em></sub> in the sets <em>LT</em> <sub><em>A</em></sub> and <em>LT</em> <sub><em>B</em></sub>. If <em>LT</em> <sub><em>A</em></sub>?<em>LT</em> <sub><em>B</em></sub> ? <em>?</em>, it implies that there exist some common or similar links between <em>S</em> <sub><em>A</em></sub> and <em>S</em> <sub><em>B</em></sub>, which can be regarded as the phrase correlations between the two sentences. In our design, common main links with similar subtypes will form a matrix, named<em> Grammar_Matrix</em> (<em>GM</em>). Each<em> GM</em> implies certain degree of correlations between phrases; the value of each term in<em> GM</em> is calculated by the Wu and Palmer algorithm. <a href="/pmc/articles/PMC4005080/figure/alg3/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg3" rid-ob="ob-alg3" co-legend-rid="lgnd_alg3"><span> Algorithm 3</span></a> depicts the details of the evaluation process. In <a href="/pmc/articles/PMC4005080/figure/alg3/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg3" rid-ob="ob-alg3" co-legend-rid="lgnd_alg3"><span>Algorithm 3</span></a>,<em> GM</em> was composed by the common links. Since the number of subtypes varies from each link, we set the links with less subtypes as the rows and the other as the columns. For each row <em>i</em>, the maximal term was reserved and forms a<em> Grammar_Vector</em> (<em>GV</em>), which represents the maximal semantic inclusion of a specific link between <em>S</em> <sub><em>A</em></sub> and <em>S</em> <sub><em>B</em></sub>.</p>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="alg2" co-legend-rid="lgnd_alg2">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/alg2/" target="figure" rid-figpopup="alg2" rid-ob="ob-alg2"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.alg.002.gif" class="small-thumb" alt="Algorithm 2" title="Algorithm 2" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.alg.002.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_alg2">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/alg2/" target="figure" rid-figpopup="alg2" rid-ob="ob-alg2">Algorithm 2</a>
                </div>
                <!--caption a4-->
                <div>
                 <span> <em>Semantic sentence similarity</em>.</span>
                </div>
               </div>
              </div>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="alg3" co-legend-rid="lgnd_alg3">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/alg3/" target="figure" rid-figpopup="alg3" rid-ob="ob-alg3"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.alg.003.gif" class="small-thumb" alt="Algorithm 3" title="Algorithm 3" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.alg.003.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_alg3">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/alg3/" target="figure" rid-figpopup="alg3" rid-ob="ob-alg3">Algorithm 3</a>
                </div>
                <!--caption a4-->
                <div>
                 <span> <em>Grammarmatrix</em>.</span>
                </div>
               </div>
              </div>
              <p id="__p27" class="p p-last"> <a href="/pmc/articles/PMC4005080/figure/fig2/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig2" rid-ob="ob-fig2" co-legend-rid="lgnd_fig2"><span>Figure 2</span></a> illustrates the structure of<em> GMs</em> and<em> G</em> versus <em>S</em> <sub><em>A</em></sub> and <em>S</em> <sub><em>B</em></sub> are compared sentences, <em>S</em> <sub><em>A</em>_1</sub> and <em>S</em> <sub><em>B</em>_1</sub> are the first common link and <em>l</em> <sub>1_1</sub>, <em>l</em> <sub>1_2</sub>, and so forth, are the subtypes of <em>S</em> <sub><em>A</em>_1</sub> and <em>S</em> <sub><em>B</em>_1</sub>. Each<em> GM</em> represents a correlation of certain phrases since there may exist several similar sublinks in a sentence, in which the corresponding<em> GV</em> quantifies the information and extracts latent semantics between these phrases. <a href="/pmc/articles/PMC4005080/figure/alg1/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg1" rid-ob="ob-alg1" co-legend-rid="lgnd_alg1"><span>Algorithm 1</span></a> invokes the<em> LG</em> function and generates linkages as shown in Figures <a href="/pmc/articles/PMC4005080/figure/fig3/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig3" rid-ob="ob-fig3" co-legend-rid="lgnd_fig3"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -3.5em;">Figures3,</span></span><span>3</span></a>, <a href="/pmc/articles/PMC4005080/figure/fig4/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig4" rid-ob="ob-fig4" co-legend-rid="lgnd_fig4"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -0.5em;">,4,</span></span><span>4</span></a>, and <a href="/pmc/articles/PMC4005080/figure/fig5/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig5" rid-ob="ob-fig5" co-legend-rid="lgnd_fig5"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -1.5em;">and5</span></span><span>5</span></a>.</p>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig2" co-legend-rid="lgnd_fig2">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig2/" target="figure" rid-figpopup="fig2" rid-ob="ob-fig2"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.002.gif" class="small-thumb" alt="Figure 2" title="Figure 2" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.002.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig2">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig2/" target="figure" rid-figpopup="fig2" rid-ob="ob-fig2">Figure 2</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Diagram of grammar matrices and grammar vectors.</span>
                </div>
               </div>
              </div>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig3" co-legend-rid="lgnd_fig3">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig3/" target="figure" rid-figpopup="fig3" rid-ob="ob-fig3"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.003.gif" class="small-thumb" alt="Figure 3" title="Figure 3" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.003.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig3">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig3/" target="figure" rid-figpopup="fig3" rid-ob="ob-fig3">Figure 3</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Linkages of sentence<em> A</em>.</span>
                </div>
               </div>
              </div>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig4" co-legend-rid="lgnd_fig4">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig4/" target="figure" rid-figpopup="fig4" rid-ob="ob-fig4"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.004.gif" class="small-thumb" alt="Figure 4" title="Figure 4" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.004.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig4">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig4/" target="figure" rid-figpopup="fig4" rid-ob="ob-fig4">Figure 4</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Linkages of sentence<em> B</em>.</span>
                </div>
               </div>
              </div>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig5" co-legend-rid="lgnd_fig5">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig5/" target="figure" rid-figpopup="fig5" rid-ob="ob-fig5"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.005.gif" class="small-thumb" alt="Figure 5" title="Figure 5" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.005.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig5">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig5/" target="figure" rid-figpopup="fig5" rid-ob="ob-fig5">Figure 5</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Linkages of sentence<em> C</em>.</span>
                </div>
               </div>
              </div>
             </div>
             <div id="sec3.2" class="sec sec-last">
              <h3>3.2. A Work through Example</h3>
              <p id="__p28" class="p p-first-last">This section gives an example to demonstrate the proposed similarity algorithm. Let<em> A</em> = “<em>Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier</em>.”,<em> B</em> = “<em>With the scandal hanging over Stewart's company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier</em>.”, and<em> C</em> = “<em>The result is an overall package that will provide significant economic growth for our employees over the next four years</em>.” This example is from the Microsoft Research Paraphrase Corpus (MRPC) [<a href="#B42" rid="B42" class=" bibr popnode">53</a>], which will be introduced in more details in the following section. In this example we compare the semantic similarities between<em> A-B</em>,<em> A-C</em>, and<em> B-C</em>. <a href="/pmc/articles/PMC4005080/figure/alg1/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg1" rid-ob="ob-alg1" co-legend-rid="lgnd_alg1"><span>Algorithm 1</span></a> first generates the corresponding linkages for each sentence and the results are shown in Figures <a href="/pmc/articles/PMC4005080/figure/fig3/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig3" rid-ob="ob-fig3" co-legend-rid="lgnd_fig3"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -3.5em;">Figures3</span></span><span>3</span></a>–<a href="/pmc/articles/PMC4005080/figure/fig5/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig5" rid-ob="ob-fig5" co-legend-rid="lgnd_fig5"><span>5</span></a>. There are totally 17, 26, and 20 original linkages generated by<em> LG</em>. After the preprocessing step, the remaining linkages are (the detailed data structure is omitted here) <em>LT</em> <sub><em>A</em></sub> = {<em>Wd</em>, <em>S</em>(<em>Ss</em>), <em>Mp</em>, <em>D</em>(<em>Ds</em>), <em>J</em>(<em>Js</em>)}, <em>LT</em> <sub><em>B</em></sub> = {<em>Wd</em>, <em>S</em>(<em>Ss</em>), <em>MP</em>, <em>J</em>(<em>Jp</em>, <em>Js</em>), <em>D</em>(<em>Ds</em>, <em>D</em>?<em>u</em>)}, and <em>LT</em> <sub><em>C</em></sub> = {<em>Wd</em>, <em>D</em>(<em>Ds</em>), <em>DD</em>, <em>J</em>(<em>Jp</em>)}, respectively. In <a href="/pmc/articles/PMC4005080/figure/alg2/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg2" rid-ob="ob-alg2" co-legend-rid="lgnd_alg2"><span>Algorithm 2</span></a>, the compared sentence pair was sent to the<em> Grammar matrix</em> (i.e., <a href="/pmc/articles/PMC4005080/figure/alg3/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg3" rid-ob="ob-alg3" co-legend-rid="lgnd_alg3"><span>Algorithm 3</span></a>) according to their common linking types, and each linking type with their subtypes forms a<em> Grammar_Matrix</em>. Tables <a href="/pmc/articles/PMC4005080/table/tab2/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab2" rid-ob="ob-tab2" co-legend-rid=""><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -3em;">Tables2,</span></span><span>2</span></a>, <a href="/pmc/articles/PMC4005080/table/tab3/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab3" rid-ob="ob-tab3" co-legend-rid=""><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -0.5em;">,3,</span></span><span>3</span></a>, and <a href="/pmc/articles/PMC4005080/table/tab4/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab4" rid-ob="ob-tab4" co-legend-rid=""><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -1.5em;">and4</span></span><span>4</span></a> show the<em> GMs</em> and their word-to-word similarities of pairs<em> A-B</em>,<em> A-C</em>, and<em> B-C</em>. In <a href="/pmc/articles/PMC4005080/table/tab2/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab2" rid-ob="ob-tab2" co-legend-rid=""><span>Table 2</span></a>, the linking types of <em>LT</em> <sub><em>A</em></sub>?<em>LT</em> <sub><em>B</em></sub> are<em> Wd</em>,<em> S</em>,<em> Mp</em>,<em> D</em>, and<em> J</em>; therefore, there are five<em> GMs</em> in pair<em> A-B</em>. The first<em> GM</em> is a 1 × 1 matrix with row = {<em>Wd</em>} and column = {<em>Wd</em>}, the second<em> GM</em> is also a 1 × 1 matrix with row = {<em>Ss</em>} and column = {<em>Ss</em>}, the third<em> GM</em> is a 3 × 1 matrix with row = {<em>Mp</em>} and column = {<em>Mp</em>, <em>Mp</em>, <em>Mp</em>}, the fourth<em> GM</em> is a 3 × 4 matrix with row = {<em>Js</em>, <em>Js</em>, <em>Js</em>} and column = {<em>Jp</em>, <em>Jp</em>, <em>Js</em>, <em>Js</em>}, and so on. In step 5 of <a href="/pmc/articles/PMC4005080/figure/alg3/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="alg3" rid-ob="ob-alg3" co-legend-rid="lgnd_alg3"><span>Algorithm 3</span></a>, we evaluate the single word similarity via the<em> WordNet</em> ontology and the<em> Wu&amp;Palmer</em> method. The results are also shown in Tables <a href="/pmc/articles/PMC4005080/table/tab2/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab2" rid-ob="ob-tab2" co-legend-rid=""><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -3em;">Tables2</span></span><span>2</span></a>–<a href="/pmc/articles/PMC4005080/table/tab4/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab4" rid-ob="ob-tab4" co-legend-rid=""><span>4</span></a>. This phase evaluates all possible semantics between similar links, and obviously a word may be linked twice or even more in the general case. The next phase reduces each<em> GM</em> to a<em> Grammar_Vector </em>(<em>GV</em>) by reserving the maximal value of each row. Thus in the pair<em> A-B</em>, <em>GV</em> <sub><em>Wd</em></sub> = [1], <em>GV</em> <sub><em>S</em></sub> = [1], <em>GV</em> <sub><em>Mp</em></sub> = [1], <em>GV</em> <sub><em>J</em></sub> = [0.91, 1, 0.91], and <em>GV</em> <sub><em>D</em></sub> = [0.91, 0.91]. In the pair<em> A-C</em>, <em>GV</em> <sub><em>Wd</em></sub> = [0.31], <em>GV</em> <sub><em>J</em></sub> = [0.73], <em>GV</em> <sub><em>Wd</em></sub> = [0.4, 0.55], and <em>GV</em> <sub><em>Wd</em></sub> = [0.31], <em>GV</em> <sub><em>J</em></sub> = [0.22, 0.91], and <em>GV</em> <sub><em>D</em></sub> = [0.71, 0.55] in the pair<em> B-C</em>. In the final stage, all elements of GVs are taken the number of the elements' power for balancing the effects of nonevaluated subtypes. The final scores of<em> A</em> versus<em> B</em> = 0.987,<em> A</em> versus<em> C</em> = 0.817, and<em> B</em> versus<em> C</em> = 0.651, respectively.</p>
              <!--table ft1-->
              <!--table-wrap mode=article t1-->
              <div class="table-wrap iconblock ten_col whole_rhythm clearfix" id="tab2">
               <a href="/pmc/articles/PMC4005080/table/tab2/" target="table" rid-ob="ob-tab2" rid-figpopup="tab2" class="table img_link icnblk_img figpopup"><img alt="Table 2" title="Table 2" class="small-thumb" src="/pmc/articles/PMC4005080/table/tab2/?report=thumb" src-large="/pmc/articles/PMC4005080/table/tab2/?report=previmg"></a>
               <div class="icnblk_cntnt">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/table/tab2/" target="table" rid-figpopup="tab2" rid-ob="ob-tab2">Table 2</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>The <em>GMs</em> of sentence <em>A</em> versus sentence <em>B</em>.</span>
                </div>
               </div>
              </div>
              <!--table ft1-->
              <!--table-wrap mode=article t1-->
              <div class="table-wrap iconblock ten_col whole_rhythm clearfix" id="tab3">
               <a href="/pmc/articles/PMC4005080/table/tab3/" target="table" rid-ob="ob-tab3" rid-figpopup="tab3" class="table img_link icnblk_img figpopup"><img alt="Table 3" title="Table 3" class="small-thumb" src="/pmc/articles/PMC4005080/table/tab3/?report=thumb" src-large="/pmc/articles/PMC4005080/table/tab3/?report=previmg"></a>
               <div class="icnblk_cntnt">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/table/tab3/" target="table" rid-figpopup="tab3" rid-ob="ob-tab3">Table 3</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>The <em>GMs</em> of sentence <em>A</em> versus sentence <em>C</em>.</span>
                </div>
               </div>
              </div>
              <!--table ft1-->
              <!--table-wrap mode=article t1-->
              <div class="table-wrap iconblock ten_col whole_rhythm clearfix" id="tab4">
               <a href="/pmc/articles/PMC4005080/table/tab4/" target="table" rid-ob="ob-tab4" rid-figpopup="tab4" class="table img_link icnblk_img figpopup"><img alt="Table 4" title="Table 4" class="small-thumb" src="/pmc/articles/PMC4005080/table/tab4/?report=thumb" src-large="/pmc/articles/PMC4005080/table/tab4/?report=previmg"></a>
               <div class="icnblk_cntnt">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/table/tab4/" target="table" rid-figpopup="tab4" rid-ob="ob-tab4">Table 4</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>The <em>GMs</em> of sentence <em>B </em>versus sentence <em>C</em>.</span>
                </div>
               </div>
              </div>
             </div>
            </div>
            <div id="sec4" class="tsec sec">
             <h2 class="head no_bottom_margin" id="sec4title">4. Experiments</h2>
             <div id="sec4.1" class="sec sec-first">
              <h3>4.1. Experiment with Li's Benchmark</h3>
              <p id="__p29" class="p p-first-last">Based on the notion of semantic and syntactic information contributed to the understanding of natural language sentences, Li et al. [<a href="#B33" rid="B33" class=" bibr popnode">8</a>] defined a sentence similarity measure as a linear combination that based on the similarity of semantic vector and word order. A preliminary data set was constructed by Li et al. with human similarity scores provided by 32 volunteers who are all native speakers of English. Li's dataset used 65 word pairs which were originally provided by Rubenstein and Goodenough [<a href="#B47" rid="B47" class=" bibr popnode">60</a>] and were replaced with the definitions from the Collins Cobuild dictionary [<a href="#B54" rid="B54" class=" bibr popnode">61</a>]. The Collins Cobuild dictionary was constructed by a large corpus that contains more than 400 million words. Each pair was rated on the scale of 0.0 to 4.0 according to their similarity of meaning. We used a subset of the 65 pairs to obtain a more even distribution across the similarity range. This subset contains 30 pairs from the original 65 pairs, in which 10 pairs were taken from the range 3~4, 10 pairs from the range 1~3, and 10 pairs from the low level 0~1. We list the full Li's dataset in <a href="/pmc/articles/PMC4005080/table/tab7/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab7" rid-ob="ob-tab7" co-legend-rid=""><span>Table 7</span></a>.<a href="/pmc/articles/PMC4005080/table/tab5/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab5" rid-ob="ob-tab5" co-legend-rid=""><span>Table 5</span></a> shows human similarity scores along with Li et al. [<a href="#B33" rid="B33" class=" bibr popnode">8</a>], an LSA based approach described by O'Shea et al. [<a href="#B38" rid="B38" class=" bibr popnode">54</a>], STS Meth. proposed by Islam and Inkpen [<a href="#B20" rid="B20" class=" bibr popnode">55</a>], SyMSS, a syntax-based measure proposed by Oliva et al. [<a href="#B39" rid="B39" class=" bibr popnode">56</a>], Omiotis proposed by Tsatsaronis et al. [<a href="#B57" rid="B57" class=" bibr popnode">57</a>], and our grammar-based semantic measure. The results indicate that our grammar-based approach achieves a better performance in low and medium similarity sentence pairs (levels 0~1 and 1~3). The average deviation from human judgments in level 0~1 is 0.2, which is better than the most approaches. (Li et al. avg. = 0.356, LSA avg. = 0.496, and SyMSS avg. = 0.266). The average deviation in level 1~3 is 0.208, which is also better than Li et al. and LSA. The result shows that our grammar-based semantic similarity measure achieved a reasonably good performance and the observation is that our approach tries to identify and quantify the potential semantic relation among syntaxes and words, although the common words of the compared sentence pairs are few or even none.</p>
              <!--table ft1-->
              <!--table-wrap mode=article t1-->
              <div class="table-wrap iconblock ten_col whole_rhythm clearfix" id="tab5">
               <a href="/pmc/articles/PMC4005080/table/tab5/" target="table" rid-ob="ob-tab5" rid-figpopup="tab5" class="table img_link icnblk_img figpopup"><img alt="Table 5" title="Table 5" class="small-thumb" src="/pmc/articles/PMC4005080/table/tab5/?report=thumb" src-large="/pmc/articles/PMC4005080/table/tab5/?report=previmg"></a>
               <div class="icnblk_cntnt">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/table/tab5/" target="table" rid-figpopup="tab5" rid-ob="ob-tab5">Table 5</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Benchmark number and the results compared with Li et al. [<a href="#B33" rid="B33" class=" bibr popnode">8</a>], LSA [<a href="#B38" rid="B38" class=" bibr popnode">54</a>], STS Meth. [<a href="#B20" rid="B20" class=" bibr popnode">55</a>], SyMSS [<a href="#B39" rid="B39" class=" bibr popnode">56</a>], Omiotis [<a href="#B57" rid="B57" class=" bibr popnode">57</a>], and our grammar-based approach.</span>
                </div>
               </div>
              </div>
              <!--table ft1-->
              <!--table-wrap mode=article t1-->
              <div class="table-wrap iconblock ten_col whole_rhythm clearfix" id="tab7">
               <a href="/pmc/articles/PMC4005080/table/tab7/" target="table" rid-ob="ob-tab7" rid-figpopup="tab7" class="table img_link icnblk_img figpopup"><img alt="Table 7" title="Table 7" class="small-thumb" src="/pmc/articles/PMC4005080/table/tab7/?report=thumb" src-large="/pmc/articles/PMC4005080/table/tab7/?report=previmg"></a>
               <div class="icnblk_cntnt">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/table/tab7/" target="table" rid-figpopup="tab7" rid-ob="ob-tab7">Table 7</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Li's Dataset [<a href="#B33" rid="B33" class=" bibr popnode">8</a>].</span>
                </div>
               </div>
              </div>
             </div>
             <div id="sec4.2" class="sec sec-last">
              <h3>4.2. Experiment with Microsoft Research Paraphrase Corpus</h3>
              <p id="__p30" class="p p-first">In order to further evaluate the performance of the proposed grammar-based approach with a larger dataset, we use the Microsoft Research Paraphrase Corpus [<a href="#B42" rid="B42" class=" bibr popnode">53</a>]. This dataset consists of 5801 pairs of sentences, including 4076 training pairs and 1725 test pairs collected from thousands of news sources on the web over 18 months. Each pair was examined by 2 human judges to determine whether the two sentences in a pair were semantically equivalent paraphrases or not. The interjudge agreement between annotators is approximately 83%. In this experiment, we use different similarity thresholds ranging from 0 to 1 with an interval 0.1 to determine whether a sentence pair is a paraphrase or not. For this task, we computed the proposed approach between the sentences of each pair in the training and test sets and marked as paraphrases only those pairs with similarity value greater than the given threshold. This paper compares the performance of the proposed grammar-based approach against several categories: (1) two baseline methods, a random selection approach that marks each pair as paraphrase randomly, and a traditional VSM-cosine based similarity measure with TF-IDF weighting; (2) corpus-based approaches, the PMI-IR, proposed by Turney at 2001 [<a href="#B58" rid="B58" class=" bibr popnode">62</a>], the LSA [<a href="#B38" rid="B38" class=" bibr popnode">54</a>], STS Meth. [<a href="#B20" rid="B20" class=" bibr popnode">55</a>], SyMSS (with two variations: SyMSS_JCN and SyMSS_Vector) [<a href="#B39" rid="B39" class=" bibr popnode">56</a>], and Omiotis [<a href="#B57" rid="B57" class=" bibr popnode">57</a>]; and (3) lexicon-based approaches, including Jiang and Conrath (JC) at 1997 [<a href="#B22" rid="B22" class=" bibr popnode">63</a>], Leacock et al. (LC) at 1998 [<a href="#B29" rid="B29" class=" bibr popnode">64</a>], Lin (L) at 1998 [<a href="#B34" rid="B34" class=" bibr popnode">65</a>], Resnik (R) [<a href="#B45" rid="B45" class=" bibr popnode">66</a>, <a href="#B46" rid="B46" class=" bibr popnode">67</a>], Lesk (Lesk) [<a href="#B32" rid="B32" class=" bibr popnode">68</a>], Wu and Palmer (W&amp;P) [<a href="#B64" rid="B64" class=" bibr popnode">50</a>], and Mihalcea et al. (M) at 2006 [<a href="#B37" rid="B37" class=" bibr popnode">69</a>], and (4) machine-learning based approaches, including Wan et al. at 2006 (Wan et al.) [<a href="#B61" rid="B61" class=" bibr popnode">58</a>], Zhang and Patrick at 2005 (Z&amp;P) [<a href="#B70" rid="B70" class=" bibr popnode">70</a>], and Qiu et al. at 2006 (Qiu et al.) [<a href="#B41" rid="B41" class=" bibr popnode">59</a>], which is a SVM [<a href="#B60" rid="B60" class=" bibr popnode">71</a>] based approach.</p>
              <p id="__p31">The results of the evaluation are shown in <a href="/pmc/articles/PMC4005080/table/tab6/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab6" rid-ob="ob-tab6" co-legend-rid=""><span>Table 6</span></a>. The effectiveness of an information retrieval system is usually measured by two quantities and one combined measure, named “recall” and “precision” rate. In this paper, we evaluate the results in terms of accuracy, and the corresponding precision, recall, and <em>F</em>-measure are also shown in <a href="/pmc/articles/PMC4005080/table/tab6/" target="true" class="fig-table-link table figpopup" rid-figpopup="tab6" rid-ob="ob-tab6" co-legend-rid=""><span>Table 6</span></a>. The performance measures are defined as follows: </p>
              <div class="disp-formula" id="eq2">
               <div class="f">
                <math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="M2" overflow="scroll">
                 <mtable>
                  <mtr>
                   <mtd>
                    <mtext>
                     Precision
                    </mtext>
                    <mo>
                     =
                    </mo>
                    <mfrac>
                     <mrow>
                      <mtext>
                       TP
                      </mtext>
                     </mrow>
                     <mrow>
                      <mtext>
                       TP
                      </mtext>
                      <mo>
                       +
                      </mo>
                      <mtext>
                       FP
                      </mtext>
                     </mrow>
                    </mfrac>
                    <mo>
                     ,
                    </mo>
                   </mtd>
                  </mtr>
                  <mtr>
                   <mtd>
                    <mtext>
                     Recall
                    </mtext>
                    <mo>
                     =
                    </mo>
                    <mfrac>
                     <mrow>
                      <mtext>
                       TP
                      </mtext>
                     </mrow>
                     <mrow>
                      <mtext>
                       TP
                      </mtext>
                      <mo>
                       +
                      </mo>
                      <mtext>
                       FN
                      </mtext>
                     </mrow>
                    </mfrac>
                    <mo>
                     ,
                    </mo>
                   </mtd>
                  </mtr>
                  <mtr>
                   <mtd>
                    <mtext>
                     Accuracy
                    </mtext>
                    <mo>
                     =
                    </mo>
                    <mfrac>
                     <mrow>
                      <mtext>
                       TP
                      </mtext>
                      <mo>
                       +
                      </mo>
                      <mtext>
                       TN
                      </mtext>
                     </mrow>
                     <mrow>
                      <mtext>
                       TP
                      </mtext>
                      <mo>
                       +
                      </mo>
                      <mtext>
                       FP
                      </mtext>
                      <mo>
                       +
                      </mo>
                      <mtext>
                       TN
                      </mtext>
                      <mo>
                       +
                      </mo>
                      <mtext>
                       FN
                      </mtext>
                     </mrow>
                    </mfrac>
                    <mo>
                     ,
                    </mo>
                   </mtd>
                  </mtr>
                  <mtr>
                   <mtd>
                    <mi>
                     F
                    </mi>
                    <mtext>
                     -Measure
                    </mtext>
                    <mo>
                     =
                    </mo>
                    <mfrac>
                     <mrow>
                      <mn mathvariant="normal">
                       2
                      </mn>
                      <mo>
                       ×
                      </mo>
                      <mtext>
                       Recall
                      </mtext>
                      <mo>
                       ×
                      </mo>
                      <mtext>
                       Precision
                      </mtext>
                     </mrow>
                     <mrow>
                      <mtext>
                       Recall
                      </mtext>
                      <mo>
                       +
                      </mo>
                      <mtext>
                       Precision
                      </mtext>
                     </mrow>
                    </mfrac>
                    <mo>
                     .
                    </mo>
                   </mtd>
                  </mtr>
                 </mtable>
                </math>
               </div>
               <div class="l">
                (2)
               </div>
              </div>
              <p> TP, TN, FP, and FN stand for true positive (the number of pairs correctly labeled as paraphrases), true negative (the number of pairs correctly labeled as nonparaphrases), false positive (the number of pairs incorrectly labeled as paraphrases), and false negative (the number of pairs incorrectly labeled as nonparaphrases), respectively. Recall in this experiment is defined as the number of true positives divided by the total number of pairs that actually belong to the positive class, precision is the number of true positives divided by the total number of pairs labeled as belonging to the positive class, accuracy is the number of true results (true positive + true negative) divided by the number of all pairs, and <em>F</em>-measure is the geometric mean of recall and precision. After evaluation, the best similarity threshold of accuracy is 0.6. The results indicate that the grammar-based approach surpasses all baselines, lexicon-based, and most of the corpus-based approaches in terms of accuracy and <em>F</em>-measure. We must mention that the results of each approach listed above were based on the best accuracy through all thresholds instead of under the same similarity threshold. STS Meth. [<a href="#B20" rid="B20" class=" bibr popnode">55</a>] achieved the best accuracy 72.64 with similarity threshold 0.6, SyMSS_JCN and SyMSS_Vector were two variants of SyMSS [<a href="#B39" rid="B39" class=" bibr popnode">56</a>] who accomplished the best performance in similarity threshold 0.45, and moreover, the best similarity thresholds of Omiotis [<a href="#B57" rid="B57" class=" bibr popnode">57</a>], Mihalcea et al. [<a href="#B37" rid="B37" class=" bibr popnode">69</a>], random selection, and VSM-cosine based similarity measures were 0.2, 0.5, 0.5, and 0.5, respectively. In all lexicon and corpus-based approaches, STS Meth. Reference [<a href="#B20" rid="B20" class=" bibr popnode">55</a>] earns the best similarity score 72.64 and the similarity threshold 0.6 is also reasonable, besides only the STS Meth. Reference [<a href="#B20" rid="B20" class=" bibr popnode">55</a>] has provided detailed recall, precision, accuracy, and <em>F</em>-measured values with various thresholds. The following compares our grammar-based approach with STS Meth. [<a href="#B20" rid="B20" class=" bibr popnode">55</a>] in thresholds 0~1. <a href="/pmc/articles/PMC4005080/figure/fig6/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig6" rid-ob="ob-fig6" co-legend-rid="lgnd_fig6"><span> Figure 6</span></a> shows the precision versus similarity threshold curves of STS Meth. and grammar-based method for eleven different similarity thresholds. Figures <a href="/pmc/articles/PMC4005080/figure/fig7/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig7" rid-ob="ob-fig7" co-legend-rid="lgnd_fig7"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -3.5em;">Figures7,</span></span><span>7</span></a>, <a href="/pmc/articles/PMC4005080/figure/fig8/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig8" rid-ob="ob-fig8" co-legend-rid="lgnd_fig8"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -0.5em;">,8,</span></span><span>8</span></a>, and <a href="/pmc/articles/PMC4005080/figure/fig9/" target="figure" class="fig-table-link fig figpopup" rid-figpopup="fig9" rid-ob="ob-fig9" co-legend-rid="lgnd_fig9"><span style="position: relative;text-decoration:none;">?<span class="figpopup-sensitive-area" style="left: -1.5em;">and9</span></span><span>9</span></a> depict the recall, accuracy, and <em>F</em>-measure versus similarity threshold curves of STS Meth. and grammar-based method, respectively.</p>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig6" co-legend-rid="lgnd_fig6">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig6/" target="figure" rid-figpopup="fig6" rid-ob="ob-fig6"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.006.gif" class="small-thumb" alt="Figure 6" title="Figure 6" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.006.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig6">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig6/" target="figure" rid-figpopup="fig6" rid-ob="ob-fig6">Figure 6</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Precision versus similarity threshold curves of STS and LG for eleven different similarity thresholds.</span>
                </div>
               </div>
              </div>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig7" co-legend-rid="lgnd_fig7">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig7/" target="figure" rid-figpopup="fig7" rid-ob="ob-fig7"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.007.gif" class="small-thumb" alt="Figure 7" title="Figure 7" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.007.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig7">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig7/" target="figure" rid-figpopup="fig7" rid-ob="ob-fig7">Figure 7</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Recall versus similarity threshold curves of STS and LG for eleven different similarity thresholds.</span>
                </div>
               </div>
              </div>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig8" co-legend-rid="lgnd_fig8">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig8/" target="figure" rid-figpopup="fig8" rid-ob="ob-fig8"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.008.gif" class="small-thumb" alt="Figure 8" title="Figure 8" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.008.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig8">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig8/" target="figure" rid-figpopup="fig8" rid-ob="ob-fig8">Figure 8</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Accuracy versus similarity threshold curves of STS and LG for eleven different similarity thresholds.</span>
                </div>
               </div>
              </div>
              <!--fig ft0-->
              <!--fig mode=article f1-->
              <div class="fig iconblock ten_col whole_rhythm clearfix" id="fig9" co-legend-rid="lgnd_fig9">
               <a class="icnblk_img figpopup" href="/pmc/articles/PMC4005080/figure/fig9/" target="figure" rid-figpopup="fig9" rid-ob="ob-fig9"><img src="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.009.gif" class="small-thumb" alt="Figure 9" title="Figure 9" src-large="/pmc/articles/PMC4005080/bin/TSWJ2014-437162.009.jpg"></a>
               <div class="icnblk_cntnt" id="lgnd_fig9">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/figure/fig9/" target="figure" rid-figpopup="fig9" rid-ob="ob-fig9">Figure 9</a>
                </div>
                <!--caption a4-->
                <div>
                 <span> <em>F</em>-measure versus similarity threshold curves of STS and LG for eleven different similarity thresholds.</span>
                </div>
               </div>
              </div>
              <!--table ft1-->
              <!--table-wrap mode=article t1-->
              <div class="table-wrap iconblock ten_col whole_rhythm clearfix" id="tab6">
               <a href="/pmc/articles/PMC4005080/table/tab6/" target="table" rid-ob="ob-tab6" rid-figpopup="tab6" class="table img_link icnblk_img figpopup"><img alt="Table 6" title="Table 6" class="small-thumb" src="/pmc/articles/PMC4005080/table/tab6/?report=thumb" src-large="/pmc/articles/PMC4005080/table/tab6/?report=previmg"></a>
               <div class="icnblk_cntnt">
                <div>
                 <a class="figpopup" href="/pmc/articles/PMC4005080/table/tab6/" target="table" rid-figpopup="tab6" rid-ob="ob-tab6">Table 6</a>
                </div>
                <!--caption a4-->
                <div>
                 <span>Results of the grammar-based and competitive methods on the Microsoft Research Paraphrase Corpus.</span>
                </div>
               </div>
              </div>
              <p id="__p32">As acknowledged by Islam and Inkpen [<a href="#B20" rid="B20" class=" bibr popnode">55</a>] and Corley and Mihalcea [<a href="#B9" rid="B9" class=" bibr popnode">72</a>], semantic similarity measure for short texts/sentences is a necessary step in the paraphrase recognition task, but not always sufficient. In the Microsoft Research Paraphrase Corpus, sentence pairs judged to be nonparaphrases may still overlap significantly in information content and even wording. For example, the Microsoft Research Paraphrase Corpus contains the following sentence pairs.</p>
              <p id="__p33"> </p>
              <div class="statement" id="ex1">
               <p></p>
               <h4 class="inline">Example 1 —</h4>
               <p id="__p34">(<em>1)</em> “<em>Passed in 1999 but never put into effect, the law would have made it illegal for bar and restaurant patrons to light up</em>.”</p>
               <p id="__p35">(<em>2)</em> “<em>Passed in 1999 but never put into effect, the smoking law would have prevented bar and restaurant patrons from lighting up, but exempted private clubs from the regulation</em>.”</p>
              </div>
              <p> </p>
              <p id="__p36"> </p>
              <div class="statement" id="ex2">
               <p></p>
               <h4 class="inline">Example 2 —</h4>
               <p id="__p37">(<em>1)</em> “<em>Though that slower spending made 2003 look better, many of the expenditures actually will occur in 2004</em>.”</p>
               <p id="__p38">(<em>2)</em> “<em>Though that slower spending made 2003 look better, many of the expenditures will actually occur in 2004, making that year's shortfall worse</em>.”</p>
              </div>
              <p> </p>
              <p id="__p39" class="p p-last">Sentences in each pair are highly related to each other with common words and syntaxes, however, they are not considered as paraphrases and are labeled as 0 in the corpus (paraphrases are labeled as 1). For this reason, we believe that the numbers of false positive (FP) and true negative (TN) are not entirely correct and may affect the correctness of precision, <em>F</em>-measure but accuracy and recall. The result shows that the proposed grammar-based approach outperforms the result by Islam and Inkpen [<a href="#B20" rid="B20" class=" bibr popnode">55</a>] with thresholds 0.6~1.0 (0.91 versus 0.89 and 0.88 versus 0.68 of recall with thresholds 0.6 and 0.7; 0.71 versus 0.72, 0.70 versus 0.68, and 0.59 versus 0.57 of accuracy in thresholds 0.6, 0.7, and 0.8, resp.), which is a reasonable range in determining whether a sentence pair is a paraphrase or not.</p>
             </div>
            </div>
            <div id="sec5" class="tsec sec">
             <h2 class="head no_bottom_margin" id="sec5title">5. Conclusions</h2>
             <p id="__p40" class="p p-first-last">This paper presents a grammar and semantic corpus based similarity algorithm for natural language sentences. Traditional IR technologies may not always determine the perfect matching without obvious relation or concept overlap between two natural language sentences. Some approaches deal with this problem via determining the order of words and the evaluation of semantic vectors; however, they were hard to be applied to compare the sentences with complex syntax as well as long sentences and sentences with arbitrary patterns and grammars. The proposed approach takes advantage of corpus-based ontology and grammatical rules to overcome this problem. The contributions of this work can be summarized as follows: (1) to the best of our knowledge, the proposed algorithm is the first measure of semantic similarity between sentences that integrates the word-to-word evaluation to grammatical rules, (2) the specific designed<em> Grammar_Matrix</em> will quantify the correlations between phrases instead of considering common words or word order, and (3) the use of semantic trees offered by WordNet increases the chances of finding a semantic relation between any nouns and verbs, and (4) the results demonstrate that the proposed method performed very well both in the sentences similarity and the task of paraphrase recognition. Our approach achieves a good average deviation for 30 sentence pairs and outperforms the results obtained by Li et al. [<a href="#B33" rid="B33" class=" bibr popnode">8</a>] and LSA [<a href="#B38" rid="B38" class=" bibr popnode">54</a>]. For the paraphrase recognition task, our grammar-based method surpasses most of the existing approaches and limits the best performance in a reasonable range of thresholds.</p>
            </div>
            <div id="__sec1" class="tsec bk-sec">
             <h2 class="head no_bottom_margin" id="__sec1title">Conflict of Interests</h2>
             <!--/article/back/sec/-->
             <p id="__p41" class="p p-first-last">The authors declare that there is no conflict of interests regarding the publication of this paper.</p>
            </div>
            <div id="__ref-listidm140111969705664" class="tsec sec">
             <h2 class="head no_bottom_margin" id="__ref-listidm140111969705664title">References</h2>
             <div class="ref-list-sec sec" id="reference-list">
              <div class="ref-cit-blk half_rhythm" id="B49">
               1. 
               <span class="element-citation">Salton G. <span class="ref-journal"><em>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer</em>.</span> Wokingham, UK: Addison-Wesley; 1989. </span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B50">
               2. 
               <span class="element-citation">Salton G. <span class="ref-journal"><em>The SMART Retrieval System-Experiments in Automatic Document Processing</em>.</span> Englewood Cliffs, NJ, USA: Prentice Hall; 1971. </span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B51">
               3. 
               <span class="element-citation">Salton G, Lesk ME. Computer evaluation of indexing and text processing. <span><span class="ref-journal"><em>Journal of the ACM</em>. </span>1998;<span class="ref-vol">15</span>(1):8–36.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B65">
               4. 
               <span class="element-citation">Wu C-H, Yeh J-F, Lai Y-S. Semantic segment extraction and matching for internet FAQ retrieval. <span><span class="ref-journal"><em>IEEE Transactions on Knowledge and Data Engineering</em>. </span>2006;<span class="ref-vol">18</span>(7):930–940.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B7">
               5. 
               <span class="element-citation">Chim H, Deng X. Efficient phrase-based document similarity for clustering. <span><span class="ref-journal"><em>IEEE Transactions on Knowledge and Data Engineering</em>. </span>2008;<span class="ref-vol">20</span>(9):1217–1229.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B67">
               6. 
               <span class="element-citation">Zamir O, Etzioni O. Web document clustering: a feasibility demonstration. <span><span class="ref-journal"><em>Proceedings of the19th International ACM SIGIR Conference SIGIR Forum</em>. </span>1998:46–54.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B68">
               7. 
               <span class="element-citation">Zamir O, Etzioni O, Madani O, Karp RM. Fast and intuitive clustering of web documents. Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining; 1997; pp. 287–290.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B33">
               8. 
               <span class="element-citation">Li Y, McLean D, Bandar ZA, O’Shea JD, Crockett K. Sentence similarity based on semantic nets and corpus statistics. <span><span class="ref-journal"><em>IEEE Transactions on Knowledge and Data Engineering</em>. </span>2006;<span class="ref-vol">18</span>(8):1138–1150.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B36">
               9. 
               <span class="element-citation">Michie D. Return of the imitation game. <span><span class="ref-journal"><em>Electronic Transactions on Artificial Intelligence</em>. </span>2001;<span class="ref-vol">6</span>(2):203–221.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B3">
               10. 
               <span class="element-citation">Atkinson-Abutridy J, Mellish C, Aitken S. Combining information extraction with genetic algorithms for text mining. <span><span class="ref-journal"><em>IEEE Intelligent Systems</em>. </span>2004;<span class="ref-vol">19</span>(3):22–30.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B11">
               11. 
               <span class="element-citation">Erkan G, Radev DR. LexRank: graph-based lexical centrality as salience in text summarization. <span><span class="ref-journal"><em>Journal of Artificial Intelligence Research</em>. </span>2004;<span class="ref-vol">22</span>:457–479.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B26">
               12. 
               <span class="element-citation">Ko Y, Park J, Seo J. Improving text categorization using the importance of sentences. <span><span class="ref-journal"><em>Information Processing and Management</em>. </span>2004;<span class="ref-vol">40</span>(1):65–79.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B35">
               13. 
               <span class="element-citation">Liu Y, Zong CQ. Example-based Chinese-English MT. Proceedings of the IEEE International Conference on Systems, Man and Cybernetics (SMC '04); October 2004; pp. 6093–6096.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B12">
               14. 
               <span class="element-citation">Foltz PW, Kintsch W, Landauer TK. The measurement of textual coherence with latent semantic analysis. <span><span class="ref-journal"><em>Discourse Processes</em>. </span>1998;<span class="ref-vol">25</span>(2-3):285–307.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B27">
               15. 
               <span class="element-citation">Landauer TK, Foltz PW, Laham D. Introduction to latent semantic analysis. <span><span class="ref-journal"><em>Discourse Processes</em>. </span>1998;<span class="ref-vol">25</span>(2-3):259–284.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B28">
               16. 
               <span class="element-citation">Landauer TK, Laham D, Rehder B, Schreiner ME. How well can passage meaning be derived without using word order? A comparison of latent semantic analysis and humans. Proceedings of the 19th Annual Meeting of the Cognitive Science Society; 1997; pp. 412–417.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B19">
               17. 
               <span class="element-citation">Hofmann T. Probabilistic latent semantic indexing. Proceedings of Proceedings of the International ACM SIGIR Conference (SIGIR ’99); 1999; pp. 50–57.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B4">
               18. 
               <span class="element-citation">Bellegarda JR. Exploiting latent sematic information in statistical language modeling. <span><span class="ref-journal"><em>Proceedings of the IEEE</em>. </span>2000;<span class="ref-vol">88</span>(8):1279–1296.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B8">
               19. 
               <span class="element-citation">Chou T-C, Chen MC. Using incremental PLSI for threshold-resilient online event analysis. <span><span class="ref-journal"><em>IEEE Transactions on Knowledge and Data Engineering</em>. </span>2008;<span class="ref-vol">20</span>(3):289–299.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B24">
               20. 
               <span class="element-citation">Valle-Lisboa JC, Mizraji E. The uncovering of hidden structures by Latent Semantic Analysis. <span><span class="ref-journal"><em>Information Sciences</em>. </span>2007;<span class="ref-vol">177</span>(19):4122–4147.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B25">
               21. 
               <span class="element-citation">Kim H, Seo J. Cluster-based FAQ retrieval using latent term weights. <span><span class="ref-journal"><em>IEEE Intelligent Systems</em>. </span>2008;<span class="ref-vol">23</span>(2):58–65.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B56">
               22. 
               <span class="element-citation">Letsche TA, Berry MW. Large-scale information retrieval with latent semantic indexing. <span><span class="ref-journal"><em>Information Sciences</em>. </span>1997;<span class="ref-vol">100</span>(1–4):105–137.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B63">
               23. 
               <span class="element-citation">Wu C-H, Hsia C-C, Chen J-F, Wang J-F. Variable-length unit selection in TTS using structural syntactic cost. <span><span class="ref-journal"><em>IEEE Transactions on Audio, Speech and Language Processing</em>. </span>2007;<span class="ref-vol">15</span>(4):1227–1235.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B72">
               24. 
               <span class="element-citation">Zhou K, Gui-Rong X, Yang Q, Yu Y. Learning with positive and unlabeled examples using topic-sensitive PLSA. <span><span class="ref-journal"><em>IEEE Transactions on Knowledge and Data Engineering</em>. </span>2010;<span class="ref-vol">22</span>(1):46–58.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B5">
               25. 
               <span class="element-citation">Burgess C, Livesay K, Lund K. Explorations in context space: words. <span><span class="ref-journal"><em>Sentences, Discourse, Discourse Processes</em>. </span>1998;<span class="ref-vol">25</span>(2-3):211–257.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B16">
               26. 
               <span class="element-citation">Gruber TR. A translation approach to portable ontology specifications. <span><span class="ref-journal"><em>Knowledge Acquisition</em>. </span>1993;<span class="ref-vol">5</span>(2):199–220.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B17">
               27. 
               <span class="element-citation">Gruber TR. Toward principles for the design of ontologies used for knowledge sharing? <span><span class="ref-journal"><em>International Journal of Human: Computer Studies</em>. </span>1995;<span class="ref-vol">43</span>(5-6):907–928.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B31">
               28. 
               <span class="element-citation">Berners-Lee T, Hendler J, Lassila O. The semantic web. <span><span class="ref-journal"><em>Scientific American</em>. </span>2001;<span class="ref-vol">284</span>(5):34–43.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B13">
               29. 
               <span class="element-citation">Formica A. Ontology-based concept similarity in Formal Concept Analysis. <span><span class="ref-journal"><em>Information Sciences</em>. </span>2006;<span class="ref-vol">176</span>(18):2624–2641.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B21">
               30. 
               <span class="element-citation">Moreda P, Llorens H, Saquete E, Palomar M. Combining semantic information in question answering systems. <span><span class="ref-journal"><em>Information Processing and Management</em>. </span>2011;<span class="ref-vol">47</span>(6):870–885.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B23">
               31. 
               <span class="element-citation">Janev V, Vraneš S. Applicability assessment of Semantic Web technologies. <span><span class="ref-journal"><em>Information Processing and Management</em>. </span>2011;<span class="ref-vol">47</span>(4):507–517.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B2">
               32. 
               <span class="element-citation">Albert LK. <span class="ref-journal"><em>YMIR: an ontology for engineering design [Ph.D. thesis]</em></span> Twente, The Netherlands: University of Twente; 1993. </span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B14">
               33. 
               <span class="element-citation">Guarino N. Understanding, building and using ontologies. <span><span class="ref-journal"><em>International Journal of Human Computer Studies</em>. </span>1997;<span class="ref-vol">46</span>(2-3):293–310.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B18">
               34. 
               <span class="element-citation">van Heijst G, Schreiber AT, Wielinga BJ. Using explicit ontologies in KBS development. <span><span class="ref-journal"><em>International Journal of Human Computer Studies</em>. </span>1997;<span class="ref-vol">46</span>(2-3):183–292.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B15">
               35. 
               <span class="element-citation">Guarino N, Giaretta P. <span class="ref-journal"><em>Towards Very Large Knowledge Bases: Knowledge Building and Knowledge Sharing</em>.</span> 1995. Ontologies and knowledge bases: towards a terminological clarification; pp. 25–32.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B52">
               36. 
               <span class="element-citation">Schreiber AT, Wielinga BJ, Jansweijer WHJ. The KACTUS view on the “O” Word. Proceedings of the IJCAI Workshop on Basic Ontological Issues in Knowledge Sharing; 1995; pp. 159–168.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B59">
               37. 
               <span class="element-citation">Uschold M, Gruninger M. Ontologies: principles, methods and applications. <span><span class="ref-journal"><em>Knowledge Engineering Review</em>. </span>1996;<span class="ref-vol">11</span>(2):93–136.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B43">
               38. 
               <span class="element-citation">RDF. <a href="http://www.w3.org/RDF/" ref="reftype=extlink&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI&amp;rendering-type=normal" target="pmc_ext">http://www.w3.org/RDF/</a></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B44">
               39. 
               <span class="element-citation">RDF-Schema. <a href="http://www.w3.org/TR/rdf-schema/" ref="reftype=extlink&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI&amp;rendering-type=normal" target="pmc_ext">http://www.w3.org/TR/rdf-schema/</a></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B66">
               40. 
               <span class="element-citation">XML. <a href="http://www.w3.org/XML/" ref="reftype=extlink&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI&amp;rendering-type=normal" target="pmc_ext">http://www.w3.org/XML/</a></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B40">
               41. 
               <span class="element-citation">OWL-REF. <a href="http://www.w3.org/TR/owl-ref/" ref="reftype=extlink&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI&amp;rendering-type=normal" target="pmc_ext">http://www.w3.org/TR/owl-ref/</a></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B62">
               42. 
               <span class="element-citation">WordNet. <a href="http://wordnet.princeton.edu/" ref="reftype=extlink&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI&amp;rendering-type=normal" target="pmc_ext">http://wordnet.princeton.edu/</a></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B6">
               43. 
               <span class="element-citation">Chen KU, Zhao JC, Zuo WL, He FL, Chen YH. Automatic table integration by domain-specific ontology. <span><span class="ref-journal"><em>International Journal of Digital Content Technology and its Applications</em>. </span>2011;<span class="ref-vol">5</span>(1):218–225.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B10">
               44. 
               <span class="element-citation">Pan D. An integrative framework for continuous knowledge discovery. <span><span class="ref-journal"><em>Journal of Convergence Information Technology</em>. </span>2010;<span class="ref-vol">5</span>(3):46–53.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B30">
               45. 
               <span class="element-citation">Lee MC, Chen HH, Li YS. FCA based concept constructing and similarity measurement algorithms. <span><span class="ref-journal"><em>International Journal of Advancements in Computing Technology</em>. </span>2011;<span class="ref-vol">3</span>(1):97–105.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B48">
               46. 
               <span class="element-citation">Ruiz-Casado M, Alfonseca E, Castells P. Automatising the learning of lexical patterns: an application to the enrichment of WordNet by extracting semantic relationships from Wikipedia. <span><span class="ref-journal"><em>Data and Knowledge Engineering</em>. </span>2007;<span class="ref-vol">61</span>(3):484–499.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B53">
               47. 
               <span class="element-citation">Seo H-C, Chung H, Rim H-C, Myaeng SH, Kim S-H. Unsupervised word sense disambiguation using WordNet relatives. <span><span class="ref-journal"><em>Computer Speech and Language</em>. </span>2004;<span class="ref-vol">18</span>(3):253–273.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B69">
               48. 
               <span class="element-citation">Ye Z, Guoqiang C, Dongyue J. A modified method for concepts similarity calculation. <span><span class="ref-journal"><em>Journal of Convergence Information Technology</em>. </span>2011;<span class="ref-vol">6</span>(1):34–40.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B71">
               49. 
               <span class="element-citation">Zhang W, Yoshida T, Tang X. Using ontology to improve precision of terminology extraction from documents. <span><span class="ref-journal"><em>Expert Systems with Applications</em>. </span>2009;<span class="ref-vol">36</span>(5):9333–9339.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B64">
               50. 
               <span class="element-citation">Wu Z, Palmer M. Verb semantics and lexical selection. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics; 1994; pp. 133–138.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B55">
               51. 
               <span class="element-citation">Sleator D, Temperley D. Parsing English with a link grammar. <span><span class="ref-journal"><em>Carnegie Mellon University Computer Science Technical Report</em>. </span>1991;(CMU-CS-91-196)</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B1">
               52. 
               <span class="element-citation">Abiword Project. <a href="http://www.abisource.com/projects/link-grammar/" ref="reftype=extlink&amp;article-id=4005080&amp;issue-id=233213&amp;journal-id=1623&amp;FROM=Article%7CCitationRef&amp;TO=External%7CLink%7CURI&amp;rendering-type=normal" target="pmc_ext">http://www.abisource.com/projects/link-grammar/</a></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B42">
               53. 
               <span class="element-citation">Quirk C, Brockett C, Dolan W. Monolingual machine translation for paraphrase generation. In: Lin D, Wu D, editors. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP '04); 2004; pp. 142–149.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B38">
               54. 
               <span class="element-citation">O'Shea J, Bandar Z, Crockett K, McLean D. A comparative study of two short text semantic similarity measures. (Lecture Notes in Artificial Intelligence).<span><span class="ref-journal"><em>Agent and Multi-Agent Systems: Technologies and Applications</em>. </span>2008;<span class="ref-vol">4953</span>:172–181.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B20">
               55. 
               <span class="element-citation">Islam A, Inkpen D. Semantic text similarity using corpus-based word similarity and string similarity. <span><span class="ref-journal"><em>ACM Transactions on Knowledge Discovery from Data</em>. </span>2008;<span class="ref-vol">2</span>(2, article 10)</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B39">
               56. 
               <span class="element-citation">Oliva J, Serrano JI, del Castillo MD, Iglesias Á. SyMSS: a syntax-based measure for short-text semantic similarity. <span><span class="ref-journal"><em>Data and Knowledge Engineering</em>. </span>2011;<span class="ref-vol">70</span>(4):390–405.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B57">
               57. 
               <span class="element-citation">Tsatsaronis G, Varlamis I, Vazirgiannis M. Text relatedness based on aword thesaurus. <span><span class="ref-journal"><em>Journal of Artificial Intelligence Research</em>. </span>2010;<span class="ref-vol">37</span>:1–39.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B61">
               58. 
               <span class="element-citation">Wan S, Dras M, Dale R, Paris C. Using dependency-based features to take the parafarce out of paraphrase. Proceedings of the Australasian Language Technology Workshop; 2006; pp. 131–138.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B41">
               59. 
               <span class="element-citation">Qiu L, Kan M-Y, Chua T-S. Paraphrase recognition via dissimilarity significance classification. Proceedings of the 11th Conference on Empirical Methods in Natural Language Proceessing (EMNLP '06); July 2006; pp. 18–26.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B47">
               60. 
               <span class="element-citation">Rubenstein H, Goodenough JB. Contextual correlates of synonymy. <span><span class="ref-journal"><em>Communications of the ACM</em>. </span>1965;<span class="ref-vol">8</span>(10):627–633.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B54">
               61. 
               <span class="element-citation">Sinclair J. <span class="ref-journal"><em>Collins Cobuild English Dictionary for Advanced Learners</em>.</span> 3rd edition. Harper Collins; 2001. </span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B58">
               62. 
               <span class="element-citation">Turney P. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. Proceedings of the 12th European Conference on Machine Learning (ECML '01); 2001; pp. 491–502.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B22">
               63. 
               <span class="element-citation">Jiang J, Conrath D. Semantic similarity based on corpus statistics and lexical taxonomy. Proceedings of the International Conference Research on Computational Linguistics (ROCLING '96),; 1996; pp. 19–33.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B29">
               64. 
               <span class="element-citation">Leacock C, Miller GA, Chodorow M. Using corpus statistics and wordnet relations for sense identification. <span><span class="ref-journal"><em>Computational Linguistics</em>. </span>1998;<span class="ref-vol">24</span>(1):146–165.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B34">
               65. 
               <span class="element-citation">Lin D. An information-theoretic definition of similarity. Proceedings of the 15th International Conference on Machine Learning (ICML '98); 1998; pp. 296–304.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B45">
               66. 
               <span class="element-citation">Resnik P. Semantic similarity in a taxonomy: an information-based measure and its application to problems of ambiguity in natural language. <span><span class="ref-journal"><em>Journal of Artificial Intelligence Research</em>. </span>1999;<span class="ref-vol">11</span>:95–130.</span></span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B46">
               67. 
               <span class="element-citation">Resnik P. Using information content to evaluate semantic similarity. Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI '95); 1995; pp. 448–453.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B32">
               68. 
               <span class="element-citation">Lesk M. Automated sense disambiguation using machine-readable dictionaries: how to tell a pine cone from an ice cream cone. Proceedings of the 5th Annual International Conference on Systems Documentation (SIGDOC '86); 1986; pp. 24–26.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B37">
               69. 
               <span class="element-citation">Mihalcea R, Corley C, Strapparava C. Corpus-based and knowledge-based measures of text semantic similarity. Proceedings of the 21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference (AAAI '06); July 2006; pp. 775–780.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B70">
               70. 
               <span class="element-citation">Zhang Y, Patrick J. Paraphrase identification by text canonicalization. Proceedings of the Australasian Language Technology Workshop; 2005; pp. 160–166.</span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B60">
               71. 
               <span class="element-citation">Vapnik V. <span class="ref-journal"><em>The Nature of Statistical Learning Theory</em>.</span> New York, NY, USA: Springer; 1995. </span>
              </div>
              <div class="ref-cit-blk half_rhythm" id="B9">
               72. 
               <span class="element-citation">Corley C, Mihalcea R. Measures of text semantic similarity. Proceedings of the ACL workshop on Empirical Modeling of Semantic Equivalence; 2005; Ann Arbor, Mich, USA. </span>
              </div>
             </div>
            </div>
           </div>
           <!--post-content-->
           <hr class="whole_rhythm no_bottom_margin">
           <div class="courtesy-note no_margin small">
            Articles from 
            <span class="acknowledgment-journal-title">The Scientific World Journal</span> are provided here courtesy of 
            <strong>Hindawi</strong>
           </div>
          </div> 
         </div> 
         <!-- Book content --> 
        </div> 
        <div id="rightcolumn" class="four_col col last"> 
         <!-- Custom content above discovery portlets --> 
         <div class="col6"> 
         </div> 
         <div xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
          <div class="format-menu">
           <h2>Formats:</h2>
           <ul>
            <li class="selected">Article</li> | 
            <li><a href="/pmc/articles/PMC4005080/?report=reader">PubReader</a></li> | 
            <li class="epub-link"><a href="/pmc/articles/PMC4005080/epub/">ePub (beta)</a></li> | 
            <li><a href="/pmc/articles/PMC4005080/pdf/TSWJ2014-437162.pdf">PDF (1.0M)</a></li> | 
            <li><a href="#" data-citationid="PMC4005080" class="citationexporter ctxp">Citation</a></li>
           </ul>
          </div>
         </div>
         <div xmlns:np="http://ncbi.gov/portal/XSLT/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="share-buttons">
          <h2>Share</h2>
          <ul>
           <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4005080%2F"><img src="//static.pubmed.gov/portal/portal3rc.fcgi/4158155/img/4047626" alt="Share on Facebook"> Facebook </a></li>
           <li class="twitter"><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4005080%2F&amp;text=A%20Grammar-Based%20Semantic%20Similarity%20Algorithm%20for%20Natural%20Language%20Sentences"><img src="//static.pubmed.gov/portal/portal3rc.fcgi/4158155/img/4047627" alt="Share on Twitter"> Twitter </a></li>
           <li class="gplus"><a href="https://plus.google.com/share?url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4005080%2F"><img src="//static.pubmed.gov/portal/portal3rc.fcgi/4158155/img/4047628" alt="Share on Google Plus"> Google+ </a></li>
          </ul>
         </div> 
         <div id="ajax-portlets" data-pmid="24982952" data-aiid="4005080" data-aid="4005080" data-iid="233213" data-domainid="1623" data-domain="tswj" data-accid="PMC4005080" data-md5="04403e0dfc7ed97ca617dbe541bc8e23"></div> 
         <!-- Custom content below discovery portlets --> 
         <div class="col7"> 
         </div> 
        </div> 
       </div> 
       <!-- Custom content after all --> 
       <div class="col8"> 
       </div> 
       <div class="col9"> 
       </div> 
       <script src="/corehtml/pmc/js/jquery.scrollTo-1.4.2.js"></script> 
       <script>
    (function($){
        $('.skiplink').each(function(i, item){
            var href = $($(item).attr('href'));
            href.attr('tabindex', '-1').addClass('skiptarget'); // ensure the target can receive focus
            $(item).on('click', function(event){
                event.preventDefault();
                $.scrollTo(href, 0, {
                    onAfter: function(){
                        href.focus();
                    }
                });
            });
        });
    })(jQuery);
</script> 
       <div id="body-link-poppers"></div> 
      </div> 
      <div class="bottom"> 
       <div id="NCBIFooter_dynamic"> 
        <a id="help-desk-link" class="help_desk jig-ncbihelpwindow" target="_blank" href="">Support Center</a> 
        <a id="help-desk-link" class="help_desk jig-ncbihelpwindow" target="_blank" href="https://support.ncbi.nlm.nih.gov/ics/support/KBList.asp?Time=2017-10-03T08:08:50-04:00&amp;Snapshot=%2Fprojects%2FPMC%2FPMCViewer@4.45&amp;Host=ptpmc201&amp;ncbi_phid=F4FB5C579D37DCA10000000000080008&amp;ncbi_session=F4FB5C579D37DD11_0008SID&amp;from=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4005080%2F&amp;Db=pmc&amp;folderID=132&amp;Ncbi_App=pmc&amp;Page=literature&amp;style=classic&amp;deptID=28049">Support Center</a> 
       </div> 
       <div class="footer" id="footer"> 
        <div class="subfooter"> 
        </div>
        <script type="text/javascript" src="/portal/portal3rc.fcgi/static/js/preloaderWidget.js"> </script> 
        <div id="external-disclaimer" class="offscreen_noflow">
          External link. Please review our 
         <a href="https://www.nlm.nih.gov/privacy.html">privacy policy</a>. 
        </div> 
        <div id="ncbifooter" class="contact_info"> 
         <div id="footer-contents-right"> 
          <div id="nlm_thumb_logo"> 
           <a href="https://www.nlm.nih.gov" title="NLM">NLM</a> 
          </div> 
          <div id="nih_thumb_logo"> 
           <a href="https://www.nih.gov" title="NIH">NIH</a> 
          </div> 
          <div id="hhs_thumb_logo"> 
           <a href="https://www.hhs.gov" title="DHHS">DHHS</a> 
          </div> 
          <div id="usagov_thumb_logo"> 
           <a href="https://www.usa.gov" title="USA.gov">USA.gov</a> 
          </div> 
         </div> 
         <div id="footer-contents-left"> 
          <p class="address vcard"> <span class="url"> <a class="fn url newdomain" href="https://www.ncbi.nlm.nih.gov">National Center for Biotechnology Information</a>, </span> <span class="org url newdomain"><a href="https://www.nlm.nih.gov/">U.S. National Library of Medicine</a></span> <span class="adr"> <span class="street-address">8600 Rockville Pike</span>, <span class="locality">Bethesda</span> <span class="region">MD</span>, <span class="postal-code">20894</span> <span class="country-name">USA</span> </span> </p> 
          <a href="/home/about/policies.shtml">Policies and Guidelines</a> | 
          <a href="/home/about/contact.shtml">Contact</a> 
         </div> 
        </div> 
        <script type="text/javascript" src="/portal/portal3rc.fcgi/rlib/js/InstrumentOmnitureBaseJS/InstrumentNCBIConfigJS/InstrumentNCBIBaseJS/InstrumentPageStarterJS.js?v=1"> </script> 
        <script type="text/javascript" src="/portal/portal3rc.fcgi/static/js/hfjs2.js"> </script> 
       </div> 
      </div> 
     </div> 
     <!--/.page--> 
    </div> 
    <!--/.wrap--> 
   </div>
   <!-- /.twelve_col --> 
  </div> 
  <!-- /.grid --> 
  <span class="PAFAppResources"></span> 
  <!-- BESelector tab --> 
  <noscript>
   <img alt="statistics" src="/stat?jsdisabled=true&amp;ncbi_db=pmc&amp;ncbi_pdid=article&amp;ncbi_acc=&amp;ncbi_domain=tswj&amp;ncbi_report=record&amp;ncbi_type=fulltext&amp;ncbi_objectid=&amp;ncbi_pcid=/articles/PMC4005080/&amp;ncbi_app=pmc">
  </noscript> 
  <!-- usually for JS scripts at page bottom --> 
  <!--<component id="PageFixtures" label="styles"></component>--> 
  <!-- F4FB5C579D37DD11_0008SID /projects/PMC/PMCViewer@4.45 ptpmc201 v4.1.r526566 Fri, Feb 03 2017 14:27:04 --> 
  <script type="text/javascript" src="//static.pubmed.gov/portal/portal3rc.fcgi/4158155/js/3879255/4121861/3818874/3821238/4117325/4087685/4072593/4076480/3921943/4105668/4065628.js" snapshot="pmc"></script> 
 </body>
</html>