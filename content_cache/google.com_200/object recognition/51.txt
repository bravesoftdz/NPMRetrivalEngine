<!doctype html>
<!--[if lte IE 8]>
    <html id="doc" class="lt-ie9" lang="en">
<![endif]-->
<!--[if gt IE 8]>
    <html id="doc" lang="en">
<![endif]-->
<!--[if !IE]> -->
<html id="doc" lang="en">
 <!-- <![endif]-->
 <head id="Head1">
  <meta charset="utf-8">
  <title>
	Looking as if you know: Systematic object inspection precedes object recognition | JOV | ARVO Journals
</title>
  <link rel="canonical" href="http://jov.arvojournals.org/article.aspx?articleid=2122108">
  <link id="lnkFavicon" rel="icon" type="image/png" href="/UI/app/images/JOV_favicon.png"> 
  <link href="/UI/app/images/60x60_jov.png" rel="apple-touch-icon" sizes="60x60"> 
  <link href="/UI/app/images/76x76_jov.png" rel="apple-touch-icon" sizes="76x76"> 
  <link href="/UI/app/images/120x120_jov.png" rel="apple-touch-icon" sizes="120x120"> 
  <link href="/UI/app/images/152x152_jov.png" rel="apple-touch-icon" sizes="152x152"> 
  <link href="http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,300,600,400,800" rel="stylesheet" type="text/css"> 
  <meta name="citation_author" content="Linus Holm">
  <meta name="citation_author_email" content="linus.holm@psy.umu.se">
  <meta name="citation_author_institution" content="Department of Psychology, Umeå University, Umeå, Sweden">
  <meta name="citation_author" content="Johan Eriksson">
  <meta name="citation_author_email" content="johan.eriksson@psy.umu.se">
  <meta name="citation_author_institution" content="Department of Psychology, Umeå University, Umeå, Sweden">
  <meta name="citation_author" content="Linus Andersson">
  <meta name="citation_author_email" content="linus.andersson@psy.umu.se">
  <meta name="citation_author_institution" content="Department of Psychology, Umeå University, Umeå, Sweden">
  <meta name="citation_title" content="Looking as if you know: Systematic object inspection precedes object recognition">
  <meta name="citation_firstpage" content="14">
  <meta name="citation_lastpage" content="14">
  <meta name="citation_doi" content="10.1167/8.4.14">
  <meta name="citation_keyword" content="inspection">
  <meta name="citation_keyword" content="object recognition">
  <meta name="citation_journal_title" content="Journal of Vision">
  <meta name="citation_journal_abbrev" content="Journal of Vision">
  <meta name="citation_volume" content="8">
  <meta name="citation_issue" content="4">
  <meta name="citation_publication_date" content="2008/04/01">
  <meta name="citation_issn" content="1534-7362">
  <meta name="citation_publisher" content="The Association for Research in Vision and Ophthalmology">
  <meta name="citation_reference" content="citation_title=Top-down facilitation of visual recognition; citation_author=Bar  M.; citation_author=Kassam  K. S.; citation_author=Ghuman  A. S.; citation_author=Boschyan  J.; citation_author=Schmid  A. M.; citation_author=Dale  A. M.;  citation_journal_title=Proceedings of the National Academy of Science of the United States of America;  citation_year=2006;  citation_volume=103;  citation_pages=449-454; ">
  <meta name="citation_reference" content="citation_title=A neuro-cognitive visual system for object recognition based on testing of interactive attentional top-down hypotheses; citation_author=Deco  G.; citation_author=Schürmann  B.;  citation_journal_title=Perception;  citation_year=2000;  citation_volume=29;  citation_pages=1249-1264; ">
  <meta name="citation_reference" content="citation_title=How the brain learns to see objects and faces in an impoverished context; citation_author=Dolan  R. J.; citation_author=Fink  G. R.; citation_author=Rolls  E.; citation_author=Booth  M.; citation_author=Holmes  A.; citation_author=Franckowiak  R. S.;  citation_journal_title=Nature;  citation_year=1997;  citation_volume=389;  citation_pages=596-599; ">
  <meta name="citation_reference" content="citation_title=Eye movements during the viewing of Necker cubes; citation_author=Ellis  S. R.; citation_author=Stark  L.;  citation_journal_title=Perception;  citation_year=1978;  citation_volume=7;  citation_pages=575-581; ">
  <meta name="citation_reference" content="citation_title=Visual consciousness: Dissociating the neural correlates of perceptual transitions from sustained perception with fMRI; citation_author=Eriksson  J.; citation_author=Larsson  A.; citation_author=Riklund-Åhlström  K.; citation_author=Nyberg  L.;  citation_journal_title=Consciousness and Cognition;  citation_year=2004;  citation_volume=13;  citation_pages=61-72; ">
  <meta name="citation_reference" content="citation_title=Learning and inference in the brain; citation_author=Friston  K.;  citation_journal_title=Neural Networks;  citation_year=2003;  citation_volume=16;  citation_pages=1325-1352; ">
  <meta name="citation_reference" content="citation_title=Edge co-occurrence in natural images predicts contour grouping performance; citation_author=Geisler  W. S.; citation_author=Perry  J. S.; citation_author=Super  B. J.; citation_author=Gallogly  D. P.;  citation_journal_title=Vision Research;  citation_year=2001;  citation_volume=41;  citation_pages=711-724; ">
  <meta name="citation_reference" content="citation_title=As soon as you know it is there, you know what it is; citation_author=Grill-Spector  K.; citation_author=Kanwisher  N.;  citation_journal_title=Psychological Science;  citation_year=2005;  citation_volume=16;  citation_pages=152-160; ">
  <meta name="citation_reference" content="citation_author=Helmholtz  H.; citation_publisher=Leopold Voss, Leipzig;  citation_title=Handbuch der physiologischen optik: Vol. 3;  citation_year=1910; ">
  <meta name="citation_reference" content="citation_title=Does consistent scene context facilitate object perception; citation_author=Hollingworth  A.; citation_author=Henderson  J. M.;  citation_journal_title=Journal of Experimental Psychology: General;  citation_year=1998;  citation_volume=127;  citation_pages=398-415; ">
  <meta name="citation_reference" content="citation_title=To see and remember: Visually specific information is retained in memory from previously attended objects in natural scenes; citation_author=Hollingworth  A.; citation_author=Williams  C. C.; citation_author=Henderson  J. M.;  citation_journal_title=Psychonomic Bulletin &amp; Review;  citation_year=2001;  citation_volume=8;  citation_pages=761-768; ">
  <meta name="citation_reference" content="citation_author=Holm  L.;  citation_title=Predictive eyes precede retrieval: Visual recognition as hypothesis testing;  citation_year=2007; ">
  <meta name="citation_reference" content="citation_title=Memory for scenes: Refixations reflect retrieval; citation_author=Holm  L.; citation_author=Mäntylä  T.;  citation_journal_title=Memory &amp; Cognition;  citation_year=2007;  citation_volume=35;  citation_pages=1664-1674; ">
  <meta name="citation_reference" content="citation_title=Perceptual switching, eye movements, and the bus paradox; citation_author=Ito  J.; citation_author=Nikolaev  A. R.; citation_author=Luman  M.; citation_author=Aukes  M. F.; citation_author=Nakatani  C.; citation_author=van Leeuwen  C.;  citation_journal_title=Perception;  citation_year=2003;  citation_volume=32;  citation_pages=681-698; ">
  <meta name="citation_reference" content="citation_title=Object perception as Bayesian inference; citation_author=Kersten  D.; citation_author=Mamassian  P.; citation_author=Yuille  A.;  citation_journal_title=Annual Review of Psychology;  citation_year=2004;  citation_volume=55;  citation_pages=271-304; ">
  <meta name="citation_reference" content="citation_title=Early neural activity in Necker-cube reversal: Evidence for low-level processing of a gestalt phenomenon; citation_author=Kornmeier  J.; citation_author=Bach  M.;  citation_journal_title=Psychophysiology;  citation_year=2004;  citation_volume=41;  citation_pages=1-8; ">
  <meta name="citation_reference" content="citation_title=Fixation patterns made during brief examination of two-dimensional images; citation_author=Mannan  S. K.; citation_author=Ruddock  K. H.; citation_author=Wooding  D. S.;  citation_journal_title=Perception;  citation_year=1997;  citation_volume=26;  citation_pages=1059-1072; ">
  <meta name="citation_reference" content="citation_title=Visual skills in airport-security screening; citation_author=McCarley  J. S.; citation_author=Kramer  A. F.; citation_author=Wickens  C. D.; citation_author=Vidoni  E. D.; citation_author=Boot  W. R.;  citation_journal_title=Psychological Science;  citation_year=2004;  citation_volume=15;  citation_pages=302-306; ">
  <meta name="citation_reference" content="citation_title=Shapes, surfaces and saccades; citation_author=Melcher  D.; citation_author=Kowler  E.;  citation_journal_title=Vision Research;  citation_year=1999;  citation_volume=39;  citation_pages=2929-2946; ">
  <meta name="citation_reference" content="citation_title=Associative knowledge controls deployment of visual attention; citation_author=Moores  E.; citation_author=Laiti  L.; citation_author=Chelazzi  L.;  citation_journal_title=Nature Neuroscience;  citation_year=2003;  citation_volume=6;  citation_pages=182-189; ">
  <meta name="citation_reference" content="citation_title=Panoramic search: The interaction of memory and vision in search through a familiar scene; citation_author=Oliva  A.; citation_author=Wolfe  J. M.; citation_author=Arsenio  H. C.;  citation_journal_title=Journal of Experimental Psychology: Human Perception and Performance;  citation_year=2004;  citation_volume=30;  citation_pages=1132-1146; ">
  <meta name="citation_reference" content="citation_title=Disambiguating complex visual information: Towards communication of personal views of a scene; citation_author=Pomplun  M.; citation_author=Ritter  H.; citation_author=Velichovsky  B.;  citation_journal_title=Perception;  citation_year=1996;  citation_volume=25;  citation_pages=941-948; ">
  <meta name="citation_reference" content="citation_title=Time to understand pictures and words; citation_author=Potter  M. C.; citation_author=Faulconer  B. A.;  citation_journal_title=Nature;  citation_year=1975;  citation_volume=253;  citation_pages=437-438; ">
  <meta name="citation_reference" content="citation_title=Amnesia is a deficit in relational memory; citation_author=Ryan  J. D.; citation_author=Althoff  R. R.; citation_author=Whitlow  S.; citation_author=Cohen  N. J.;  citation_journal_title=Psychological Science;  citation_year=2000;  citation_volume=11;  citation_pages=454-461; ">
  <meta name="citation_reference" content="citation_title=A standardized set of 260 pictures: Norms for name agreement, image agreement and visual complexity; citation_author=Snodgrass  J. G.; citation_author=Vanderwart  M.;  citation_journal_title=Journal of Experimental Psychology: Human Learning and Memory;  citation_year=1980;  citation_volume=6;  citation_pages=174-215; ">
  <meta name="citation_reference" content="citation_title=Predictive codes for forthcoming perception in the frontal cortex; citation_author=Summerfield  C.; citation_author=Egner  T.; citation_author=Greene  M.; citation_author=Koechlin  E.; citation_author=Mangels  J.; citation_author=Hirsch  J.;  citation_journal_title=Science;  citation_year=2006;  citation_volume=314;  citation_pages=1311-1314; ">
  <meta name="citation_reference" content="citation_title=Orienting attention based on long-term memory experience; citation_author=Summerfield  J. J.; citation_author=Lepsien  J.; citation_author=Gitelman  D. R.; citation_author=Mesulam  M. M.; citation_author=Nobre  A. C.;  citation_journal_title=Neuron;  citation_year=2006;  citation_volume=49;  citation_pages=905-916; ">
  <meta name="citation_reference" content="citation_title=Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search; citation_author=Torralba  A.; citation_author=Oliva  A.; citation_author=Castelhano  S. M.; citation_author=Henderson  J. M.;  citation_journal_title=Psychological Review;  citation_year=2006;  citation_volume=113;  citation_pages=766-786; ">
  <meta name="citation_reference" content="citation_title=Is visual image segmentation a bottom-up or an interactive process; citation_author=Vecera  S. P.; citation_author=Farah  M. J.;  citation_journal_title=Perception &amp; Psychophysics;  citation_year=1997;  citation_volume=59;  citation_pages=1280-1296; ">
  <meta name="citation_fulltext_world_readable" content="">
  <meta name="citation_pdf_url" content="http://jov.arvojournals.org/data/journals/jov/932856/jov-8-4-14.pdf"> 
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge"> 
  <link href="/cassette.axd/stylesheet/e7cb59e7c037432b0cb276d7f7b6b33adbd0d27b/bundles/shared-css" type="text/css" rel="stylesheet"> 
  <link href="/cassette.axd/stylesheet/46e0f9365c9762e1cf929ed822c54bdf7b20a8dd/bundles/css/content-pages" type="text/css" rel="stylesheet"> 
  <link href="//cdn.jsdelivr.net/chartist.js/latest/chartist.min.css" type="text/css" rel="stylesheet"> 
  <link rel="stylesheet" href="UI/app/fonts/franklingothic.css">
  <link rel="stylesheet" href="UI/app/styles/branding/background.css"> 
  <script src="/cassette.axd/script/715a9cf00724a70d5d3ed9610f15ee9c7fef351e/bundles/js/Core" type="text/javascript"></script> 
  <!--[if (gte IE 6)&(lte IE 8)]>
            <script type="text/javascript" src="/UI/app/scripts/polyfills/respond.min.js"></script>
            <link rel="stylesheet" href="UI/app/styles/global/iefix.css" />
        <![endif]--> 
  <script type="text/javascript">
            var googletag = googletag || {};
            googletag.cmd = googletag.cmd || [];
            (function () {
                var gads = document.createElement('script');
                gads.async = true;
                gads.type = 'text/javascript';
                var useSSL = 'https:' == document.location.protocol;
                gads.src = (useSSL ? 'https:' : 'http:') +
                    '//www.googletagservices.com/tag/js/gpt.js';
                var node = document.getElementsByTagName('script')[0];
                node.parentNode.insertBefore(gads, node);
            })();
        </script> 
  <script type="text/javascript">
            googletag.cmd.push(function () {
                // responsive mappings
                var mapping_leaderboard = googletag.sizeMapping()
                  .addSize([750, 480], [728, 90]) // desktop 
                  .addSize([320, 400], [320, 100]) // Mobile 
                  .addSize([0, 0], [])
                  .build();
                var mapping_tower = googletag.sizeMapping()                
                  .addSize([750, 480], [160, 600]) // desktop 
                  .addSize([320, 400], [300, 250]) // Mobile 
                  .addSize([0, 0], [])
                  .build();

                // ARVO ad codes
                var w = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
                if (w > 750) {
                    // desktop
                    googletag.defineSlot('/130817127/JOV_ROS_leaderboard', [728, 90], 'div-gpt-ad-mast-leaderboard').defineSizeMapping(mapping_leaderboard).addService(googletag.pubads());
                    googletag.defineSlot('/130817127/JOV_ROS_tower', [160, 600], 'div-gpt-ad-content-tower').defineSizeMapping(mapping_tower).addService(googletag.pubads());
                } else {
                    // mobile
                    googletag.defineSlot('/130817127/JOV_mobile_leaderboard', [320, 100], 'div-gpt-ad-mast-leaderboard').defineSizeMapping(mapping_leaderboard).addService(googletag.pubads());
                    googletag.defineSlot('/130817127/JOV_mobile_pillow', [300, 250], 'div-gpt-ad-content-tower').defineSizeMapping(mapping_tower).addService(googletag.pubads());
                }
                
                googletag.pubads().enableSingleRequest();
                googletag.enableServices();
            });
        </script> 
 </head> 
 <body class="off-canvas arvo pg_article" data-sitename="jov" theme-jov> 
  <!-- Google Tag Manager --> 
  <noscript> 
   <iframe src="//www.googletagmanager.com/ns.html?id=GTM-MG7R8N" height="0" width="0" style="display: none; visibility: hidden"></iframe> 
  </noscript> 
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
        'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
    j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
    '//www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-MG7R8N');</script> 
  <!-- End Google Tag Manager --> 
  <form method="post" action="./article.aspx?articleid=2122108" id="webform"> 
   <div class="aspNetHidden"> 
    <input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="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"> 
   </div> 
   <div class="aspNetHidden"> 
    <input type="hidden" name="__VIEWSTATEGENERATOR" id="__VIEWSTATEGENERATOR" value="2173C2F0"> 
    <input type="hidden" name="__EVENTTARGET" id="__EVENTTARGET" value=""> 
    <input type="hidden" name="__EVENTARGUMENT" id="__EVENTARGUMENT" value=""> 
    <input type="hidden" name="__EVENTVALIDATION" id="__EVENTVALIDATION" value="/wEdAAqpKzskKS5NzgbGW190V9W/ud3klMxrHMtT1QhALa51hLRdgq07h3Ia1yoLUiQl7e+GDR/9H3H16vWfuk+ll/JrS4g2jmPfdcDjZLHdifFLXgYKvj1+ye81504o04JafrL8SBdQnEYKcdwa8FmgsnU2KTkiU5F5yVreXqTg8CmPEtraQ4OoTRHTMQwQE4Y2EF/By42jglXfXUG4T5SSlkzcao6VYTAiSM9gyJWvY20+oQGL2Bk="> 
   </div> 
   <input id="hdnSiteID" type="hidden" value="170"> 
   <a id="Top"></a> 
   <div class="master-container row collapse"> 
    <div class="large-12 columns"> 
     <section class="master-masthead"> 
      <div class="text-center header-ad" id="div-gpt-ad-mast-leaderboard"> 
       <script type="text/javascript">
        googletag.cmd.push(function () { googletag.display('div-gpt-ad-mast-leaderboard'); });        
        
    </script> 
      </div> 
      <header class="header-wrap"> 
       <div id="HeaderTop" class="header-top row collapse"> 
        <div class="left-tri-wrap tri-wrap"> 
         <div class="left-tri tri"></div> 
        </div> 
        <div class="right-tri-wrap tri-wrap"> 
         <div class="right-tri tri"></div> 
        </div> 
        <div id="UmbDropdownWrap" class="large-2 small-6 columns"> 
         <div class="umb-dropdown-area" data-toggle-target="#UmbNavDropdown"> 
          <div class="umb-drop-trigger"> 
           <img class="umb-logo" src="/UI/app/images/arvo_journals_logo-white.png"> 
           <i class="icon-caret-down"></i> 
           <div id="BodyContent_masthead_JournalShortName" class="journal-shortname">
            jov
           </div> 
          </div> 
          <div id="UmbNavDropdown"> 
           <ul id="MicroLinks" class="micro-nav"> 
            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_0"> <a href="/issues.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_0"> Issues </a> </li> 
            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_1"> <a href="/topics.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_1"> Topics </a> </li> 
            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_2"> <a href="/ss/forauthors.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_2"> For Authors </a> </li> 
            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_3" class="has-menu-dropdown"> <a href="/ss/about.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_3"> About <i id="BodyContent_masthead_rptNavBarUmbDropdown_IconCarrot_3" class="icon-angle-down"></i> </a> 
             <ul class="site-submenu"> 
              <li> <a href="/ss/editorial_board.aspx"> Editorial Board</a> </li> 
             </ul> </li> 
           </ul> 
           <ul class="all-journals"> 
            <li><a href="/index.aspx"><i class="icon-home"></i>Journals Home</a></li> 
            <li> <a href="http://iovs.arvojournals.org"> <img class="lazy" data-original="/UI/app/images/arvo_iovs_logo.png"> </a> </li> 
            <li> <a href="http://jov.arvojournals.org"> <img class="lazy" data-original="/UI/app/images/arvo_jov_logo.png"> </a> </li> 
            <li> <a href="http://tvst.arvojournals.org"> <img class="lazy" data-original="/UI/app/images/arvo_tvst_logo.png"> </a> </li> 
           </ul> 
          </div> 
         </div> 
        </div> 
        <div id="SignInWrap" class="large-3 small-6 columns large-push-7"> 
         <div class="sign-in-area"> 
          <div class="user-info"> 
           <span id="MobileSignInLink"><i class="icon-user"></i></span> 
           <span id="SignInLink" class="sign-in">SIGN IN</span> 
          </div> 
         </div> 
         <div id="SigninUserInfoDropdown" class="si-module"> 
          <div id="AccessSignIn" class="signin-container"> 
           <input type="hidden" name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$hfARVOStoreUrl" id="hfARVOStoreUrl" value="store.arvojournals.org"> 
           <div id="pnlGlobalSignin" class="si-panel" onkeypress="javascript:return WebForm_FireDefaultButton(event, 'BodyContent_masthead_ucGlobalSignInDropDown_ibSignIn')"> 
            <div class="si-form clearfix"> 
             <div class="message-error" id="wrong-username-password"></div> 
             <input name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$txtEmail" type="text" maxlength="50" id="txtEmail" class="requiredtxtusername" placeholder="Username"> 
             <div class="message-error" id="reqEmailError"></div> 
             <input name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$txtPassword" type="password" maxlength="50" id="txtPassword" class="requiredtxtpassword" placeholder="Password"> 
             <div class="message-error" id="reqPasswordError"></div> 
            </div> 
            <div class="si-forgot-pass clearfix text-center"> 
             <a href="http://www.arvo.org/ForgotPassword/?ReturnUrl=" target="_blank" class="create-acct">Forgot password?</a> 
            </div> 
            <div class="btn-group text-center">
             <input type="submit" name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$ibSignIn" value="Sign In" id="BodyContent_masthead_ucGlobalSignInDropDown_ibSignIn" class="dark-button signInBtn">
            </div> 
            <div id="BodyContent_masthead_ucGlobalSignInDropDown_divSignInCreateAccount" class="si-create-acct text-center clearfix"> 
             <a target="_blank" href="https://www.arvo.org/Login/?ReturnUrl=" class="create-acct">Create an Account</a> 
            </div> 
            <div id="signin-loading-spinner" class="signin-loading inner-circle hide"></div> 
           </div> 
           <div id="SubscribeBox" data-hideattribute="true" class="subscribe-box box"> 
            <h2>To View More...</h2> 
            <p id="BodyContent_masthead_ucGlobalSignInDropDown_pPurchaseSubInstruction">Purchase this article with an account.</p> 
            <div class="subscribe-btns"> 
             <p style="text-align: center;">or</p> 
             <a href="/ss/subscriptions.aspx"><span class="dark-button">Subscribe Now</span></a> 
            </div> 
           </div> 
          </div> 
         </div> 
         <div class="toggle-icon search-toggle" data-target="#Search">
          <i class="icon-search"></i>
         </div> 
        </div> 
        <div id="SearchWrap" class="large-7 columns large-pull-3"> 
         <div id="Search" class="search"> 
          <div class="search-tools"> 
           <div id="SearchButton" class="search-button point">
            <i class="icon-search"></i>
           </div> 
           <div class="adv-search"> 
            <a href="/solr/advancedsearch.aspx"> <span class="adv-text">Advanced</span>Search </a> 
           </div> 
          </div> 
          <div class="search-input"> 
           <input id="SearchTerm" class="search-term" type="text" placeholder="Search..." maxlength="255"> 
           <select id="searchResource" class="search-resource"> <option value="">All Journals</option> <option value="177" data-displayname="Investigative Ophthalmology &amp; Visual Science">IOVS</option> <option value="178" data-displayname="Journal of Vision">JOV</option> <option value="179" data-displayname="Translational Vision Science &amp; Technology">TVST</option> </select> 
           <input name="ctl00$ctl00$BodyContent$masthead$hfGlobalSearchSiteURL" type="hidden" id="hfGlobalSearchSiteURL" value="http://arvojournals.org"> 
          </div> 
         </div> 
        </div> 
       </div> 
       <div id="BodyContent_masthead_HeaderBottom" class="header-bottom clearfix"> 
        <div class="micro-logo"> 
         <a href="/"> <img src="/UI/app/images/arvo_jov_logo.png" id="BodyContent_masthead_ImgSiteLogo"> </a> 
        </div> 
        <div class="toggle-icon nav-toggle" data-target="#SiteNavMenu">
         <i class="icon-nav"></i>
        </div> 
        <nav class="nav-wrap"> 
         <ul id="SiteNavMenu" class="site-nav"> 
          <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_0"> <a href="/issues.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_0"> Issues </a> </li> 
          <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_1"> <a href="/topics.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_1"> Topics </a> </li> 
          <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_2"> <a href="/ss/forauthors.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_2"> For Authors </a> </li> 
          <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_3" class="has-menu-dropdown"> <a href="/ss/about.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_3"> About <i id="BodyContent_masthead_rptNavigationBar_IconCarrot_3" class="icon-angle-down"></i> </a> 
           <ul class="site-submenu"> 
            <li> <a href="/ss/editorial_board.aspx"> Editorial Board</a> </li> 
           </ul> </li> 
         </ul> 
        </nav> 
       </div> 
      </header> 
      <script type="text/javascript">
	var App = App || {};
        App.LoginUserInfo = {
            isInstLoggedIn: [0],
            currentSessionId: 'cuprsvsh12yq2dxwcdkuo43b'
        }; 
</script> 
      <script type="text/javascript">
    $('body').on('click', '.sign-in-area', function () {
        $('#SigninUserInfoDropdown').toggleClass('expanded');
        return false;
    });

    $(document).click(function (e) {
        var container = $("#SigninUserInfoDropdown");
        var clickelemnt = $(".sign-in-area");
        if (!container.is(e.target) // if the target of the click isn't the container...
            && container.has(e.target).length === 0 // ... nor a descendant of the container...
            && clickelemnt.has(e.target).length === 0) // ... nor a signin Click...
        {
            container.removeClass('expanded');
        } else {
            clickelemnt.addClass('dropdown-active');
        }
    });
</script> 
     </section> 
     <section class="master-main"> 
      <div class="row main-row"> 
       <div id="InfoColumn" class="large-2 medium-3 columns info-column"> 
        <header id="header" class="article-nav"> 
         <div class="widget-IssueInfo widget-instance-ARVO_IssueInfo_Article"> 
          <div id="issueInfo-ARVO_IssueInfo_Article"> 
           <a href="/issues.aspx?issueid=932856"><img id="issueFallbackImage" class="fb-featured-image" src="/Images/fallbackCovers/170.jpg"></a> 
           <div class="ii-pub-date">
             April 2008 
           </div> 
           <div class="ii-vol-issue">
             Volume 8, Issue 4 
           </div> 
          </div> 
         </div> 
         <div class="widget-ArticleNavLinks widget-instance-ARVO_ArticleNavLinks_Widget"> 
          <ul class="inline-list"> 
           <li class="first prev"> <a href="/article.aspx?articleid=2122085">‹</a> </li> 
           <li class="middle tableofcontents bdlr"> <a href="http://jov.arvojournals.org/issue.aspx?issueid=932856&amp;journalid=178">Issue</a> </li> 
           <li class="last next"> <a href="/article.aspx?articleid=2122123">›</a> </li> 
          </ul> 
         </div> 
        </header> 
        <section id="LeftNavSticker"> 
         <div class="widget-ArticleJumpLinks widget-instance-ARVO_ArticleJumpLinks_Widget"> 
          <h6 class="jumplinks-heading"><i class="icon-jumpto"></i>Jump To...</h6> 
          <ul data-magellan-expedition="fixed"> 
           <li class="section-jump-link head-1" data-magellan-arrival="133847195"><a class="scrollTo" href="#133847195">Introduction</a></li> 
           <li class="section-jump-link head-1" data-magellan-arrival="133847204"><a class="scrollTo" href="#133847204">Experiment 1</a></li> 
           <li class="section-jump-link head-1" data-magellan-arrival="133847222"><a class="scrollTo" href="#133847222">Experiment 2</a></li> 
           <li class="section-jump-link head-1" data-magellan-arrival="133847238"><a class="scrollTo" href="#133847238">General discussion</a></li> 
           <li class="section-jump-link head-1" data-magellan-arrival="133847244"><a class="scrollTo" href="#133847244">Conclusions</a></li> 
           <li class="section-jump-link head-1" data-magellan-arrival="133847246"><a class="scrollTo" href="#133847246">Acknowledgments</a></li> 
           <li class="section-jump-link head-1" data-magellan-arrival="133847252"><a class="scrollTo" href="#133847252">References</a></li> 
          </ul> 
         </div> 
        </section> 
       </div> 
       <div class="link-to-top tab-up"> 
        <div class="link-top-circle"> 
         <i class="icon-angle-up"></i> 
        </div> 
       </div> 
       <div id="ContentColumn" class="large-7 medium-9 columns content-column"> 
        <div class="article-top-info"> 
         <div class="widget-ArticleTopInfo widget-instance-ARVO_ArticleTopInfo_Widget"> 
          <div class="module-widget article-top-widget"> 
           <div class="access-state-logos all-viewports"> 
            <span class="article-accessType all-viewports left-flag FreeAccess">Free</span> 
           </div> 
           <div class="widget-items"> 
            <span class="wi-clientType large-view-only">Research Article</span>
            <span class="wi-pub-date large-view-only">&nbsp;&nbsp;|&nbsp;&nbsp; April 2008</span>
            <div class="wi-article-title article-title-main">
             Looking as if you know: Systematic object inspection precedes object recognition 
            </div> 
            <div class="wi-authors"> 
             <div class="al-authors-list"> 
              <span class="wi-fullname brand-fg"><a href="solr/searchresults.aspx?author=Linus+Holm">Linus Holm</a></span>; 
              <span class="wi-fullname brand-fg"><a href="solr/searchresults.aspx?author=Johan+Eriksson">Johan Eriksson</a></span>; 
              <span class="wi-fullname brand-fg"><a href="solr/searchresults.aspx?author=Linus+Andersson">Linus Andersson</a></span> 
             </div> 
            </div> 
            <div class="ww-authorAffiliations"> 
             <div class="toggle-affiliation ati-toggle-trigger"> 
              <i id="toggleAffiliationIcon" class="icon-plus brand-fg"></i> 
              <span class="wi-title">Author Affiliations</span> 
             </div> 
             <ul class="wi-affiliationList ati-toggle-content hide"> 
              <li> 
               <div class="wi-fullname">
                Linus Holm
               </div> 
               <div class="wi-affiliations">
                Department of Psychology, Umeå University, Umeå, Sweden
                <a href="mailto:linus.holm@psy.umu.se" target="_blank">linus.holm@psy.umu.se</a>
               </div> </li> 
              <li> 
               <div class="wi-fullname">
                Johan Eriksson
               </div> 
               <div class="wi-affiliations">
                Department of Psychology, Umeå University, Umeå, Sweden
                <a href="mailto:johan.eriksson@psy.umu.se" target="_blank">johan.eriksson@psy.umu.se</a>
               </div> </li> 
              <li> 
               <div class="wi-fullname">
                Linus Andersson
               </div> 
               <div class="wi-affiliations">
                Department of Psychology, Umeå University, Umeå, Sweden
                <a href="mailto:linus.andersson@psy.umu.se" target="_blank">linus.andersson@psy.umu.se</a>
               </div> </li> 
             </ul> 
            </div> 
            <div class="widget-ArticleLinks widget-instance-ArticleTopInfo_ArticleLinks"> 
            </div> 
            <div class="ww-citation large-view-only"> 
             <span class="journal-name">Journal of Vision</span>
             <span> April 2008, Vol.8, 14. doi:10.1167/8.4.14</span> 
            </div> 
           </div> 
           <div class="widget-ArticleTopLinks widget-instance-ArticleTopInfo_ArticleTopLinks"> 
           </div> 
          </div> 
          <script>
    $(document).ready(function () {
        $('.article-top-widget').on('click', '.ati-toggle-trigger', function () {
            $(this).find('.icon-plus, .icon-minus').toggleClass('icon-minus icon-plus');
            $(this).siblings('.ati-toggle-content').toggleClass('hide');
        });

        // In Chrome, an anchor tag with target="_blank" and a "mailto:" href opens a new tab/window as well as the email client
        // I suspect this behavior will be corrected in the future
        // Remove the target="_blank"
        $('ul.wi-affiliationList').find('a[href^="mailto:"]').each(function () {
            $(this).removeAttr('target');
        });
    });
</script> 
         </div> 
        </div> 
        <ul id="Toolbar"> 
         <li class="toolbar-item item-with-dropdown"> <a class="toolbox-dropdown" data-dropdown="ViewsDrop"> <i class="icon-views"></i><span>Views</span><i class="icon-caret-down"></i> </a> 
          <ul id="ViewsDrop" class="f-dropdown" data-dropdown-content> 
           <li class="article-tab"><a class="tab-item" data-tab="#ContentTab"><i class="icon-fulltext"></i><span>Full Article</span></a></li> 
           <li class="figure-tab"><a class="tab-item" data-tab="#FigureTab"><i class="icon-photo"></i><span>Figures</span></a></li> 
           <li class="table-tab"><a class="tab-item" data-tab="#TableTab"><i class="icon-table"></i><span>Tables</span></a></li> 
          </ul> </li> 
         <li class="toolbar-item toolbar-pdf"> <a id="pdfLink" class="al-link pdf article-pdfLink pdfaccess" data-article-id="2122108" data-article-url="/data/journals/jov/932856/jov-8-4-14.pdf" data-ajax-url="/Content/CheckPdfAccess"> <i class="icon-file-pdf"></i><span>PDF</span> </a> </li> 
         <li class="toolbar-item"> <a class="toolbox-dropdown" id="share-icon" data-dropdown="ShareDrop"> <i class="icon-share"></i><span>Share</span><i class="icon-caret-down"></i> </a> 
          <ul id="ShareDrop" class="addthis_toolbox addthis_default_style addthis_20x20_style f-dropdown" data-dropdown-content> 
           <li><a class="addthis_button_email"><i class="icon-email"></i><span>E-mail</span></a></li> 
           <li><a class="addthis_button_facebook"><i class="icon-facebook"></i><span>Facebook</span></a></li> 
           <li><a class="addthis_button_twitter"><i class="icon-twitter"></i><span>Twitter</span></a></li> 
           <li><a class="addthis_button_google"><i class="icon-google"></i><span>Google</span></a></li> 
           <li><a class="addthis_button_digg"><i class="icon-digg"></i><span>Digg</span></a></li> 
           <li><a class="addthis_button_delicious"><i class="icon-delicious"></i><span>Delicious</span></a></li> 
           <li><a class="addthis_button_citeulike"><i class="icon-citeulike"></i><span>CiteULike</span></a></li> 
           <li><a class="addthis_button_tumblr"><i class="icon-tumblr"></i><span>Tumblr</span></a></li> 
           <li><a class="addthis_button_stumbleupon"><i class="icon-stumbleupon"></i><span>StumbleUpon</span></a></li> 
          </ul> </li> 
         <li class="toolbar-item last"> <a class="toolbox-dropdown" id="settings-icon" data-dropdown="otherToolsDrop"> <i class="icon-tools"></i><span>Tools</span><i class="icon-caret-down"></i> </a> 
          <ul id="otherToolsDrop" class="f-dropdown" data-dropdown-content> 
           <li>
            <div id="toolboxGetAlertsWidget"> 
             <a id="revealGetAlertsARVO_Get_Alerts" class="getAlertsLink" onclick="openModal(this)" data-reveal="ARVO_Get_Alerts"><i class="icon-envelope-alt"></i><span>Alerts</span></a> 
             <div id="getAlertsARVO_Get_Alerts" class="reveal-modal getAlertsModal" data-reveal="ARVO_Get_Alerts"> 
              <div class="m-alerts-title modal-title">
               User Alerts
              </div> 
              <div class="m-alerts-alert-for modal-p">
               You are adding an alert for:
              </div> 
              <div class="modal-resource-title modal-p">
               Looking as if you know: Systematic object inspection precedes object recognition 
              </div> 
              <div class="m-alerts-prompt modal-p">
                You will receive an email whenever this article is corrected, updated, or cited in the literature. You can manage this and all other alerts in 
               <a href="http://jov.arvojournals.org/Account/myaccount.aspx">My Account</a> 
              </div> 
              <div class="m-alerts-email-address modal-p">
               The alert will be sent to: 
               <a href="mailto:"></a>
              </div> 
              <input ID="UserEmailARVO_Get_Alerts" id="UserEmail" name="UserEmail" type="hidden" value=""> 
              <div> 
               <input ID="CurrentUserEmailARVO_Get_Alerts" id="CurrentUserEmail" name="CurrentUserEmail" type="hidden" value=""> 
               <input ID="IsUserAlreadySubscribedARVO_Get_Alerts" id="IsUserAlreadySubscribed" name="IsUserAlreadySubscribed" type="hidden" value="False"> 
               <input ID="SuccessfulUpdateARVO_Get_Alerts" id="SuccessfulUpdate" name="SuccessfulUpdate" type="hidden" value=""> 
               <input ID="ResourceIdARVO_Get_Alerts" id="ResourceId" name="ResourceId" type="hidden" value="2122108"> 
               <input ID="ResourceTypeARVO_Get_Alerts" id="ResourceType" name="ResourceType" type="hidden" value="Article"> 
               <input ID="ResourceTitleARVO_Get_Alerts" id="ResourceTitle" name="ResourceTitle" type="hidden" value="Looking as if you know: Systematic object inspection precedes object recognition"> 
               <input ID="AlertTypeARVO_Get_Alerts" id="AlertType" name="AlertType" type="hidden" value="Article"> 
               <input ID="IsAccessPermittedARVO_Get_Alerts" id="IsAccessPermitted" name="IsAccessPermitted" type="hidden" value="False"> 
               <input ID="ShowSaveSearchToUnauthenticatedUserARVO_Get_Alerts" id="ShowSaveSearchToUnauthenticatedUser" name="ShowSaveSearchToUnauthenticatedUser" type="hidden" value="False"> 
               <input ID="UserIsLoggedInARVO_Get_Alerts" id="UserIsLoggedIn" name="UserIsLoggedIn" type="hidden" value="False"> 
               <input ID="CurrentAlertContextSiteIdARVO_Get_Alerts" id="CurrentAlertContextSiteId" name="CurrentAlertContextSiteId" type="hidden" value="170"> 
               <input ID="ModalDivIDARVO_Get_Alerts" id="InstanceNameForModalDivID" name="InstanceNameForModalDivID" type="hidden" value="WidgetNoAccessModalSmallARVO_Get_Alerts"> 
               <div id="getAlertsConfirmationARVO_Get_Alerts"> 
                <div id="m-alerts-email-spinner-ARVO_Get_Alerts" class="css3preloader loaderWrap hide"> 
                 <div class="loading alertsSpinner"></div> 
                </div> 
                <a class="m-alerts-submit" id="confirmSaveChangesARVO_Get_Alerts" onclick="confirm(this)" data-reveal="ARVO_Get_Alerts">Confirm</a> 
               </div> 
              </div> 
              <a class="close-reveal-modal">×</a> 
             </div> 
             <div id="WidgetNoAccessModalSmallARVO_Get_Alerts" class="reveal-modal " data-reveal> 
              <h6>This feature is available to Subscribers Only</h6> 
              <a href="#" data-reveal-id="SignInModal" data-reveal>Sign In</a> or 
              <a id="aCreateAccount" target="_blank" class="brand-fg" href="https://www.arvo.org/Login/?ReturnUrl=/">Create an Account</a> 
              <a class="close-reveal-modal">×</a> 
             </div> 
            </div> <script type="text/javascript">

    function openModal(el)
    {
        var instanceName = $(el).attr("data-reveal");
        if ($("#IsAccessPermitted" + instanceName).length && !isTrue($("#IsAccessPermitted" + instanceName).val())) { // when user is not logged in, show no-access modal for non-free articles
            var divId = $("#ModalDivID" + instanceName).val();
            var alertsModal = $('#' + divId);
            if (alertsModal != undefined)
            {
                alertsModal.foundation('reveal', 'open');
            }
        } else {
            var alertsModal = $("#getAlerts" + instanceName);
            if (alertsModal != undefined)
            {
                alertsModal.foundation('reveal', 'open');
            }
        }
    }

    function confirm(el) {
        var instanceName = $(el).attr("data-reveal");
        $("#m-alerts-email-spinner-" + instanceName).removeClass("hide");
        $("#confirmSaveChanges" + instanceName).off('click');
        submitForm(instanceName);
    }

    function submitForm(instanceName)
    {
        var modelValues = {
            UserEmail: $('#UserEmail' + instanceName).val(),
            ResourceTitle: $('#ResourceTitle' + instanceName).val(),
            AlertType: $('#AlertType' + instanceName).val(),
            CurrentUserEmail: $('#CurrentUserEmail' + instanceName).val(),
            IsUserAlreadySubscribed: isTrue($('#IsUserAlreadySubscribed' + instanceName).val()),
            SuccessfulUpdate: $('#SuccessfulUpdate' + instanceName).val(),
            ResourceId: $('#ResourceId' + instanceName).val(),
            ResourceType: $('#ResourceType' + instanceName).val(),
            ShowSaveSearchToUnauthenticatedUser: isTrue($('#ShowSaveSearchToUnauthenticatedUser' + instanceName).val()),
            UserIsLoggedIn: isTrue($('#UserIsLoggedIn' + instanceName).val()),
            CurrentAlertContextSiteId: $('#CurrentAlertContextSiteId' + instanceName).val()
        };

        $.post("/Toolbox/ToolboxGetAlertsUpdateMessage", modelValues, function (data) {
            $('#getAlertsConfirmation' + instanceName).html(data);
        }).done(function () {
            $("#m-alerts-email-spinner" + instanceName).addClass("hide");
        });
    }

    function isTrue(input) {
        if (typeof input == 'string') {
            return input.toLowerCase() == 'true';
        }
        //use false as default if not string
        return false;
    }

</script> </li> 
           <li> 
            <div class="widget-ToolboxGetCitation widget-instance-ARVO_Get_Citation"> 
             <a href="#" data-reveal-id="getCitation" data-reveal><i class="icon-read-more"></i><span>Get Citation</span></a> 
             <div id="getCitation" class="reveal-modal" data-reveal> 
              <div class="modal-title">
               Citation
              </div> 
              <p>Linus Holm, Johan Eriksson, Linus Andersson; Looking as if you know: Systematic object inspection precedes object recognition. <em>Journal of Vision</em> 2008;8(4):14. doi: 10.1167/8.4.14.</p> 
              <p class="citation-label">Download citation file:</p> 
              <ul> 
               <li><a href="downloadCitation.aspx?format=ris&amp;articleid=2122108">RIS (Zotero)</a></li> 
               <li><a href="downloadCitation.aspx?format=ris&amp;articleid=2122108">EndNote</a></li> 
               <li><a href="downloadCitation.aspx?format=bibtex&amp;articleid=2122108">BibTex</a></li> 
               <li><a href="downloadCitation.aspx?format=txt&amp;articleid=2122108">Medlars</a></li> 
               <li><a href="downloadCitation.aspx?format=ris&amp;articleid=2122108">ProCite</a></li> 
               <li><a href="downloadCitation.aspx?format=txt&amp;articleid=2122108">RefWorks</a></li> 
               <li><a href="downloadCitation.aspx?format=ris&amp;articleid=2122108">Reference Manager</a></li> 
              </ul> 
              <hr> 
              <p>© ARVO (1962-2015); The Authors (2016-present)</p> 
              <a class="close-reveal-modal">×</a> 
             </div> 
            </div> </li> 
           <li> 
            <div class="widget-ToolboxPermissions widget-instance-"> 
             <div class="module-widget"> 
              <a href="http://www.copyright.com/openurl.do?issn=15347362" id="PermissionsLink" class="" target="_blank"><i class="icon-permissions"></i><span>Get Permissions</span></a> 
             </div> 
            </div> </li> 
          </ul> </li> 
         <li id="data-Supplement" class="toolbar-item supplement-tab"> <a class="tab-item" data-tab="#SupplementTab"><i class="icon-supplemental"></i><span>Supplements</span></a> </li> 
         <li class="toolbar-item toolbar-search" data-target="#Search"> <a><i class="icon-search"></i></a> </li> 
        </ul> 
        <section class="tabs-content"> 
         <div id="ContentTab" class="content active"> 
          <div class="widget-ArticleFulltext widget-instance-ARVO_ArticleFullText_Widget"> 
           <div class="module-widget"> 
            <div class="widget-items" data-widgetname="ArticleFulltext"> 
             <a id="133847194"></a> 
             <div class="content-section clearfix"> 
              <section class="abstract">
               <div class="h6 abstract-title">
                Abstract
               </div> 
               <p class="para">Sometimes we seem to look at the very object we are searching for, without consciously seeing it. How do we select object relevant information before we become aware of the object? We addressed this question in two recognition experiments involving pictures of fragmented objects. In <a href="#S2" class="sectionLink">Experiment 1</a>, participants preferred to look at the target object rather than a control region 25 fixations prior to explicit recognition. Furthermore, participants inspected the target as if they had identified it around 9 fixations prior to explicit recognition. In <a href="#S3" class="sectionLink">Experiment 2</a>, we investigated the influence of semantic knowledge in guiding object inspection prior to explicit recognition. Consistently, more specific knowledge about target identity made participants scan the fragmented stimulus more efficiently. For instance, non-target regions were rejected faster when participants knew the target object's name. Both experiments showed that participants were looking at the objects as if they knew them before they became aware of their identity.</p>
              </section> 
             </div> 
             <a id="s1"></a> 
             <a id="133847195"></a> 
             <div class="h6" data-magellan-destination="133847195">
              Introduction
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847196"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               At one instance, there is just a bunch of objects in the refrigerator, in the next, you clearly discern the package of milk. Do we systematically inspect the object before we become aware of its presence, or do we see it the moment we lay eyes on it? Several object recognition studies suggest there is little need for systematic inspection in object recognition. For instance, Potter and Faulconer (
               <a reveal-id="bib23" class="revealLink refLink">1975</a>) showed that humans can identify objects presented for only 50 ms in rapid serial visual streams (i.e., one stimulus picture immediately follows the next). Consistently, eye movement studies of ambiguous figures indicate that attention and perception are tightly linked to the extent that it remains unclear which is cause and effect (Ellis &amp; Stark, 
               <a reveal-id="bib4" class="revealLink refLink">1978</a>; Ito et al., 
               <a reveal-id="bib14" class="revealLink refLink">2003</a>; Pomplun, Ritter, &amp; Velichovsky, 
               <a reveal-id="bib22" class="revealLink refLink">1996</a>). Furthermore, in visual search studies, participants generally fixate the target at the instant they overtly indicate its presence with a button press. It has even been suggested that detection and recognition are parts of the same process (Grill-Spector &amp; Kanwisher, 
               <a reveal-id="bib8" class="revealLink refLink">2005</a>).&nbsp;
              </div> 
             </div> 
             <a id="133847197"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Although detection and explicit recognition can be closely related in some instances (Grill-Spector &amp; Kanwisher 
               <a reveal-id="bib8" class="revealLink refLink">2005</a>), more difficult situations might tease them apart. Indeed, a few studies using more complex stimulus (McCarley, Kramer, Wickens, Vidoni, &amp; Boot, 
               <a reveal-id="bib18" class="revealLink refLink">2004</a>; Pomplun et al., 
               <a reveal-id="bib22" class="revealLink refLink">1996</a>) indicated that explicit recognition and deployment of attentional resources can be dissociated to some extent. If the information gained from each fixation is poor, it seems reasonable to aggregate it across fixations to make a more accurate recognition decision.&nbsp;
              </div> 
             </div> 
             <a id="133847198"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               The primary objective of this study was to test the hypothesis that information is gradually accumulated before explicit object recognition. For this purpose, we registered eye movements in a fragmented object recognition task. Fragmented objects are well suited for this purpose because there is no immediate pop-out of the target following presentation and there is only one single transition in perceptual awareness of the target object (i.e., explicit recognition). In addition, the change in perceptual awareness seems to occur very fast (see 
               <a reveal-id="fig1" class="revealLink tablelink">Figure 1</a>), and similar perceptual transitions in ambiguous figures have been shown to occur within a few 100 ms. (Kornmeier &amp; Bach, 
               <a reveal-id="bib16" class="revealLink refLink">2004</a>). Therefore, systematic inspection of the object preceding explicit object recognition should reflect selection processes at a stage where the observer is yet agnostic about the object's identity. That is, systematic eye movements on the object prior to recognition should not reflect a gradual build up of perceptual awareness because the transition in awareness is expected to be instantaneous (see also Eriksson, Larsson, Riklund-Åhlström, &amp; Nyberg, 
               <a reveal-id="bib5" class="revealLink refLink">2004</a>, regarding neural correlates of the transition in visual awareness of fragmented objects).&nbsp;
              </div> 
             </div> 
             <a id="fig1"></a> 
             <a id="133847199"></a> 
             <div class="content-section clearfix"> 
              <div data-id="fig1" class="figure-section">
               <div class="title">
                <span class="label">Figure 1</span>
               </div>
               <div class="graphic-wrapper">
                <a reveal-id="fig1" class="revealLink figLink"><img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/m_jov-8-4-14-fig001.jpeg" alt="An example of an average difficult stimulus picture (55% of participants identified it in Experiment 1). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?" class="contentFigures lazy" path-from-xml="jov-8-4-14-fig001"></a>
                <div class="original-slide">
                 <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg" path-from-xml="jov-8-4-14-fig001" target="_blank">View Original</a>
                 <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg&amp;sec=133847199&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig001">Download Slide</a>
                </div>
               </div>
               <div class="caption">
                <div class="caption-legend">
                 <a id="" class="jumplink-placeholder">&nbsp;</a>
                 <div class="para">
                  An example of an average difficult stimulus picture (55% of participants identified it in 
                  <a href="#S2" class="sectionLink">Experiment 1</a>). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?
                 </div>
                </div>
               </div>
              </div>
              <div content-id="fig1" class="hide">
               <div class="figure-section">
                <div class="title">
                 <span class="label">Figure 1</span>
                 <div class="caption">
                  <div class="caption-legend">
                   <a id="" class="jumplink-placeholder">&nbsp;</a>
                   <div class="para">
                    An example of an average difficult stimulus picture (55% of participants identified it in 
                    <a href="#S2" class="sectionLink">Experiment 1</a>). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?
                   </div>
                  </div>
                 </div>
                </div>
                <div class="graphic-wrapper">
                 <img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg" alt="An example of an average difficult stimulus picture (55% of participants identified it in Experiment 1). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?" class="contentFigures lazy" path-from-xml="jov-8-4-14-fig001">
                 <div class="original-slide">
                  <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg" path-from-xml="jov-8-4-14-fig001" target="_blank">View Original</a>
                  <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg&amp;sec=133847199&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig001">Download Slide</a>
                 </div>
                </div>
               </div>
              </div> 
             </div> 
             <a id="133847200"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               When objects are not immediately discernible, the observer has to decide which information to select for further processing in order to recognize the content. Under such uncertain conditions, the observer might draw on prior knowledge to guide information selection. Recognition would then resemble hypothesis testing (Bar et al. 
               <a reveal-id="bib1" class="revealLink refLink">2006</a>; Deco &amp; Shürmann, 
               <a reveal-id="bib2" class="revealLink refLink">2000</a>; Kersten, Mamassian, &amp; Yuille, 
               <a reveal-id="bib15" class="revealLink refLink">2004</a>; Torralba, Oliva, Castelhano, &amp; Henderson, 
               <a reveal-id="bib28" class="revealLink refLink">2006</a>) with the eyes. Provided the observer has the right hypothesis, he would structure the visual input by moving his eyes to diagnostic regions for confirmation. The structured information hence accumulated would then be adequate for determining the objects' identity.&nbsp;
              </div> 
             </div> 
             <a id="133847201"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               A secondary objective of this study was to test whether eye movements are guided by prior knowledge in object recognition. Decisions about where to look next could be influenced by perceptual information such as gestalt properties. Alternatively, eye movements might be guided by semantic knowledge (e.g., looking for horse-like shapes in the picture). For instance, when the target's identity is uncertain, one might expect extended periods of viewing in limited regions due to increased iteration between perception and memory (e.g., testing different semantic interpretations of the object). Instead, when one knows the identity of the object beforehand, a more sweeping pattern of eye movements would be expected, reflecting search in the scene rather than in memory (see also Oliva, Wolfe, &amp; Arsenio, 
               <a reveal-id="bib21" class="revealLink refLink">2004</a>).&nbsp;
              </div> 
             </div> 
             <a id="133847202"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               In 
               <a href="#S2" class="sectionLink">Experiment 1</a>, we tested the hypothesis that information is systematically sampled prior to recognition in a fragmented object recognition test. If object recognition is a function of progressive object information accumulation, then eye movements should increasingly fixate the object prior to the recognition decision. Alternatively, recognition decisions might be independent of preceding information, and there is no increase in object fixations prior to recognition.&nbsp;
              </div> 
             </div> 
             <a id="133847203"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               In 
               <a href="#S3" class="sectionLink">Experiment 2</a>, we tested the influence of prior knowledge on information acquisition by manipulating the accuracy of prior target information. If object recognition is characterized by hypothesis testing, and semantic information affects perception (see e.g., Dolan et al., 
               <a reveal-id="bib3" class="revealLink refLink">1997</a>; Vecera &amp; Farah, 
               <a reveal-id="bib29" class="revealLink refLink">1997</a>) and attention deployment (Moores, Laiti, &amp; Chelazzi, 
               <a reveal-id="bib20" class="revealLink refLink">2003</a>), then accurate prior knowledge should facilitate object recognition by guiding the eyes to diagnostic object regions.&nbsp;
              </div> 
             </div> 
             <a id="s2"></a> 
             <a id="133847204"></a> 
             <div class="h6" data-magellan-destination="133847204">
              Experiment 1
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847205"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               In 
               <a href="#S2" class="sectionLink">Experiment 1</a>, we tested the hypothesis that explicit recognition (i.e., the point in time when participants become aware of the objects presence) is preceded by systematic object information selection in a fragmented object recognition test. Explicit recognition should be preceded by eye movements on the object resembling the scan pattern exhibited after explicit recognition. In other words, participants should look at the object as if they had recognized it before they explicitly recognize it. Systematic information selection would be expressed as consecutive eye movements on the object.&nbsp;
              </div> 
             </div> 
             <a id="ss1"></a> 
             <a id="133847206"></a> 
             <div class="h7" data-magellan-destination="133847206">
              Method
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="sss1"></a> 
             <a id="133847207"></a> 
             <div class="h8" data-magellan-destination="133847207">
              Participants
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847208"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Twenty-one Umeå University students participated in the experiment (13 females). They were between 19 and 32 years old ( 
               <em>M</em> = 24.8). Participants had normal or corrected-to-normal visual acuity (with contact lenses) and color vision (assessed by the Ishihara color test).&nbsp;
              </div> 
             </div> 
             <a id="sss2"></a> 
             <a id="133847209"></a> 
             <div class="h8" data-magellan-destination="133847209">
              Stimulus
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847210"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               A set of 20 fragmented animal pictures were selected from the Eriksson et al. (
               <a reveal-id="bib5" class="revealLink refLink">2004</a>) study. The pictures consisted of four footed animals (16), birds (3), and other (snail, 1). In order to avoid confounding target fixations with general tendencies to favor central regions in scenes (Mannan, Ruddock, &amp; Wooding, 
               <a reveal-id="bib17" class="revealLink refLink">1997</a>), the fragmented animals were positioned to the right or the left in the picture (see 
               <a reveal-id="fig1" class="revealLink tablelink">Figure 1</a>). In the other half of the picture, a 180° or 90° reoriented copy of the first half was inserted, but with the animal-related fragments altered to make recognition impossible (see 
               <a reveal-id="fig1" class="revealLink tablelink">Figure 1</a>). This way, each half of a stimulus contained similar basic level visual parameters. Each animal was defined by one of three colors (blue, brown, or green). The animals covered on average 9% (
               <em>SD</em> = 2%) of the screen area. Five different lists of presentation order were created.&nbsp;
              </div> 
             </div> 
             <a id="sss3"></a> 
             <a id="133847211"></a> 
             <div class="h8" data-magellan-destination="133847211">
              Apparatus
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847212"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               An EyeLink I eye-tracker (SR Research Ltd.) with a sampling rate of 250 Hz and a spatial precision of approximately 0.5 degrees was used to monitor the subjects' eye movements. An eye movement was defined as a saccade when its distance exceeded .15° or its velocity reached 35°/s or when its acceleration reached 9500°/s 
               <sup>2</sup>. The pictures were presented in 800 × 600 pixel resolution on a 19-in. CRT screen with an 85-Hz refresh rate.&nbsp;
              </div> 
             </div> 
             <a id="sss4"></a> 
             <a id="133847213"></a> 
             <div class="h8" data-magellan-destination="133847213">
              Procedure
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847214"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               The participants were seated approximately 50 cm away from the computer screen with a chin support which provided constant viewing distance and reduced head movement. They were instructed to identify fragmented line drawings of animals. Before test, participants were shown an example of the stimuli and had to search for the target animal. All participants managed to identify the animal after the experimenter had disclosed its color and location.&nbsp;
              </div> 
             </div> 
             <a id="133847215"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               At test, each of the twenty stimulus pictures was presented for 30 s. Each stimulus picture was only presented once to every participant. Participants were told to identify the animal as quickly and accurately as possible. Participants indicated their recognition by pressing a button on a response box. After recognition, participants were instructed to keep looking at the target until the picture disappeared. After picture removal, participants reported the animal.&nbsp;
              </div> 
             </div> 
             <a id="ss2"></a> 
             <a id="133847216"></a> 
             <div class="h7" data-magellan-destination="133847216">
              Results and discussion
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847217"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               One participant was excluded from analysis, having misinterpreted the instructions. The average performance of the remaining 20 participants was 10.2 items recognized ( 
               <em>SD</em> = 3.62) of 20 possible. Participants recognized the animals on average after 14.4 s ( 
               <em>SD</em> = 2.5) corresponding to 44.1 fixations ( 
               <em>SD</em> = 11.3). All reported items were correctly identified, except for two responses (one participant identified a snail as an elephant and a gorilla as a dog), corresponding to 1% of responses.&nbsp;
              </div> 
             </div> 
             <a id="133847218"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               To analyze eye movement data, the target region of each stimulus was first defined by a polygon circumscribing the outlines of the animal with an eccentricity corresponding to 0.5° (see yellow outline in 
               <a reveal-id="fig2" class="revealLink tablelink">Figure 2</a>). For comparison, we created a control region of exactly the same size as the target polygon but mirror-reversed to the opposite half of the picture (see red outline in 
               <a reveal-id="fig2" class="revealLink tablelink">Figure 2</a>). The mirror control region hence constituted a spatial control region, although it did not contain any structured object information. First fixation in the target region after stimulus onset occurred on average 5.65 fixations after stimulus onset. The corresponding average for the control region was 6.82. The difference was not statistically reliable, 
               <em>t</em>(19) = 1.23, 
               <em>p</em> = .23, indicating that the target region did not stand out as perceptually salient over the control region.&nbsp;
              </div> 
             </div> 
             <a id="fig2"></a> 
             <a id="133847219"></a> 
             <div class="content-section clearfix"> 
              <div data-id="fig2" class="figure-section">
               <div class="title">
                <span class="label">Figure 2</span>
               </div>
               <div class="graphic-wrapper">
                <a reveal-id="fig2" class="revealLink figLink"><img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/m_jov-8-4-14-fig002.jpeg" alt="The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig002"></a>
                <div class="original-slide">
                 <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg" path-from-xml="jov-8-4-14-fig002" target="_blank">View Original</a>
                 <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg&amp;sec=133847219&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig002">Download Slide</a>
                </div>
               </div>
               <div class="caption">
                <div class="caption-legend">
                 <a id="" class="jumplink-placeholder">&nbsp;</a>
                 <div class="para">
                  The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
                 </div>
                </div>
               </div>
              </div>
              <div content-id="fig2" class="hide">
               <div class="figure-section">
                <div class="title">
                 <span class="label">Figure 2</span>
                 <div class="caption">
                  <div class="caption-legend">
                   <a id="" class="jumplink-placeholder">&nbsp;</a>
                   <div class="para">
                    The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
                   </div>
                  </div>
                 </div>
                </div>
                <div class="graphic-wrapper">
                 <img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg" alt="The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig002">
                 <div class="original-slide">
                  <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg" path-from-xml="jov-8-4-14-fig002" target="_blank">View Original</a>
                  <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg&amp;sec=133847219&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig002">Download Slide</a>
                 </div>
                </div>
               </div>
              </div> 
             </div> 
             <a id="133847220"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               As suggested by 
               <a reveal-id="fig2" class="revealLink tablelink">Figure 2</a>, the target region was increasingly fixated in favor of the control region, as participants approached explicit recognition. Data presented in 
               <a reveal-id="fig2" class="revealLink tablelink">Figure 2</a> were limited to the last 26 fixations prior to recognition because earlier data points would have been based on few observations (due to fast responses) and were hence less reliable. However, it seems like the proportion of target and control fixations would have converged on a similar proportion, had earlier fixation data been available. This conclusion is also justified by the finding that there was no significant difference in number of fixations before first entry into the target and control regions, respectively. The probability of fixating the target region rather than the control region was statistically significant at 25 fixations prior to recognition, 
               <em>t</em>(19) = 2.77, 
               <em>p</em> &lt; .05.&nbsp;
              </div> 
             </div> 
             <a id="133847221"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               As indicated by 
               <a reveal-id="fig2" class="revealLink tablelink">Figure 2</a>, participants reached ceiling for proportion of target fixations several fixations prior to explicit recognition. This suggests that participants had detected and systematically inspected the target region before they became aware of its semantic content. Consistently, the average number of consecutive fixations on target (termed “gaze” henceforth) preceding recognition was 9.2 ( 
               <em>SD</em> = 3.5).&nbsp;
              </div> 
             </div> 
             <a id="s3"></a> 
             <a id="133847222"></a> 
             <div class="h6" data-magellan-destination="133847222">
              Experiment 2
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847223"></a> 
             <div class="content-section clearfix"> 
              <div class="para"> 
               <a href="#S2" class="sectionLink">Experiment 1</a> showed that object information was systematically selected prior to explicit object recognition. Participants inspected the objects so closely before their overt recognition decisions, that it seems possible they were accurately hypothesis testing with their eyes, guided by visual characteristics such as gestalt properties or even by semantic interpretations. Ryan, Althoff, Whitlow, and Cohen (
               <a reveal-id="bib24" class="revealLink refLink">2000</a>) found that non-amnesic participants revisited changed scene regions more often than amnesic patients did, even when neither participant category was able to explicitly identify the change. Therefore, it seems like memory might guide the eyes to diagnostic regions even in the absence of target awareness. Similarly, Hollingworth, Williams, and Henderson (
               <a reveal-id="bib11" class="revealLink refLink">2001</a>) found prolonged gaze on changed target regions in a change blindness paradigm, even when participants were unaware of the change.&nbsp;
              </div> 
             </div> 
             <a id="133847224"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               To strongly test the hypothesis testing account of information selection, we manipulated participants' prior object knowledge. If information selection is guided by prior knowledge, more accurate knowledge should produce more efficient information selection. Specifically, when the observer has the right hypothesis regarding the objects' identity, there is less need to recruit long-term memory in recognition because the right representation is already retrieved. Therefore, visual features would be more efficiently interpreted with accurate object knowledge. This might be translated into shorter gazes, as the observer is able to reject or to confirm target presence in a region based on less exchange between perception and memory. Instead, under less accurate knowledge conditions, the observer has to compare the visual information to a large set of identities in semantic memory. This should produce more iteration between visual information selection and memory, as alternative object interpretations need to be ruled out. Hence, longer gazes to both control and target regions would be expected under less specific target knowledge.&nbsp;
              </div> 
             </div> 
             <a id="133847225"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               If only gestalt properties influenced eye movements, there should be no difference in gaze between conditions.&nbsp;
              </div> 
             </div> 
             <a id="ss3"></a> 
             <a id="133847226"></a> 
             <div class="h7" data-magellan-destination="133847226">
              Methods
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="sss5"></a> 
             <a id="133847227"></a> 
             <div class="h8" data-magellan-destination="133847227">
              Participants
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847228"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Sixteen Umeå University undergraduates (7 females) participated in the experiment for payment (approximately USD 6). They were between 21 and 32 years ( 
               <em>M</em> = 24.3) and had no prior experience of similar experiments. Participants had self reported normal or corrected-to-normal visual acuity (contact lenses) and color vision (assessed with the Ishihara color test).&nbsp;
              </div> 
             </div> 
             <a id="sss6"></a> 
             <a id="133847229"></a> 
             <div class="h8" data-magellan-destination="133847229">
              Stimulus
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847230"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               The stimulus set of 
               <a href="#S2" class="sectionLink">Experiment 1</a> was extended with 20 fragmented pictures from the pool of pictures used by Eriksson et al. (
               <a reveal-id="bib5" class="revealLink refLink">2004</a>). These additional items depicted non-living, man-made objects. The objects (number of items in parenthesis) consisted of furniture (3), vehicles (5), weapon (1), musical instruments (3) articles of clothing (4), tools (3), and other (anchor, 1). The new items were prepared in the same manner as in 
               <a href="#S2" class="sectionLink">Experiment 1</a>. The new target objects were of similar size to those of 
               <a href="#S2" class="sectionLink">Experiment 1</a>. Half of the items were presented following a text label indicating its superordinate category membership (e.g., animal), whereas the other half was preceded by its basic level name (e.g., monkey). Across participants, each item was presented equally often following the superordinate as the basic level labels. In addition to the items from Eriksson et al. (
               <a reveal-id="bib5" class="revealLink refLink">2004</a>), a set of ten practice stimulus items were created from Snodgrass and Vanderwart (
               <a reveal-id="bib25" class="revealLink refLink">1980</a>) along the same principles mentioned above.&nbsp;
              </div> 
             </div> 
             <a id="sss7"></a> 
             <a id="133847231"></a> 
             <div class="h8" data-magellan-destination="133847231">
              Procedure
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847232"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               The procedure of 
               <a href="#S2" class="sectionLink">Experiment 1</a> was used with the following exceptions. To maximize the number of recognized items and hence obtain more data, participants were given training in identifying fragmented objects on the practice set before test. At test, each fragmented picture was preceded by a label appearing for 3 s. Fragmented pictures were presented for 60 s, or until recognition as indicated with a button press. The pictures were presented for an additional 3 s after button press in order to sample data post recognition. After each trial, participants were instructed to report the identity, the position, and the color of the target item.&nbsp;
              </div> 
             </div> 
             <a id="ss4"></a> 
             <a id="133847233"></a> 
             <div class="h7" data-magellan-destination="133847233">
              Results and discussion
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847234"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               One participant abandoned the experiment due to a headache. Recognition scores for the remaining 15 participants were based on correct reports of name, color, and position of the target item. On average, participants correctly recognized 18.9 and 15.8 out of 20 items from the basic and the superordinate conditions, respectively. The difference in performance between the conditions was statistically reliable, 
               <em>t</em>(14) = 5.43, 
               <em>p</em> &lt; .001.&nbsp;
              </div> 
             </div> 
             <a id="133847235"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               The same overall pattern of results from 
               <a href="#S2" class="sectionLink">Experiment 1</a> was replicated in that participants increasingly favored the target region rather than the control region prior to explicit recognition (see 
               <a reveal-id="fig3" class="revealLink tablelink">Figure 3</a>). Furthermore, there was a steeper increase in proportion of target fixations in the basic condition compared to the superordinate condition, suggesting that specificity of prior target knowledge affected how participants scanned the stimulus. Specifically, it seems like recognition in the basic condition required less target information sampling than the superordinate condition.&nbsp;
              </div> 
             </div> 
             <a id="fig3"></a> 
             <a id="133847236"></a> 
             <div class="content-section clearfix"> 
              <div data-id="fig3" class="figure-section">
               <div class="title">
                <span class="label">Figure 3</span>
               </div>
               <div class="graphic-wrapper">
                <a reveal-id="fig3" class="revealLink figLink"><img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/m_jov-8-4-14-fig003.jpeg" alt="The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig003"></a>
                <div class="original-slide">
                 <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg" path-from-xml="jov-8-4-14-fig003" target="_blank">View Original</a>
                 <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg&amp;sec=133847236&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig003">Download Slide</a>
                </div>
               </div>
               <div class="caption">
                <div class="caption-legend">
                 <a id="" class="jumplink-placeholder">&nbsp;</a>
                 <div class="para">
                  The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
                 </div>
                </div>
               </div>
              </div>
              <div content-id="fig3" class="hide">
               <div class="figure-section">
                <div class="title">
                 <span class="label">Figure 3</span>
                 <div class="caption">
                  <div class="caption-legend">
                   <a id="" class="jumplink-placeholder">&nbsp;</a>
                   <div class="para">
                    The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
                   </div>
                  </div>
                 </div>
                </div>
                <div class="graphic-wrapper">
                 <img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg" alt="The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig003">
                 <div class="original-slide">
                  <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg" path-from-xml="jov-8-4-14-fig003" target="_blank">View Original</a>
                  <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg&amp;sec=133847236&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig003">Download Slide</a>
                 </div>
                </div>
               </div>
              </div> 
             </div> 
             <a id="133847237"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               The first gaze on the target regions averaged 3.70 and 5.66 fixations in the basic and the superordinate conditions respectively, 
               <em>t</em>(14) = 3.56, 
               <em>p</em> &lt; .01. First gaze towards the control region was 2.46 and 2.91 fixations for the basic and the superordinate conditions, respectively, 
               <em>t</em>(14) = 2.88, 
               <em>p</em> &lt; .05. That is, the control region was faster rejected when the observer had more specific target knowledge. Finally, the last gaze prior to recognition was 5.49 and 9.12 for the basic and the superordinate conditions, respectively, 
               <em>t</em>(14) = 4.08, 
               <em>p</em> &lt; .001.&nbsp;
              </div> 
             </div> 
             <a id="s5"></a> 
             <a id="133847238"></a> 
             <div class="h6" data-magellan-destination="133847238">
              General discussion
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847239"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               This study investigated how visual information is selected prior to explicit object recognition. Both experiments showed that the fragmented objects were inspected several fixations before explicit object recognition. In 
               <a href="#S2" class="sectionLink">Experiment 1</a>, the target region was favored over the similarly sized control region even 25 fixations before target recognition. Interestingly, participants often made saccades to the center of the fragmented target (see picture B in 
               <a reveal-id="fig2" class="revealLink tablelink">Figure 2</a>), which suggests that they had inferred the target prior to explicit recognition (Melcher &amp; Kowler, 
               <a reveal-id="bib19" class="revealLink refLink">1999</a>). Furthermore, both experiments showed that the target region was inspected very systematically before explicit recognition because target recognition was preceded by several consecutive target fixations. Specifically, objects were fixated as if the observer had recognized the objects around nine fixations (i.e., last gaze) prior to explicit object recognition in 
               <a href="#S2" class="sectionLink">Experiment 1</a>. An alternative account would be that participants were just slow to overtly respond following identification and continued to look at the object while preparing their response. However, that interpretation seems unlikely, considering that the time required for nine fixations is much longer than what even a generous estimate of reaction time would suggest.&nbsp;
              </div> 
             </div> 
             <a id="133847240"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               There was no immediate preference for the target region as compared to the similarly sized control region in 
               <a href="#S2" class="sectionLink">Experiment 1</a>; participants' fixated target and control regions equally fast after stimulus onset. This result is inconsistent with a perceptual saliency account of the systematic target inspection prior to recognition. Instead, the results indicate that the processes preceding explicit recognition are influenced by more complex visual information (e.g., gestalts) or hypotheses based on prior knowledge (e.g., semantic memory).&nbsp;
              </div> 
             </div> 
             <a id="133847241"></a> 
             <div class="content-section clearfix"> 
              <div class="para"> 
               <a href="#S3" class="sectionLink">Experiment 2</a> showed that prior target knowledge affected information selection as indicated by eye movements. With basic level knowledge about object identity, participants employed a more sweeping pattern of fixations across the stimulus pictures compared to superordinate object knowledge. Specifically, participants would both reject and confirm target presence more readily in the basic condition because gazes on both control and target regions were shorter compared to the superordinate condition. This finding is also consistent with knowledge facilitating the ability to group visual contour information (for a discussion, see Geisler, Perry, Super, &amp; Gallogly, 
               <a reveal-id="bib7" class="revealLink refLink">2001</a>).&nbsp;
              </div> 
             </div> 
             <a id="133847242"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               The difference in number of object fixations prior to recognition might reflect a response bias between the basic and the superordinate conditions (Hollingworth &amp; Henderson, 
               <a reveal-id="bib10" class="revealLink refLink">1998</a>). Specifically, with only superordinate information, participants could have been more conservative in responding and therefore made more fixations on the object before responding. However, this interpretation is not consistent with the finding that participants inspected the control region more meticulously in the superordinate condition.&nbsp;
              </div> 
             </div> 
             <a id="133847243"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Because the present study used highly artificial stimulus, its generalization to object recognition in more natural conditions is limited. However, even under more natural conditions, object recognition might involve several fixations and iterative information sampling. For instance, recognizing an animal in a tree or making fine within?category judgments such as finding the right utensil in the drawer seem to resemble the situation tested in the present experiments. Future experiments might investigate difficult recognition tasks in natural conditions.&nbsp;
              </div> 
             </div> 
             <a id="s6"></a> 
             <a id="133847244"></a> 
             <div class="h6" data-magellan-destination="133847244">
              Conclusions
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847245"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Collectively, our findings suggest that information can be accumulated across fixations prior to explicit object recognition. Furthermore, the findings support the view that information selection in object recognition is carried out in a hypothesis testing manner (Deco &amp; Schürman, 
               <a reveal-id="bib2" class="revealLink refLink">2000</a>; Friston, 
               <a reveal-id="bib6" class="revealLink refLink">2003</a>). Our results support recent findings on predictive coding (Friston, 
               <a reveal-id="bib6" class="revealLink refLink">2003</a>) in object recognition (Summerfield, Egner, et al. 
               <a reveal-id="bib26" class="revealLink refLink">2006</a>; Summerfield, Lepsien, Gitelman, Mesulam, &amp; Nobre, 
               <a reveal-id="bib27" class="revealLink refLink">2006</a>), suggesting that expectations guide attention in object recognition and visual search (see also Holm &amp; Mäntylä 
               <a reveal-id="bib13" class="revealLink refLink">2007</a>; Moores et al., 
               <a reveal-id="bib20" class="revealLink refLink">2003</a>). Importantly, predictive information sampling seems to precede explicit object recognition, suggesting that object recognition is a function of implicit inference (Helmholtz, 
               <a reveal-id="bib9" class="revealLink refLink">1910</a>). Therefore, we look as if we know before we know.&nbsp;
              </div> 
             </div> 
             <a id="133847246"></a> 
             <div class="h6" data-magellan-destination="133847246">
              Acknowledgments
             </div> 
             <div class="content-section clearfix"> 
             </div> 
             <a id="133847247"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               We are thankful for the valuable comments by Timo Mäntylä, Bruno Laeng, and Stephen Engel in the preparation of this manuscript. The findings described have been reported in a doctoral thesis (Holm, 
               <a reveal-id="bib12" class="revealLink refLink">2007</a>).&nbsp;
              </div> 
             </div> 
             <a id="133847248"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Commercial relationships: none.&nbsp;
              </div> 
             </div> 
             <a id="133847249"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Corresponding author: Linus Holm.&nbsp;
              </div> 
             </div> 
             <a id="133847250"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Email: linus.holm@psy.umu.se.&nbsp;
              </div> 
             </div> 
             <a id="133847251"></a> 
             <div class="content-section clearfix"> 
              <div class="para">
               Address: Department of Psychology, Umeå University, S-901 87 Umeå, Sweden.&nbsp;
              </div> 
             </div> 
             <a id="133847252"></a> 
             <div class="h6" data-magellan-destination="133847252">
              References
             </div> 
             <div class="content-section clearfix"> 
              <div class="reference-section">
               <div content-id="bib1">
                <div class="refRow  clearfix">
                 <div role="bib1" class="refContent ">
                   Bar, M. Kassam, K. S. Ghuman, A. S. Boschyan, J. Schmid, A. M. Dale, A. M. (2006). Top-down facilitation of visual recognition. 
                  <em>Proceedings of the National Academy of Science of the United States of America</em>, 103, 449–454. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=16407167&amp;ordinalpos=3&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>] [
                  <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pubmed&amp;pubmedid=16407167" target="_blank">Article</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1073/pnas.0507062103" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1073/pnas.0507062103"> </span>
                 </div>
                </div>
               </div>
               <div content-id="bib2">
                <div class="refRow  clearfix">
                 <div role="bib2" class="refContent ">
                   Deco, G. Schürmann, B. (2000). A neuro-cognitive visual system for object recognition based on testing of interactive attentional top-down hypotheses. 
                  <em>Perception</em>, 29, 1249–1264. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=11220215&amp;ordinalpos=2&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p3010" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1068/p3010"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11220215" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib3">
                <div class="refRow  clearfix">
                 <div role="bib3" class="refContent ">
                   Dolan, R. J. Fink, G. R. Rolls, E. Booth, M. Holmes, A. Franckowiak, R. S. (1997). How the brain learns to see objects and faces in an impoverished context. 
                  <em>Nature</em>, 389, 596–599. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=9335498&amp;ordinalpos=208&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1038/39309" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1038/39309"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/9335498" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib4">
                <div class="refRow  clearfix">
                 <div role="bib4" class="refContent ">
                   Ellis, S. R. Stark, L. (1978). Eye movements during the viewing of Necker cubes. 
                  <em>Perception</em>, 7, 575–581. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=733448&amp;ordinalpos=47&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p070575" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1068/p070575"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/733448" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib5">
                <div class="refRow  clearfix">
                 <div role="bib5" class="refContent ">
                   Eriksson, J. Larsson, A. Riklund-Åhlström, K. Nyberg, L. (2004). Visual consciousness: Dissociating the neural correlates of perceptual transitions from sustained perception with fMRI. 
                  <em>Consciousness and Cognition</em>, 13, 61–72. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=14990241&amp;ordinalpos=2&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/S1053-8100(03)00050-3" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1016/S1053-8100(03)00050-3"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/14990241" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib6">
                <div class="refRow  clearfix">
                 <div role="bib6" class="refContent ">
                   Friston, K. (2003). Learning and inference in the brain. 
                  <em>Neural Networks</em>, 16, 1325–1352. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=14622888&amp;ordinalpos=58&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.neunet.2003.06.005" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1016/j.neunet.2003.06.005"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/14622888" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib7">
                <div class="refRow  clearfix">
                 <div role="bib7" class="refContent ">
                   Geisler, W. S. Perry, J. S. Super, B. J. Gallogly, D. P. (2001). Edge co-occurrence in natural images predicts contour grouping performance. 
                  <em>Vision Research</em>, 41, 711–724. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=11248261&amp;ordinalpos=2&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/S0042-6989(00)00277-7" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1016/S0042-6989(00)00277-7"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11248261" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib8">
                <div class="refRow  clearfix">
                 <div role="bib8" class="refContent ">
                   Grill-Spector, K. Kanwisher, N. (2005). As soon as you know it is there, you know what it is. 
                  <em>Psychological Science</em>, 16, 152–160. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=15686582&amp;ordinalpos=6&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1111/psci.2005.16.issue-2" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1111/psci.2005.16.issue-2"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/15686582" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib9">
                <div class="refRow  clearfix">
                 <div role="bib9" class="refContent ">
                   Helmholtz, H. (1910). 
                  <em>Handbuch der physiologischen optik: Vol. 3</em>. Leipzig: Leopold Voss.
                 </div>
                </div>
               </div>
               <div content-id="bib10">
                <div class="refRow  clearfix">
                 <div role="bib10" class="refContent ">
                   Hollingworth, A. Henderson, J. M. (1998). Does consistent scene context facilitate object perception? 
                  <em>Journal of Experimental Psychology: General</em>, 127, 398–415. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=9857494&amp;ordinalpos=20&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1037/0096-3445.127.4.398" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1037/0096-3445.127.4.398"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/9857494" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib11">
                <div class="refRow  clearfix">
                 <div role="bib11" class="refContent ">
                   Hollingworth, A. Williams, C. C. Henderson, J. M. (2001). To see and remember: Visually specific information is retained in memory from previously attended objects in natural scenes. 
                  <em>Psychonomic Bulletin &amp; Review</em>, 8, 761–768. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=11848597&amp;ordinalpos=1&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVLinkOut" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.3758/BF03196215" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.3758/BF03196215"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11848597" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib12">
                <div class="refRow  clearfix">
                 <div role="bib12" class="refContent ">
                   Holm, L. (2007). 
                  <em>Predictive eyes precede retrieval: Visual recognition as hypothesis testing</em>.
                 </div>
                </div>
               </div>
               <div content-id="bib13">
                <div class="refRow  clearfix">
                 <div role="bib13" class="refContent ">
                   Holm, L. Mäntylä, T. (2007). Memory for scenes: Refixations reflect retrieval. 
                  <em>Memory &amp; Cognition</em>, 35, 1664–1674. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=18062544&amp;ordinalpos=1&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVLinkOut" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.3758/BF03193500" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.3758/BF03193500"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/18062544" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib14">
                <div class="refRow  clearfix">
                 <div role="bib14" class="refContent ">
                   Ito, J. Nikolaev, A. R. Luman, M. Aukes, M. F. Nakatani, C. van Leeuwen, C. (2003). Perceptual switching, eye movements, and the bus paradox. 
                  <em>Perception</em>, 32, 681–698. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=12892429&amp;ordinalpos=21&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p5052" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1068/p5052"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/12892429" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib15">
                <div class="refRow  clearfix">
                 <div role="bib15" class="refContent ">
                   Kersten, D. Mamassian, P. Yuille, A. (2004). Object perception as Bayesian inference. 
                  <em>Annual Review of Psychology</em>, 55, 271–304. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=14744217&amp;ordinalpos=4&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1146/annurev.psych.55.090902.142005" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1146/annurev.psych.55.090902.142005"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/14744217" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib16">
                <div class="refRow  clearfix">
                 <div role="bib16" class="refContent ">
                   Kornmeier, J. Bach, M. (2004). Early neural activity in Necker-cube reversal: Evidence for low-level processing of a gestalt phenomenon. 
                  <em>Psychophysiology</em>, 41, 1–8. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=14692995&amp;ordinalpos=42&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1046/j.1469-8986.2003.00126.x" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1046/j.1469-8986.2003.00126.x"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/14692995" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib17">
                <div class="refRow  clearfix">
                 <div role="bib17" class="refContent ">
                   Mannan, S. K. Ruddock, K. H. Wooding, D. S. (1997). Fixation patterns made during brief examination of two-dimensional images. 
                  <em>Perception</em>, 26, 1059–1072. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=9509164&amp;ordinalpos=5&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p261059" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1068/p261059"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/9509164" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib18">
                <div class="refRow  clearfix">
                 <div role="bib18" class="refContent ">
                   McCarley, J. S. Kramer, A. F. Wickens, C. D. Vidoni, E. D. Boot, W. R. (2004). Visual skills in airport-security screening. 
                  <em>Psychological Science</em>, 15, 302–306. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=15102138&amp;ordinalpos=2&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1111/psci.2004.15.issue-5" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1111/psci.2004.15.issue-5"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/15102138" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib19">
                <div class="refRow  clearfix">
                 <div role="bib19" class="refContent ">
                   Melcher, D. Kowler, E. (1999). Shapes, surfaces and saccades. 
                  <em>Vision Research</em>, 39, 2929–2946. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=10492819&amp;ordinalpos=2&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/S0042-6989(99)00029-2" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1016/S0042-6989(99)00029-2"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/10492819" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib20">
                <div class="refRow  clearfix">
                 <div role="bib20" class="refContent ">
                   Moores, E. Laiti, L. Chelazzi, L. (2003). Associative knowledge controls deployment of visual attention. 
                  <em>Nature Neuroscience</em>, 6, 182–189. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=12514738&amp;ordinalpos=3&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1038/nn996" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1038/nn996"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/12514738" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib21">
                <div class="refRow  clearfix">
                 <div role="bib21" class="refContent ">
                   Oliva, A. Wolfe, J. M. Arsenio, H. C. (2004). Panoramic search: The interaction of memory and vision in search through a familiar scene. 
                  <em>Journal of Experimental Psychology: Human Perception and Performance</em>, 30, 1132–1146. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=15584820&amp;ordinalpos=2&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1037/0096-1523.30.6.1132" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1037/0096-1523.30.6.1132"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/15584820" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib22">
                <div class="refRow  clearfix">
                 <div role="bib22" class="refContent ">
                   Pomplun, M. Ritter, H. Velichovsky, B. (1996). Disambiguating complex visual information: Towards communication of personal views of a scene. 
                  <em>Perception</em>, 25, 941–948. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=8938007&amp;ordinalpos=24&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p250931" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1068/p250931"> </span>
                 </div>
                </div>
               </div>
               <div content-id="bib23">
                <div class="refRow  clearfix">
                 <div role="bib23" class="refContent ">
                   Potter, M. C. Faulconer, B. A. (1975). Time to understand pictures and words. 
                  <em>Nature</em>, 253, 437–438. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=1110787&amp;ordinalpos=3&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1038/253437a0" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1038/253437a0"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/1110787" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib24">
                <div class="refRow  clearfix">
                 <div role="bib24" class="refContent ">
                   Ryan, J. D. Althoff, R. R. Whitlow, S. Cohen, N. J. (2000). Amnesia is a deficit in relational memory. 
                  <em>Psychological Science</em>, 11, 454–461. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=11202489&amp;ordinalpos=3&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1111/1467-9280.00288" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1111/1467-9280.00288"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11202489" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib25">
                <div class="refRow  clearfix">
                 <div role="bib25" class="refContent ">
                   Snodgrass, J. G. Vanderwart, M. (1980). A standardized set of 260 pictures: Norms for name agreement, image agreement and visual complexity. 
                  <em>Journal of Experimental Psychology: Human Learning and Memory</em>, 6, 174–215. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=7373248&amp;ordinalpos=2&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1037/0278-7393.6.2.174" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1037/0278-7393.6.2.174"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/7373248" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib26">
                <div class="refRow  clearfix">
                 <div role="bib26" class="refContent ">
                   Summerfield, C. Egner, T. Greene, M. Koechlin, E. Mangels, J. Hirsch, J. (2006). Predictive codes for forthcoming perception in the frontal cortex. 
                  <em>Science</em>, 314, 1311–1314. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=17124325&amp;ordinalpos=8&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1126/science.1132028" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1126/science.1132028"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/17124325" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib27">
                <div class="refRow  clearfix">
                 <div role="bib27" class="refContent ">
                   Summerfield, J. J. Lepsien, J. Gitelman, D. R. Mesulam, M. M. Nobre, A. C. (2006). Orienting attention based on long-term memory experience. 
                  <em>Neuron</em>, 49, 905–916. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=16543137&amp;ordinalpos=8&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>] [
                  <a href="http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WSS-4JGJF17-K&amp;_user=10&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;view=c&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=9e1fb9acaa823ed072b856f1b5839a20" target="_blank">Article</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.neuron.2006.01.021" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1016/j.neuron.2006.01.021"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/16543137" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib28">
                <div class="refRow  clearfix">
                 <div role="bib28" class="refContent ">
                   Torralba, A. Oliva, A. Castelhano, S. M. Henderson, J. M. (2006). Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search. 
                  <em>Psychological Review</em>, 113, 766–786. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=17014302&amp;ordinalpos=1&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.1037/0033-295X.113.4.766" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.1037/0033-295X.113.4.766"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/17014302" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
               <div content-id="bib29">
                <div class="refRow  clearfix">
                 <div role="bib29" class="refContent ">
                   Vecera, S. P. Farah, M. J. (1997). Is visual image segmentation a bottom-up or an interactive process. 
                  <em>Perception &amp; Psychophysics</em>, 59, 1280–1296. [
                  <a href="http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&amp;Cmd=ShowDetailView&amp;TermToSearch=9401461&amp;ordinalpos=4&amp;itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum" target="_blank">PubMed</a>]
                  <span class="crossrefDoi"> <a href="http://dx.doi.org/10.3758/BF03214214" target="_blank">[CrossRef]</a></span>
                  <span class="inst-open-url-holders" data-targetid="10.3758/BF03214214"> </span>
                  <span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/9401461" class="special sans" target="_blank">[PubMed]</a></span>
                 </div>
                </div>
               </div>
              </div> 
             </div> 
             <span id="UserHasAccess" data-userhasaccess="True"></span> 
            </div> 
           </div> 
           <script type="text/javascript">
    // For un-authenticated users, replace reveal-modal links with spans, since target of link is 99.99% likely to be missing
    $(document).ready(function() {
        var hasAccess = $('div[data-widgetname="ArticleFulltext"] span#UserHasAccess').attr('data-userHasAccess');
        if (hasAccess && hasAccess.length && hasAccess.toLowerCase() == 'false') {
            var revealModalLinks = $('div[data-widgetname="ArticleFulltext"] a[data-reveal-id]');
            revealModalLinks.each(function() {
                $(this).replaceWith(function() {
                    var anchorHtml = this.innerHTML;
                    return '<span>' + anchorHtml + '</span>';
                });
            });
        }
        // now call institutionopenurl 
        SCM.InstitutionOpenUrl.init();
    });
</script> 
          </div> 
         </div> 
         <div id="FigureTab" class="content"> 
          <div class="widget-ArticleFiguresAndTables widget-instance-ARVO_Article_Figures_Tab"> 
           <section class="figure-table-wrapper"> 
            <div data-id="fig1" class="figure-section">
             <div class="title">
              <span class="label">Figure 1</span>
             </div>
             <div class="graphic-wrapper">
              <a reveal-id="fig1" class="revealLink figLink"><img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/m_jov-8-4-14-fig001.jpeg" alt="An example of an average difficult stimulus picture (55% of participants identified it in Experiment 1). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?" class="contentFigures lazy" path-from-xml="jov-8-4-14-fig001"></a>
              <div class="original-slide">
               <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg" path-from-xml="jov-8-4-14-fig001" target="_blank">View Original</a>
               <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg&amp;sec=133847199&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig001">Download Slide</a>
              </div>
             </div>
             <div class="caption">
              <div class="caption-legend">
               <a id="" class="jumplink-placeholder">&nbsp;</a>
               <div class="para">
                An example of an average difficult stimulus picture (55% of participants identified it in 
                <a href="#S2" class="sectionLink">Experiment 1</a>). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?
               </div>
              </div>
             </div>
            </div>
            <div content-id="fig1" class="hide">
             <div class="figure-section">
              <div class="title">
               <span class="label">Figure 1</span>
               <div class="caption">
                <div class="caption-legend">
                 <a id="" class="jumplink-placeholder">&nbsp;</a>
                 <div class="para">
                  An example of an average difficult stimulus picture (55% of participants identified it in 
                  <a href="#S2" class="sectionLink">Experiment 1</a>). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?
                 </div>
                </div>
               </div>
              </div>
              <div class="graphic-wrapper">
               <img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg" alt="An example of an average difficult stimulus picture (55% of participants identified it in Experiment 1). Overall average performance for the stimuli was 51%. Can you find the blue horse to the left?" class="contentFigures lazy" path-from-xml="jov-8-4-14-fig001">
               <div class="original-slide">
                <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg" path-from-xml="jov-8-4-14-fig001" target="_blank">View Original</a>
                <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig001.jpeg&amp;sec=133847199&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig001">Download Slide</a>
               </div>
              </div>
             </div>
            </div> 
           </section> 
           <section class="figure-table-wrapper"> 
            <div data-id="fig2" class="figure-section">
             <div class="title">
              <span class="label">Figure 2</span>
             </div>
             <div class="graphic-wrapper">
              <a reveal-id="fig2" class="revealLink figLink"><img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/m_jov-8-4-14-fig002.jpeg" alt="The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig002"></a>
              <div class="original-slide">
               <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg" path-from-xml="jov-8-4-14-fig002" target="_blank">View Original</a>
               <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg&amp;sec=133847219&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig002">Download Slide</a>
              </div>
             </div>
             <div class="caption">
              <div class="caption-legend">
               <a id="" class="jumplink-placeholder">&nbsp;</a>
               <div class="para">
                The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
               </div>
              </div>
             </div>
            </div>
            <div content-id="fig2" class="hide">
             <div class="figure-section">
              <div class="title">
               <span class="label">Figure 2</span>
               <div class="caption">
                <div class="caption-legend">
                 <a id="" class="jumplink-placeholder">&nbsp;</a>
                 <div class="para">
                  The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
                 </div>
                </div>
               </div>
              </div>
              <div class="graphic-wrapper">
               <img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg" alt="The three stimulus pictures on top are overlaid with aggregated fixations from the ten participants who identified this particular item. Fixations in pictures A–C are reflected as dots, with an equal number of fixations in each picture. The yellow and red outlines define the target and control regions, respectively. Picture A illustrates fixations 16 to 9 before target recognition. Picture B illustrates the 8 to 1 fixations preceding target recognition, and picture C illustrates the 0 to 7 fixations following target recognition. The graph illustrates target and control region fixation probability as a function of fixation index. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig002">
               <div class="original-slide">
                <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg" path-from-xml="jov-8-4-14-fig002" target="_blank">View Original</a>
                <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig002.jpeg&amp;sec=133847219&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig002">Download Slide</a>
               </div>
              </div>
             </div>
            </div> 
           </section> 
           <section class="figure-table-wrapper"> 
            <div data-id="fig3" class="figure-section">
             <div class="title">
              <span class="label">Figure 3</span>
             </div>
             <div class="graphic-wrapper">
              <a reveal-id="fig3" class="revealLink figLink"><img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/m_jov-8-4-14-fig003.jpeg" alt="The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig003"></a>
              <div class="original-slide">
               <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg" path-from-xml="jov-8-4-14-fig003" target="_blank">View Original</a>
               <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg&amp;sec=133847236&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig003">Download Slide</a>
              </div>
             </div>
             <div class="caption">
              <div class="caption-legend">
               <a id="" class="jumplink-placeholder">&nbsp;</a>
               <div class="para">
                The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
               </div>
              </div>
             </div>
            </div>
            <div content-id="fig3" class="hide">
             <div class="figure-section">
              <div class="title">
               <span class="label">Figure 3</span>
               <div class="caption">
                <div class="caption-legend">
                 <a id="" class="jumplink-placeholder">&nbsp;</a>
                 <div class="para">
                  The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM.
                 </div>
                </div>
               </div>
              </div>
              <div class="graphic-wrapper">
               <img src="/Images/grey.gif" data-original="/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg" alt="The graph illustrates target and control region fixation probability as a function of fixation index for the basic and superordinate condition, respectively. Data are synchronized around recognition (RC, fixation index = 0). Error bars indicate SEM." class="contentFigures lazy" path-from-xml="jov-8-4-14-fig003">
               <div class="original-slide">
                <a section="[XSLTSectionID]" class="viewOriginalSlide" href="/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg" path-from-xml="jov-8-4-14-fig003" target="_blank">View Original</a>
                <a section="[XSLTSectionID]" href="/downloadimage.aspx?image=/data/Journals/JOV/932856/jov-8-4-14-fig003.jpeg&amp;sec=133847236&amp;ar=2122108&amp;imagename=" class="downloadSlide" path-from-xml="jov-8-4-14-fig003">Download Slide</a>
               </div>
              </div>
             </div>
            </div> 
           </section> 
          </div> 
         </div> 
         <div id="TableTab" class="content"> 
         </div> 
         <div id="SupplementTab" class="content"> 
         </div> 
        </section> 
        <div class="content articleCopyright text-center"> 
         <span id="BodyContent_PageContent_lblArticleCopyright"> 
          <div class="copyright-statement">
           © 2008 ARVO
          </div> </span> 
        </div> 
        <div class="inline-signin-module"> 
        </div> 
       </div> 
       <div id="SidebarColumn" class="large-3 medium-9 medium-push-3 large-push-0 end columns sidebar-column"> 
        <div class="sidebar-widget clearfix"> 
         <div class="widget-ArticleLevelMetrics widget-instance-ARVO_ArticleMetrics"> 
          <div class="artmet-condensed-wrap clearfix"> 
           <div class="artmet-condensed-stats clearfix"> 
            <div class="artmet-item artmet-views"> 
             <span class="artmet-number">558</span> 
             <span class="artmet-text">Views</span> 
            </div> 
            <div class="artmet-item artmet-citations"> 
             <span class="artmet-number"> <a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&amp;SrcApp=PARTNER_APP&amp;SrcAuth=LinksAMR&amp;KeyUT=WOS:000255976000014&amp;DestLinkType=CitingArticles&amp;DestApp=ALL_WOS&amp;UsrCustomerID=61f30d8ae69c46f86624c5f98a3bc13a" target="_blank">16</a> </span> 
             <span class="artmet-text">Citations</span> 
            </div> 
            <div class="artmet-item artmet-altmetric"> 
             <div class="widget-AltmetricLink widget-instance-ArticleLevelMetrics_AltmetricLinkSummary"> 
              <!-- Altmetrics --> 
              <div id="altmetricEmbedId" runat="server" class="altmetric-embed" data-link-target="_blank" data-badge-type="donut" data-hide-no-mentions="false" data-doi="10.1167/8.4.14"></div> 
              <script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> 
             </div> 
            </div> 
           </div> 
           <div class="artmet-modal-trigger-wrap clearfix"> 
            <a class="artmet-modal-trigger" data-article-id="2122108"><i class="icon-metrics"></i><span>View Metrics</span></a> 
           </div> 
          </div> 
          <div class="artmet-modal" id="MetricsModal"> 
           <div class="artmet-modal-contents"> 
            <a class="artmet-close-modal">×</a> 
           </div> 
          </div> 
         </div> 
        </div> 
        <div id="divSeeAlso" class="sidebar-widget clearfix"> 
         <div class="widget-ArticleLinks widget-instance-ARVO_SeeAlso_ArticleLinks"> 
         </div> 
        </div> 
        <div class="sidebar-widget clearfix"> 
         <div class="widget-EditorsChoice widget-instance-ARVO_Editors_Choice"> 
         </div> 
        </div> 
        <div class="sidebar-widget clearfix"> 
         <div class="widget-RelatedContent widget-instance-ARVO_Article_RelatedContent"> 
          <h4>Related Articles</h4> 
          <div class="article-title">
           <a href="http://jov.arvojournals.org/article.aspx?articleid=2549962">Breaking object correspondence across saccades impairs object recognition: The role of color and luminance</a>
          </div> 
          <div class="article-title">
           <a href="http://jov.arvojournals.org/article.aspx?articleid=2213253">Disentangling the effects of spatial inconsistency of targets and distractors when searching in realistic scenes</a>
          </div> 
          <div class="article-title">
           <a href="http://jov.arvojournals.org/article.aspx?articleid=2213233">Intrinsic and contextual features in object recognition</a>
          </div> 
          <div class="article-title">
           <a href="http://jov.arvojournals.org/article.aspx?articleid=2193914">Time to wave good-bye to phase scrambling: Creating controlled scrambled images using diffeomorphic transformations</a>
          </div> 
          <div class="article-title">
           <a href="http://jov.arvojournals.org/article.aspx?articleid=2194091">The role of color in expert object recognition</a>
          </div> 
         </div> 
        </div> 
        <div class="sidebar-widget clearfix"> 
         <div class="widget-RelatedContent widget-instance-ARVO_Article_RelatedContentOther"> 
          <h4>From Other Journals</h4> 
          <div class="article-title">
           <a href="http://jov.arvojournals.org/article.aspx?articleid=2129004">Author Response: Passing–Bablok Regression Is Inappropriate for Assessing Association Between Structure and Function</a>
          </div> 
         </div> 
        </div> 
        <div class="sidebar-widget clearfix"> 
         <div class="widget-RelatedTopics widget-instance-ARVO_Article_RelatedTopics"> 
          <h4 class="h4">Related Topics</h4> 
          <ul> 
           <li> <a href="solr/topicresults.aspx?f_Categories=Visual+Psychophysics+and+Physiological+Optics&amp;resourceid=38488" id="item_Link">Visual Psychophysics and Physiological Optics</a></li> 
           <li> <a href="solr/topicresults.aspx?f_Categories=Crowding&amp;resourceid=38501" id="item_Link_1">Crowding</a></li> 
          </ul> 
         </div> 
        </div> 
        <div id="BodyContent_PageContent_ucAdvertisingBlock_adTextCenterDiv" class="advertisement text-center"> 
         <div class="ad-text">
          Advertisement
         </div> 
         <div class="div-gpt-ad-content-towers"> 
          <script type="text/javascript">
        DisplayAdvertisingBlock();
    </script> 
         </div> 
        </div> 
       </div> 
      </div> 
     </section> 
     <section class="master-footer"> 
      <footer class="footer"> 
       <div class="row collapse" data-equalizer> 
        <div id="FooterMicro" class="large-5 medium-12 columns footer-micro" data-equalizer-watch=""> 
         <img src="/UI/app/images/arvo_jov_logo-white.png" id="BodyContent_footer_JournalLogo" class="journal-logo"> 
         <a href="https://twitter.com/arvojov" id="BodyContent_footer_JovTwitterFollow" class="twitter-follow-button clearfix" data-show-count="false" data-size="large">Follow @ARVOjov</a> 
         <script>!function (d, s, id) { var js, fjs = d.getElementsByTagName(s)[0], p = /^http:/.test(d.location) ? 'http' : 'https'; if (!d.getElementById(id)) { js = d.createElement(s); js.id = id; js.src = p + '://platform.twitter.com/widgets.js'; fjs.parentNode.insertBefore(js, fjs); } }(document, 'script', 'twitter-wjs');</script> 
         <div class="row"> 
          <div class="medium-4 large-6 columns"> 
           <ul class="foot-menu"> 
            <li id="liMenuItem"><a href="/index.aspx" id="aMenuItem">JOV Home</a> </li> 
            <li id="liMenuItem"><a href="/issues.aspx" id="aMenuItem">Issues</a> </li> 
            <li id="liMenuItem"><a href="/topics.aspx" id="aMenuItem">Topics</a> </li> 
            <li id="liMenuItem"><a href="/ss/forauthors.aspx" id="aMenuItem">For Authors</a> </li> 
           </ul> 
          </div> 
          <div class="medium-8 large-6 columns"> 
           <ul class="foot-menu"> 
            <li id="liMenuItem" class="has-sub-menu"><a href="/ss/about.aspx" id="aMenuItem" class="no-link">About</a> 
             <ul class="foot-sub-menu"> 
              <li><a href="/ss/editorial_board.aspx">Editorial Board</a></li> 
             </ul> </li> 
           </ul> 
           <div class="issn">
             Online ISSN: 1534-7362 
           </div> 
          </div> 
         </div> 
         <i class="icon-arvo"></i> 
        </div> 
        <div id="FooterUmb" class="large-7 medium-12 columns footer-umb" data-equalizer-watch=""> 
         <div class="social outside clearfix"> 
          <a href="http://www.facebook.com/ARVOinfo?ref=search&amp;sid=41805257.1733737545..1" target="_blank"><i class="icon-facebook"></i></a> 
          <a href="http://twitter.com/ARVOinfo" target="_blank"><i class="icon-twitter"></i></a> 
          <a href="http://www.linkedin.com/groupSharingMsg?displayCreate=&amp;connId=-1&amp;groupID=735037" target="_blank"><i class="icon-linkedin"></i></a> 
          <a href="http://www.youtube.com/arvoinfo" target="_blank"><i class="icon-youtube"></i></a> 
         </div> 
         <div class="row"> 
          <div class="medium-12 columns"> 
           <a href="/index.aspx"><img class="journals-logo lazy" data-original="/UI/app/images/arvo_journals_logo.png"></a> 
           <div class="social inside clearfix"> 
            <a href="http://www.facebook.com/ARVOinfo?ref=search&amp;sid=41805257.1733737545..1" target="_blank"><i class="icon-facebook"></i></a> 
            <a href="http://twitter.com/ARVOinfo" target="_blank"><i class="icon-twitter"></i></a> 
            <a href="http://www.linkedin.com/groupSharingMsg?displayCreate=&amp;connId=-1&amp;groupID=735037" target="_blank"><i class="icon-linkedin"></i></a> 
            <a href="http://www.youtube.com/arvoinfo" target="_blank"><i class="icon-youtube"></i></a> 
           </div> 
          </div> 
         </div> 
         <div class="row"> 
          <div id="JournalsFoot" class="medium-4 columns journals-foot"> 
           <ul> 
            <li><a href="http://iovs.arvojournals.org">Investigative Ophthalmology &amp; Visual Science</a></li> 
            <li><a href="http://jov.arvojournals.org">Journal of Vision</a></li> 
            <li><a href="http://tvst.arvojournals.org">Translational Vision Science &amp; Technology</a></li> 
           </ul> 
          </div> 
          <div id="JournalsExtraLinks" class="medium-8 columns"> 
           <ul class="clearfix"> 
            <li><a href="http://arvojournals.org/index.aspx">JOURNALS HOME</a></li> 
            <li><a href="http://arvojournals.org/topics.aspx">TOPICS</a></li> 
            <li class="has-sub-menu"><a href="http://arvojournals.org/ss/about.aspx">ABOUT ARVO JOURNALS</a> 
             <ul class="foot-sub-menu"> 
              <li><a href="http://arvojournals.org/ss/terms.aspx">Rights &amp; Permissions</a></li> 
              <li><a href="http://arvojournals.org/ss/privacy.aspx">Privacy Statement</a></li> 
              <li><a target="_blank" href="http://www.arvo.org/mediakit/">Advertising</a></li> 
              <li><a target="_blank" href="http://www.arvo.org/journals_and_publications/submit_your_article/">Submit a Manuscript</a></li> 
              <li><a href="http://arvojournals.org/ss/disclaimer.aspx">Disclaimer</a></li> 
              <li><a href="http://arvojournals.org/contact.aspx">Contact Us</a></li> 
              <li><a href="http://arvo.org/" target="_blank">ARVO.org</a></li> 
             </ul> </li> 
           </ul> 
          </div> 
         </div> 
        </div> 
       </div> 
      </footer> 
      <div class="text-center footer-bar"> 
       <a target="_blank" href="http://arvo.org"> <img data-original="/UI/app/Images/arvo_logo.png" class="arvo-logo lazy"> </a> 
       <a target="_blank" href="http://www.silverchair.com"> <img data-original="/UI/app/Images/logo_silverchair_footer.png" class="sis-logo lazy"> </a> 
       <div class="copyright">
        Copyright © 2015 Association for Research in Vision and Ophthalmology.
       </div> 
      </div> 
     </section> 
    </div> 
   </div> 
   <div id="SignInModal" class="reveal-modal small" data-reveal> 
    <div id="AccessSignIn" class="signin-container"> 
     <input type="hidden" name="ctl00$ctl00$BodyContent$globalSignInMaster$hfARVOStoreUrl" id="hfARVOStoreUrl" value="store.arvojournals.org"> 
     <div id="pnlGlobalSignin" class="si-panel" onkeypress="javascript:return WebForm_FireDefaultButton(event, 'BodyContent_globalSignInMaster_ibSignIn')"> 
      <div class="si-form clearfix"> 
       <div class="message-error" id="wrong-username-password"></div> 
       <input name="ctl00$ctl00$BodyContent$globalSignInMaster$txtEmail" type="text" maxlength="50" id="txtEmail" class="requiredtxtusername" placeholder="Username"> 
       <div class="message-error" id="reqEmailError"></div> 
       <input name="ctl00$ctl00$BodyContent$globalSignInMaster$txtPassword" type="password" maxlength="50" id="txtPassword" class="requiredtxtpassword" placeholder="Password"> 
       <div class="message-error" id="reqPasswordError"></div> 
      </div> 
      <div class="si-forgot-pass clearfix text-center"> 
       <a href="http://www.arvo.org/ForgotPassword/?ReturnUrl=" target="_blank" class="create-acct">Forgot password?</a> 
      </div> 
      <div class="btn-group text-center">
       <input type="submit" name="ctl00$ctl00$BodyContent$globalSignInMaster$ibSignIn" value="Sign In" id="BodyContent_globalSignInMaster_ibSignIn" class="dark-button signInBtn">
      </div> 
      <div id="signin-loading-spinner" class="signin-loading inner-circle hide"></div> 
     </div> 
     <div id="SubscribeBox" data-hideattribute="true" class="subscribe-box box"> 
      <h2>To View More...</h2> 
      <p id="BodyContent_globalSignInMaster_pPurchaseSubInstruction">Purchase this article with an account.</p> 
      <div class="subscribe-btns"> 
       <a target="_blank" href="https://www.arvo.org/Login/?ReturnUrl=" class="create-acct"><span class="dark-button">Create an Account</span></a> 
       <p style="text-align: center;">or</p> 
       <a href="/ss/subscriptions.aspx"><span class="dark-button">Subscribe Now</span></a> 
      </div> 
     </div> 
    </div> 
    <a class="close-reveal-modal">×</a> 
   </div> 
   <div id="revealModal" class="reveal-modal" data-reveal> 
    <div id="revealContent"></div> 
    <a class="close-reveal-modal">×</a> 
   </div> 
   <div id="NoAccessReveal" class="reveal-modal tiny" data-reveal> 
    <div id="NoAcccessRevealContent"> 
     <p>This PDF is available to Subscribers Only</p> 
     <a class="brand-bg" id="NewsletterPdfPurchaseLink" data-reveal><span>Sign in or purchase a subscription to access this content.</span></a> 
     <a class="close-reveal-modal">×</a> 
    </div> 
   </div> 
   <div id="NeedIndividualAcct" class="reveal-modal tiny" data-reveal> 
    <div class="subscription-needed-msg theme-bg-color"> 
     <h4>You must be signed into an individual account to use this feature.</h4> 
    </div> 
    <a class="close-reveal-modal">×</a> 
   </div> 
   <script type="text/javascript">
//<![CDATA[
var theForm = document.forms['webform'];
if (!theForm) {
    theForm = document.webform;
}
function __doPostBack(eventTarget, eventArgument) {
    if (!theForm.onsubmit || (theForm.onsubmit() != false)) {
        theForm.__EVENTTARGET.value = eventTarget;
        theForm.__EVENTARGUMENT.value = eventArgument;
        theForm.submit();
    }
}
//]]>
</script> 
   <script src="/WebResource.axd?d=dK6IKOuYKm4TDeV6fSv5bKqrDijIAo0gpdjpAt7BMkIjWqQplVoWr5QqXA2F2fGB4kFLurfH0kezQcWIbiSNin02VbE1&amp;t=636354762046607314" type="text/javascript"></script> 
  </form> 
  <div id="mathRevealModal" class="reveal-modal" data-reveal></div> 
  <script src="/cassette.axd/script/59c335180c17292f20a7de5700ea5a44fa6f8f9a/bundles/js/masterJS" type="text/javascript"></script> 
  <script src="/cassette.axd/script/07998848923ae0ea93b5a3654d7eb87ac816002e/bundles/js/contentJS" type="text/javascript"></script> 
  <script src="//cdn.jsdelivr.net/chartist.js/latest/chartist.min.js" type="text/javascript"></script> 
  <script src="http://s7.addthis.com/js/300/addthis_widget.js#pubid=xa-5265518246c10183&amp;domready=1" type="text/javascript"></script> 
  <script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
  <!--[if (gte IE 6)&(lte IE 8)]>
            <script type="text/javascript" src="/UI/app/scripts/polyfills/rem.min.js"></script>
        <![endif]-->  
 </body>
</html>