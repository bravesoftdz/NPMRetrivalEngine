<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
 <head profile="http://gmpg.org/xfn/11"> 
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> 
  <title>Object Recognition | the Serious Computer Vision Blog</title> 
  <link rel="pingback" href="https://computervisionblog.wordpress.com/xmlrpc.php"> 
  <style type="text/css" media="screen">
			#container h1.sitename {
			font-size: 50px;
		}
	</style> 
  <meta name="google-site-verification" content="K-zqX2b5iGOG3nYYfsiJXLYX1OsA3z7QWREDJptQS4I"> 
  <link rel="dns-prefetch" href="//s2.wp.com"> 
  <link rel="dns-prefetch" href="//s0.wp.com"> 
  <link rel="dns-prefetch" href="//s1.wp.com"> 
  <link rel="dns-prefetch" href="//s.pubmine.com"> 
  <link rel="dns-prefetch" href="//x.bidswitch.net"> 
  <link rel="dns-prefetch" href="//static.criteo.net"> 
  <link rel="dns-prefetch" href="//ib.adnxs.com"> 
  <link rel="dns-prefetch" href="//aax.amazon-adsystem.com"> 
  <link rel="dns-prefetch" href="//bidder.criteo.com"> 
  <link rel="dns-prefetch" href="//cas.criteo.com"> 
  <link rel="dns-prefetch" href="//gum.criteo.com"> 
  <link rel="dns-prefetch" href="//ads.pubmatic.com"> 
  <link rel="dns-prefetch" href="//gads.pubmatic.com"> 
  <link rel="dns-prefetch" href="//tpc.googlesyndication.com"> 
  <link rel="dns-prefetch" href="//ad.doubleclick.net"> 
  <link rel="dns-prefetch" href="//googleads.g.doubleclick.net"> 
  <link rel="dns-prefetch" href="//www.googletagservices.com"> 
  <link rel="dns-prefetch" href="//cdn.switchadhub.com"> 
  <link rel="dns-prefetch" href="//delivery.g.switchadhub.com"> 
  <link rel="dns-prefetch" href="//delivery.swid.switchadhub.com"> 
  <link rel="alternate" type="application/rss+xml" title="the Serious Computer Vision Blog » Feed" href="https://computervisionblog.wordpress.com/feed/"> 
  <link rel="alternate" type="application/rss+xml" title="the Serious Computer Vision Blog » Comments Feed" href="https://computervisionblog.wordpress.com/comments/feed/"> 
  <link rel="alternate" type="application/rss+xml" title="the Serious Computer Vision Blog » Object Recognition Tag Feed" href="https://computervisionblog.wordpress.com/tag/object-recognition/feed/"> 
  <script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script> 
  <script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s0.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1505864856h&ver=4.8.2"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,56826,8203,55356,56819),0,0),c=j.toDataURL(),b!==c&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55358,56794,8205,9794,65039),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55358,56794,8203,9794,65039),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script> 
  <style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style> 
  <link rel="stylesheet" id="all-css-0-1" href="https://s0.wp.com/_static/??-eJx9j9sKgzAMhl9oXRA25y7GnqWHUKuNDabi9varGw5B8KakP9+XA8ysbBoyDhloUhwnHwaBGHoU6DCztr36/s5W5AQb3MTk/8KcRqedgI/J6LhjN61tGrHkxDovBKELGiNSwY60OTiPueiy1irj61AhrtdBS9mWU/ZXbEdwoZUxPKKIKi+FiVRuy2577xcDTwYcclKkpUTageR3xAV/0qO63OvmVjXXqvsAYwiGkA==" type="text/css" media="all"> 
  <!--[if lte IE 8]>
<link rel='stylesheet' id='depo-ie-css'  href='https://s0.wp.com/wp-content/themes/pub/depo-masthead/ie.css?m=1220654556h&#038;ver=4.8.2' type='text/css' media='all' />
<![endif]--> 
  <link rel="stylesheet" id="all-css-2-1" href="https://s1.wp.com/_static/??-eJx9jNEKwjAMRX/IGKqC+iB+S62xRLKmLBljf29lCO6lb+eQe4JzhaTFqTgOE1SZMhfDmZ+Z3NC1QlVrZL4I7ZPZDvuJaeIowG2ylTVmLOrr8Qe9r5kURFN01rIReEnksZeO9BDNDTO21Z9+o/twC6fr+XAJ4RjeHzS6Yc8=" type="text/css" media="all"> 
  <link rel="stylesheet" id="print-css-3-1" href="https://s0.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1465851035h" type="text/css" media="print"> 
  <link rel="stylesheet" id="all-css-4-1" href="https://s2.wp.com/_static/??/wp-content/mu-plugins/actionbar/actionbar.css,/wp-content/themes/h4/global.css?m=1490786585j" type="text/css" media="all"> 
  <script type="text/javascript" src="https://s1.wp.com/_static/??-eJyFjt0KwjAMhV/Irk4m7kZ8lq6LM3X9sUk39OmNMC/EoRA4cPLlnOg5KQx2LD2QdjK3Avm+SOVoo38ByuOQDUPlMbxhGwND4BfrY4cjqEKQzSCeBJ3jCpcisQcigVa2ny9hmBDmv5gDTsZeVQbCx1cqX0D6dCqd7iFF5Q2JZXpNNmNiWlTOTv5YN+1h19b7ZuuemGdwuA=="></script> 
  <link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://computervisionblog.wordpress.com/xmlrpc.php?rsd"> 
  <link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml"> 
  <meta name="generator" content="WordPress.com"> 
  <link rel="shortcut icon" type="image/x-icon" href="https://secure.gravatar.com/blavatar/053089fcaeb6aacc5d7c36792f0319e6?s=32" sizes="16x16"> 
  <link rel="icon" type="image/x-icon" href="https://secure.gravatar.com/blavatar/053089fcaeb6aacc5d7c36792f0319e6?s=32" sizes="16x16"> 
  <link rel="apple-touch-icon-precomposed" href="https://secure.gravatar.com/blavatar/053089fcaeb6aacc5d7c36792f0319e6?s=114"> 
  <link rel="openid.server" href="https://computervisionblog.wordpress.com/?openidserver=1"> 
  <link rel="openid.delegate" href="https://computervisionblog.wordpress.com/"> 
  <link rel="search" type="application/opensearchdescription+xml" href="https://computervisionblog.wordpress.com/osd.xml" title="the Serious Computer Vision Blog"> 
  <link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com"> 
  <style id="wpcom-hotfix-masterbar-style">
			@media screen and (min-width: 783px) {
				#wpadminbar .quicklinks li#wp-admin-bar-my-account.with-avatar > a img {
					margin-top: 5px;
				}
			}
		</style> 
  <script type="text/javascript" id="webfont-output">
  
  WebFontConfig = {"typekit":{"id":"lgf5mwc"}};
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
	})();
</script>
  <meta name="application-name" content="the Serious Computer Vision Blog">
  <meta name="msapplication-window" content="width=device-width;height=device-height">
  <meta name="msapplication-tooltip" content="A blog about computer vision and serious stuff">
  <meta name="msapplication-task" content="name=Subscribe;action-uri=https://computervisionblog.wordpress.com/feed/;icon-uri=https://secure.gravatar.com/blavatar/053089fcaeb6aacc5d7c36792f0319e6?s=16">
  <meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s2.wp.com/i/favicon.ico">
  <meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s2.wp.com/i/favicon.ico">
  <meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s2.wp.com/i/favicon.ico">
  <meta name="title" content="Posts about Object Recognition on the Serious Computer Vision Blog"> 
  <meta name="description" content="Computer Vision Methods for Object Recognition"> 
  <script type="text/javascript">
		var __ATA_PP = { pt: 3, ht: 0, tn: 'depo-masthead', amp: false };
		</script> 
  <script type="text/javascript" src="//s.pubmine.com/head.js"></script> 
  <script type="text/javascript" src="https://static.criteo.net/js/ld/publishertag.js"></script>
  <style type="text/css" id="syntaxhighlighteranchor"></style> 
  <script type="text/javascript">
	window.google_analytics_uacct = "UA-52447-2";
</script> 
  <script type="text/javascript">
	var _gaq = _gaq || [];
	_gaq.push(['_setAccount', 'UA-52447-2']);
	_gaq.push(['_setDomainName', 'wordpress.com']);
	_gaq.push(['_initData']);
	_gaq.push(['_trackPageview']);

	(function() {
		var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ga);
	})();
</script> 
 </head> 
 <body class="archive tag tag-object-recognition tag-1199747 mp6 customizer-styles-applied highlander-enabled highlander-light"> 
  <div id="page"> 
   <h1 class="name"><a href="https://computervisionblog.wordpress.com/" title="A blog about computer vision and serious stuff"><span> Life is a game, take it seriously</span></a></h1> 
   <div id="container"> 
    <div class="sleeve"> 
     <div id="header"> 
      <h1 class="sitename"> <a href="https://computervisionblog.wordpress.com" title="A blog about computer vision and serious stuff"> the Serious Computer Vision Blog </a> </h1> 
      <div id="menu"> 
       <ul id="menu-standard" class="menu">
        <li id="menu-item-19" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-19"><a href="https://computervisionblog.wordpress.com/about/">About</a></li> 
        <li id="menu-item-20" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-20"><a href="https://computervisionblog.wordpress.com/category/computer-vision/">Computer Vision</a></li> 
        <li id="menu-item-21" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-21"><a href="https://computervisionblog.wordpress.com/category/serious-stuffs/">Serious Stuff</a></li> 
       </ul> 
      </div> 
     </div> 
     <div id="content" class="group"> 
      <h2 class="pagetitle">Posts Tagged ‘Object Recognition’</h2> 
      <div class="post-702 post type-post status-publish format-standard hentry category-computer-vision category-paper-talk tag-computer-vision-2 tag-local-distance-learning tag-machine-learning-2 tag-object-recognition" id="post-702"> 
       <p class="postmetadata"><a href="https://computervisionblog.wordpress.com/tag/computer-vision-2/" rel="tag">computer vision</a>, <a href="https://computervisionblog.wordpress.com/tag/local-distance-learning/" rel="tag">local distance learning</a>, <a href="https://computervisionblog.wordpress.com/tag/machine-learning-2/" rel="tag">machine learning</a>, <a href="https://computervisionblog.wordpress.com/tag/object-recognition/" rel="tag">Object Recognition</a><br></p> 
       <h2><a href="https://computervisionblog.wordpress.com/2015/02/08/local-distance-learning-in-object-recognition/" rel="bookmark">Local Distance Learning in Object&nbsp;Recognition</a></h2> 
       <small>In <a href="https://computervisionblog.wordpress.com/category/computer-vision/" rel="category tag">Computer Vision</a>, <a href="https://computervisionblog.wordpress.com/category/paper-talk/" rel="category tag">Paper Talk</a> on <strong>February 8, 2015</strong> at <strong>11:59 am</strong></small> 
       <div class="entry"> 
        <blockquote>
         <p>by Li Yang Ku (Gooly)</p>
        </blockquote> 
        <p><a href="https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg"><img data-attachment-id="711" data-permalink="https://computervisionblog.wordpress.com/2015/02/08/local-distance-learning-in-object-recognition/learning-distance/" data-orig-file="https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg?w=604&amp;h=315" data-orig-size="672,351" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;4&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Canon EOS 5D&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1363697904&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;75&quot;,&quot;iso&quot;:&quot;250&quot;,&quot;shutter_speed&quot;:&quot;0.0125&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="learning distance" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg?w=604&amp;h=315?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg?w=604&amp;h=315?w=604" class="aligncenter size-full wp-image-711" src="https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg?w=604&amp;h=315" alt="learning distance" width="604" height="315" srcset="https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg?w=604&amp;h=315 604w, https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg?w=150&amp;h=78 150w, https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg?w=300&amp;h=157 300w, https://computervisionblog.files.wordpress.com/2015/02/learning-distance.jpg 672w" sizes="(max-width: 604px) 100vw, 604px"></a></p> 
        <p>Unsupervised clustering algorithms such as K-means are often used in computer vision as a tool for feature learning. It can be used in different stages in the visual pathway. Running&nbsp;K-means algorithm on a small region of pixel patches might&nbsp;result in&nbsp;finding a lot of patches with edges of different orientation while running&nbsp;K-means on a larger HOG feature might result in&nbsp;finding contours of meaningful parts of objects such as faces if your training data consists of selfies. &nbsp;However,&nbsp;although convenient and simple as it seems, we have to keep in mind that these unsupervised clustering algorithms are all based on the&nbsp;assumption that a meaningful metric is provided. Without this criteria, clustering&nbsp;suffers from the “no right answer” problem. Whether the algorithm should group a set of images into clusters that contain objects with the same type or the same color is ambiguous and not well defined. This is especially true when your observation vectors are consists of values&nbsp;representing different types of properties.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png"><img data-attachment-id="706" data-permalink="https://computervisionblog.wordpress.com/2015/02/08/local-distance-learning-in-object-recognition/distance-learning/" data-orig-file="https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png" data-orig-size="795,298" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="distance learning" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png?w=604&amp;h=226" class="aligncenter wp-image-706 size-large" src="https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png?w=604&amp;h=226" alt="distance learning" width="604" height="226" srcset="https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png?w=604&amp;h=226 604w, https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png?w=150&amp;h=56 150w, https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png?w=300&amp;h=112 300w, https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png?w=768&amp;h=288 768w, https://computervisionblog.files.wordpress.com/2015/02/distance-learning.png 795w" sizes="(max-width: 604px) 100vw, 604px"></a></p> 
        <p>This is where Distance Learning comes into play. In the paper <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AA03.pdf" target="_blank">“Distance Metric Learning, with Application to Clustering with Side-Information”</a> written by Eric Xing, Andrew Ng, Michael Jordan and Stuart Russell, a matrix <strong>A</strong> that represents the distance metric is learned through convex optimization using user inputs specifying grouping examples. This matrix <strong>A</strong>&nbsp;can either be full or diagonal. When learning a diagonal matrix, the values simply represent the weights of each feature. If the goal is to group objects with similar color, features that can represent color will have a higher weight in the matrix. This metric learning approach was shown to improve clustering on the UCI data set.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2015/02/visual-association.png"><img data-attachment-id="707" data-permalink="https://computervisionblog.wordpress.com/2015/02/08/local-distance-learning-in-object-recognition/visual-association/" data-orig-file="https://computervisionblog.files.wordpress.com/2015/02/visual-association.png?w=604" data-orig-size="551,210" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visual association" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2015/02/visual-association.png?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2015/02/visual-association.png?w=604?w=551" class="aligncenter wp-image-707 size-full" src="https://computervisionblog.files.wordpress.com/2015/02/visual-association.png?w=604" alt="visual association" srcset="https://computervisionblog.files.wordpress.com/2015/02/visual-association.png 551w, https://computervisionblog.files.wordpress.com/2015/02/visual-association.png?w=150 150w, https://computervisionblog.files.wordpress.com/2015/02/visual-association.png?w=300 300w" sizes="(max-width: 551px) 100vw, 551px"></a></p> 
        <p>In another work <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.517&amp;rep=rep1&amp;type=pdf" target="_blank">“Recognition by Association via Learning Per-exemplar Distances”</a> written by Tomasz Malisiewicz and Alexei Efros, the object&nbsp;recognition problem is posed as data association. A region in the image is classified by associating it with a small set of exemplars&nbsp;based on visual similarity. The authors suggested that the central question for recognition might not be “What is it?” but “What is it like?”.&nbsp;In this work, 14 different type of features under 4 categories,&nbsp;shape, color, texture and location are used. Unlike the single distance metric learned in the previous work, a&nbsp;separate distance function that specifies the weights put on these 14 different type of features is learned for each exemplar. Some exemplars like cars will not be as sensitive to color as exemplars like sky or grass, therefore having a different distance metric for each exemplar becomes advantageous in such situations. These class of work that defines separate distance metrics are called Local Distance Learning.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png"><img data-attachment-id="708" data-permalink="https://computervisionblog.wordpress.com/2015/02/08/local-distance-learning-in-object-recognition/instance-distance-learning/" data-orig-file="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png?w=604" data-orig-size="430,132" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="instance distance learning" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png?w=604?w=430" class="aligncenter wp-image-708 size-full" src="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png?w=604" alt="instance distance learning" srcset="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png 430w, https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png?w=150 150w, https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning.png?w=300 300w" sizes="(max-width: 430px) 100vw, 430px"></a></p> 
        <p>In a more recent work <a href="http://mobilerobotics.cs.washington.edu/projects/postscripts/sparse-distance-icra-11.pdf" target="_blank">“Sparse Distance Learning for Object Recognition Combining RGB and Depth Information”</a> by Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox, a new approach called Instance Distance Learning is introduced, which instance is referred to one single object. When classifying a view, the view to object distance is compared simultaneously to all views of an object instead of a nearest neighbor approach. Besides learning weight vectors on each feature, weights on views are also learned. In addition, a&nbsp;<em>L1</em> regularization is used instead of a <em>L2</em> regularization in the Lagrange function. This generates a sparse weight vector which has a zero term on most views. This is quite interesting in the sense that&nbsp;this approach finds a small subset of representative views for each instance. In fact as shown in the image below, with just 8% of the exemplar data a similar decision boundaries can be achieved. This is consistent to what I talked about in <a href="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/" target="_blank">my last post</a>; human brain doesn’t store all the possible views of an object nor does it store a 3D model of the object, instead it stores a subset of views that are representing enough to recognize the same object. This work demonstrates one possible way of finding such subset of views.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png"><img data-attachment-id="709" data-permalink="https://computervisionblog.wordpress.com/2015/02/08/local-distance-learning-in-object-recognition/instance-distance-learning-decision-boundaries/" data-orig-file="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png?w=604" data-orig-size="420,261" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="instance distance learning decision boundaries" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png?w=604?w=420" class="aligncenter size-full wp-image-709" src="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png?w=604" alt="instance distance learning decision boundaries" srcset="https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png 420w, https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png?w=150 150w, https://computervisionblog.files.wordpress.com/2015/02/instance-distance-learning-decision-boundaries.png?w=300 300w" sizes="(max-width: 420px) 100vw, 420px"></a></p> 
        <p>&nbsp;</p> 
        <div class="wpcnt"> 
         <div class="wpa wpmrec"> 
          <span class="wpa-about">Advertisements</span> 
          <div class="u"> 
           <script type="text/javascript">
				(function(g){if("undefined"!=typeof g.__ATA){g.__ATA.initAd({collapseEmpty:'after', sectionId:26942, width:300, height:250});}})(window);
			</script> 
          </div> 
          <div id="crt-1807377346" style="width:300px;height:250px;"></div> 
          <script type="text/javascript">
		var o = document.getElementById('crt-1807377346');
		if ("undefined"!=typeof Criteo) {
			var p = o.parentNode;
			p.style.setProperty('display', 'inline-block', 'important');
			o.style.setProperty('display', 'block', 'important');
			Criteo.DisplayAcceptableAdIfAdblocked({zoneid:388248,containerid:"crt-1807377346",collapseContainerIfNotAdblocked:true,"callifnotadblocked": function () {var o = document.getElementById('crt-1807377346'); o.style.setProperty('display','none','important');o.style.setProperty('visbility','hidden','important'); } });
		} else {
			o.style.setProperty('display', 'none', 'important');
			o.style.setProperty('visibility', 'hidden', 'important');
		}
		</script> 
          <div id="crt-2049158154" style="width:300px;height:250px;"></div> 
          <script type="text/javascript">
		var o = document.getElementById('crt-2049158154');
		if ("undefined"!=typeof Criteo) {
			var p = o.parentNode;
			p.style.setProperty('display', 'inline-block', 'important');
			o.style.setProperty('display', 'block', 'important');
			Criteo.DisplayAcceptableAdIfAdblocked({zoneid:837497,containerid:"crt-2049158154",collapseContainerIfNotAdblocked:true,"callifnotadblocked": function () {var o = document.getElementById('crt-2049158154'); o.style.setProperty('display','none','important');o.style.setProperty('visbility','hidden','important'); } });
		} else {
			o.style.setProperty('display', 'none', 'important');
			o.style.setProperty('visibility', 'hidden', 'important');
		}
		</script> 
         </div> 
        </div> 
       </div> 
      </div> 
      <div id="showcomments" class="archive">
       <div class="divider"></div>
       <a href="https://computervisionblog.wordpress.com/2015/02/08/local-distance-learning-in-object-recognition/#comments">? Comment</a>
      </div> 
      <div class="post-671 post type-post status-publish format-standard hentry category-computer-vision category-neural-science category-paper-talk tag-3d-models tag-object-recognition" id="post-671"> 
       <p class="postmetadata"><a href="https://computervisionblog.wordpress.com/tag/3d-models/" rel="tag">3D models</a>, <a href="https://computervisionblog.wordpress.com/tag/object-recognition/" rel="tag">Object Recognition</a><br></p> 
       <h2><a href="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/" rel="bookmark">How objects are represented in human brain? Structural description models versus Image-based&nbsp;models</a></h2> 
       <small>In <a href="https://computervisionblog.wordpress.com/category/computer-vision/" rel="category tag">Computer Vision</a>, <a href="https://computervisionblog.wordpress.com/category/neural-science/" rel="category tag">Neural Science</a>, <a href="https://computervisionblog.wordpress.com/category/paper-talk/" rel="category tag">Paper Talk</a> on <strong>October 30, 2014</strong> at <strong>9:06 pm</strong></small> 
       <div class="entry"> 
        <blockquote>
         <p>by Li Yang Ku (Gooly)</p>
        </blockquote> 
        <p><a href="https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg"><img data-attachment-id="692" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/poggio/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg" data-orig-size="830,320" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="poggio" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg?w=604&amp;h=232" class="aligncenter wp-image-692 size-large" src="https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg?w=604&amp;h=232" alt="poggio" width="604" height="232" srcset="https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg?w=602&amp;h=232 602w, https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg?w=150&amp;h=58 150w, https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg?w=300&amp;h=116 300w, https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg?w=768&amp;h=296 768w, https://computervisionblog.files.wordpress.com/2014/10/poggio.jpg 830w" sizes="(max-width: 604px) 100vw, 604px"></a></p> 
        <p>A few years ago while I was still back in UCLA, Tomaso Poggio came to give a talk about the object recognition work he did with 2D templates. After the talk some student asked about whether he thought about using a 3D model to help recognizing objects from different viewpoints. “The field seems to agree that models are stored as 2D images instead of 3D models in human&nbsp;brain” was&nbsp;the short answer Tomaso replied. Since then I took&nbsp;it as a fact and never had a second thought of it till a few month ago when I actually need to argue against storing a 3D model to people in robotics.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg"><img data-attachment-id="690" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/70s-fashion-boys-47/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg" data-orig-size="1404,893" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;HP ScanJet 3770&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="70s fashion" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=604&amp;h=384" class="aligncenter wp-image-690 size-large" src="https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=604&amp;h=384" alt="70s fashion" width="604" height="384" srcset="https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=604&amp;h=384 604w, https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=1208&amp;h=768 1208w, https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=150&amp;h=95 150w, https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=300&amp;h=191 300w, https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=768&amp;h=488 768w, https://computervisionblog.files.wordpress.com/2014/10/70s-fashion-boys-47.jpg?w=1024&amp;h=651 1024w" sizes="(max-width: 604px) 100vw, 604px"></a></p> 
        <p>To get the full story we have to first go back to the late&nbsp;70s. The study of visual object recognition is often motivated by the problem of recognizing 3D objects while only receiving 2D patterns of light on our retina. The question was whether&nbsp;our object representations is more similar to abstract three-dimensional descriptions, or are they tied more closely to the two-dimensional image of an object?&nbsp;A commonly held solution at that time, popularized by Marr was that the goal of vision is to reconstruct 3D. In the paper “<a href="http://dspace.mit.edu/bitstream/handle/1721.1/6276/AIM-416.pdf?sequence=2">Representation and recognition of the spatial organization of three-dimensional shapes</a>” published in 1978 Marr and Nishihara assumes that at the end of the reconstruction process, viewer centered descriptions are mapped into object centered representations. This is based on the hypothesis&nbsp;that object representation should be invariant over changes in the retinal image. Based on this object centered theory, Biederman introduced the <a href="http://scholar.google.com/scholar?hl=en&amp;q=allintitle:Recognition-by-components:%20a%20theory%20of%20human%20image%20understanding.%20author:%27I%20Biederman%27&amp;as_occt=title">recognition by component (RBC) model</a> in 1987 which proposes that objects are represented as a collection of volumes or parts. This quite influential&nbsp;model explains how object recognition can be viewpoint invariant&nbsp;and is often referred to as a structural description model.</p> 
        <p>The structural description model or object centered theory was the dominant theory of visual object understanding&nbsp;around that time and it can correctly predict the view-independent recognition of familiar objects. On the other hand, the viewer centered models, which store a set of 2D images instead of one single 3D model, are usually considered implausible because of the amount of memory a system would require to store all discriminable views of many objects.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg"><img data-attachment-id="694" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/1980-radio-shack-catalog/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg" data-orig-size="1627,1854" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="1980-radio-shack-catalog" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=263" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=604&amp;h=688" class="aligncenter wp-image-694 size-large" src="https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=604&amp;h=688" alt="1980-radio-shack-catalog" width="604" height="688" srcset="https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=604&amp;h=688 604w, https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=1208&amp;h=1376 1208w, https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=132&amp;h=150 132w, https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=263&amp;h=300 263w, https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=768&amp;h=875 768w, https://computervisionblog.files.wordpress.com/2014/10/1980-radio-shack-catalog.jpg?w=899&amp;h=1024 899w" sizes="(max-width: 604px) 100vw, 604px"></a></p> 
        <p>However, between late 1980’s to early 1990’s a&nbsp;wide variety of psychophysical and neurophysiological experiments surprisingly showed that human object recognition performance is strongly viewpoint dependent across rotation in depth. Before jumping into late 80’s I wanna first introduce some work done by&nbsp;<a href="http://scholar.google.com/scholar?cluster=3582798039677500026&amp;hl=en&amp;oi=scholarr">Palmer, Rosch, and Chase in 1981</a>. In their work they discovered that commonplace objects such as houses or cars can be hard or easy to recognize, depending on the attitude of the object with respect to the viewer.&nbsp;Subjects tended to respond quicker when the stimulus was shown from a good or canonical perspective. These observations was important in forming the viewer centered theory.</p> 
        <div data-shortcode="caption" id="attachment_682" style="width: 310px" class="wp-caption aligncenter">
         <a href="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png"><img data-attachment-id="682" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/screen-shot-2014-10-23-at-11-08-59-am-2/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png" data-orig-size="792,645" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Paper clip like objects used in Bulthoff’s experiments" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png?w=300&amp;h=244" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png?w=604" class="wp-image-682 size-medium" src="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png?w=300&amp;h=244" alt="Paper clip like objects used in Bulthoff's experiments" width="300" height="244" srcset="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png?w=300&amp;h=244 300w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png?w=600&amp;h=488 600w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-08-59-am-2.png?w=150&amp;h=122 150w" sizes="(max-width: 300px) 100vw, 300px"></a>
         <p class="wp-caption-text">Paper clip like objects used in Bulthoff’s experiments</p>
        </div> 
        <p>In <a href="http://scholar.google.com/scholar?hl=en&amp;q=allintitle:Psychophysical%20support%20for%20a%20two-dimensional%20view%20interpolation%20theory%20of%20object%20recognition%20author:%27HH%20B%C3%BClthoff%27%20author:%27S%20Edelman%27&amp;as_occt=title">1991 Bulthoff</a> conducted&nbsp;an experiment on understanding these two theories. Subjects are shown sequences of animations where a paper clip like object is rotating. Given these sequences, the subjects have enough information to reconstruct a 3D model of the object. The subjects are then given a single image of a paper clip like object and are asked to identify whether it is the same object. Different viewing angles of the object are tested. The assumption is that if only one single complete 3D model of this object exists in our brain then recognizing it from all angles should be equally easy. However,&nbsp;according to Bulthoff&nbsp;when given every opportunity to form 3D, the subjects performed as if they have not done so.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png"><img data-attachment-id="683" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/screen-shot-2014-10-23-at-11-07-48-am/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png" data-orig-size="832,1046" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bulthoff 1991" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png?w=239" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png?w=604" class="aligncenter size-medium wp-image-683" src="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png?w=238&amp;h=300" alt="Bulthoff 1991" width="238" height="300" srcset="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png?w=238&amp;h=300 238w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png?w=476&amp;h=598 476w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-11-07-48-am.png?w=119&amp;h=150 119w" sizes="(max-width: 238px) 100vw, 238px"></a></p> 
        <p>In <a href="http://scholar.google.com/scholar?hl=en&amp;q=allintitle:Orientation%20dependence%20in%20the%20recognition%20of%20familiar%20and%20novel%20views%20of%20three-dimensional%20objects%20author:%27S%20Edelman%27%20author:%27HH%20B%C3%BClthoff%27&amp;as_occt=title">1992 Edelman</a> further showed that canonical perspectives arise even when all the views in question are shown equally often and the objects posses no intrinsic orientation that might lead to the advantage of some views.</p> 
        <div data-shortcode="caption" id="attachment_684" style="width: 614px" class="wp-caption aligncenter">
         <a href="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png"><img data-attachment-id="684" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/screen-shot-2014-10-23-at-3-26-31-pm/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png" data-orig-size="1342,554" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Edelman 1992" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=604&amp;h=249" class="wp-image-684 size-large" src="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=604&amp;h=249" alt="Edelman 1992" width="604" height="249" srcset="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=604&amp;h=249 604w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=1206&amp;h=498 1206w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=150&amp;h=62 150w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=300&amp;h=124 300w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=768&amp;h=317 768w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-3-26-31-pm.png?w=1024&amp;h=423 1024w" sizes="(max-width: 604px) 100vw, 604px"></a>
         <p class="wp-caption-text">Error rate from different viewpoint shown in Edelman’s experiment</p>
        </div> 
        <p>In <a href="http://scholar.google.com/scholar?hl=en&amp;q=allintitle:Rotating%20objects%20to%20recognize%20them:%20A%20case%20study%20on%20the%20role%20of%20viewpoint%20dependency%20in%20the%20recognition%20of%20three-dimensional%20objects%20author:%27MJ%20Tarr%27&amp;as_occt=title">1995 Tarr</a>&nbsp;confirmed the discoveries using block like objects. Instead of showing a sequence of views of the object rotating, subjects are trained to learn how to build these block structures by manually placing them through an interface with fixed angle. The result shows that response times increased proportionally to the&nbsp;angular distance from the training viewpoint. With extensive practice, performance became nearly equivalent at all familiar viewpoints; however practice at familiar viewpoints did not transfer to unfamiliar viewpoints.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png"><img data-attachment-id="685" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/screen-shot-2014-10-23-at-5-44-43-pm/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png" data-orig-size="732,672" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Tarr 1995" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png?w=300&amp;h=275" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png?w=604" class="aligncenter size-medium wp-image-685" src="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png?w=300&amp;h=275" alt="Tarr 1995" width="300" height="275" srcset="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png?w=300&amp;h=275 300w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png?w=600&amp;h=550 600w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-23-at-5-44-43-pm.png?w=150&amp;h=138 150w" sizes="(max-width: 300px) 100vw, 300px"></a></p> 
        <p>Based on these past observations,<a href="http://scholar.google.com/scholar?hl=en&amp;q=allintitle:%20Shape%20representation%20in%20the%20inferior%20temporal%20cortex%20of%20monkeys%20author:%27NK%20Logothetis%27%20author:%27J%20Pauls%27%20author:%27T%20Poggio%27&amp;as_occt=title">&nbsp;Logothetis, Pauls, and Poggio</a> raised the question&nbsp;“If monkeys are extensively trained to identify novel 3D objects, would one find neurons in the brain that respond selectively to particular views of such object?” The results published in 1995 was clear. By conducting the same paper clip object recognition task on monkeys, they found&nbsp;11.6% of the isolated neurons sampled in the IT region, which is the region that known to represent objects, responded selectively to a subset of views of one of the known target object. The response of these individual neurons decrease when the shown object rotate in all 4 axis from the canonical view which the neurons&nbsp;represent. The experiment also shows that these view specific neurons&nbsp;are scale and position invariant up to certain degree.</p> 
        <div data-shortcode="caption" id="attachment_686" style="width: 614px" class="wp-caption aligncenter">
         <a href="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png"><img data-attachment-id="686" data-permalink="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/screen-shot-2014-10-30-at-11-38-15-pm/" data-orig-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png" data-orig-size="1488,556" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Logothetis 1995" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=604&amp;h=225" class="wp-image-686 size-large" src="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=604&amp;h=225" alt="Logothetis 1995" width="604" height="225" srcset="https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=602&amp;h=225 602w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=1204&amp;h=450 1204w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=150&amp;h=56 150w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=300&amp;h=112 300w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=768&amp;h=287 768w, https://computervisionblog.files.wordpress.com/2014/10/screen-shot-2014-10-30-at-11-38-15-pm.png?w=1024&amp;h=383 1024w" sizes="(max-width: 604px) 100vw, 604px"></a>
         <p class="wp-caption-text">Viewpoint specific neurons</p>
        </div> 
        <p>These series of&nbsp;findings from human psychophysics and neurophysiolog research provided converging evidence for ‘image-based’ models in which objects are represented as collections of viewpoint-speci?c local features. A series of work in computer&nbsp;vision also shown that by allowing each canonical view to represent a range of images the model is no longer unfeasible. However despite a&nbsp;large amount of research,&nbsp;most of the detail&nbsp;mechanisms are still unknown and require further research.</p> 
        <p>Check out these papers visually in my other website&nbsp;<a href="http://www.eatpaper.org/#graph/Object%20Recognition:Gooly">EatPaper.org</a></p> 
        <p>References not linked in post:</p> 
        <p>Tarr, Michael J., and Heinrich H. Bülthoff. “Image-based object recognition in man, monkey and machine.” <i>Cognition</i> 67.1 (1998): 1-20.</p> 
        <p>Palmeri, Thomas J., and Isabel Gauthier. “Visual object understanding.” <i>Nature Reviews Neuroscience</i> 5.4 (2004): 291-303.</p> 
       </div> 
      </div> 
      <div id="showcomments" class="archive">
       <div class="divider"></div>
       <a href="https://computervisionblog.wordpress.com/2014/10/30/how-objects-are-represented-in-human-brain-structural-description-models-versus-image-based-models/#comments">? View 7 Comments</a>
      </div> 
      <div class="post-415 post type-post status-publish format-standard hentry category-computer-vision category-point-cloud-library category-robotics tag-object-recognition tag-ros tag-rviz tag-vision" id="post-415"> 
       <p class="postmetadata"><a href="https://computervisionblog.wordpress.com/tag/object-recognition/" rel="tag">Object Recognition</a>, <a href="https://computervisionblog.wordpress.com/tag/ros/" rel="tag">ros</a>, <a href="https://computervisionblog.wordpress.com/tag/rviz/" rel="tag">rviz</a>, <a href="https://computervisionblog.wordpress.com/tag/vision/" rel="tag">vision</a><br></p> 
       <h2><a href="https://computervisionblog.wordpress.com/2012/11/18/rviz-a-good-reason-to-implement-a-vision-system-in-ros/" rel="bookmark">RVIZ: a good reason to implement a vision system in&nbsp;ROS</a></h2> 
       <small>In <a href="https://computervisionblog.wordpress.com/category/computer-vision/" rel="category tag">Computer Vision</a>, <a href="https://computervisionblog.wordpress.com/category/point-cloud-library/" rel="category tag">Point Cloud Library</a>, <a href="https://computervisionblog.wordpress.com/category/robotics/" rel="category tag">Robotics</a> on <strong>November 18, 2012</strong> at <strong>2:33 pm</strong></small> 
       <div class="entry"> 
        <blockquote>
         <p>by Gooly (Li Yang Ku)</p>
        </blockquote> 
        <p><a href="https://computervisionblog.files.wordpress.com/2012/11/rviz.png"><img data-attachment-id="419" data-permalink="https://computervisionblog.wordpress.com/2012/11/18/rviz-a-good-reason-to-implement-a-vision-system-in-ros/rviz/" data-orig-file="https://computervisionblog.files.wordpress.com/2012/11/rviz.png?w=604" data-orig-size="393,240" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="rviz" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2012/11/rviz.png?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2012/11/rviz.png?w=604?w=393" class="aligncenter size-full wp-image-419" title="rviz" alt="" src="https://computervisionblog.files.wordpress.com/2012/11/rviz.png?w=604" srcset="https://computervisionblog.files.wordpress.com/2012/11/rviz.png 393w, https://computervisionblog.files.wordpress.com/2012/11/rviz.png?w=150 150w, https://computervisionblog.files.wordpress.com/2012/11/rviz.png?w=300 300w" sizes="(max-width: 393px) 100vw, 393px"></a></p> 
        <p>It might seem illogical to implement a vision system in <a href="http://www.ros.org/wiki/" target="_blank">ROS</a> (Robot Operating System) if you are working on pure vision, however after messing with ROS and PCL for a year I can see the advantages of doing this. To clarify, we started to use ROS only because we need it to communicate with <a href="http://www.nasa.gov/mission_pages/station/main/robonaut.html" target="_blank">Robonaut 2</a>, but the package <a href="http://ros.org/wiki/rviz" target="_blank">RVIZ</a> in ROS are&nbsp;truly&nbsp;very helpful such that I would recommend it even if no robots are involved.</p> 
        <p>(Keynote speech about Robonaut 2 and ROS from the brilliant guy I work for)</p> 
        <span class="embed-youtube" style="text-align:center; display: block;"><iframe class="youtube-player" type="text/html" width="604" height="370" src="https://www.youtube.com/embed/HLVwyCfXMwI?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true" style="border:0;"></iframe></span> 
        <p>&nbsp;</p> 
        <p>RVIZ is a ROS package that visualizes robots, point clouds, etc. Although <a href="https://computervisionblog.wordpress.com/2012/06/10/object-recognition-with-point-cloud-library/" target="_blank">PCL</a> does provide a visualizer for point cloud, it only provides the most basic visualize function. It is really not comparable with what RVIZ can give you.</p> 
        <ol> 
         <li>RVIZ is perfect for figuring out what went wrong in a vision system. The list on the left has a check box for each item. You can show or hide any visual information instantly.</li> 
         <li>RVIZ provides 3D visualization which you could navigate with just your mouse. At first I prefer the kind of navigation similar to <a href="http://www.microsoft.com/robotics/" target="_blank">Microsoft Robotic Studio</a> or Counter Strike. But once you get used to it, it is pretty handy. Since I already have 2 keyboards and 2 mouses, it’s quiet convenient to move around with my left mouse while not leaving my right hand from my right mouse.</li> 
         <li>The best part of RVIZ is the <a href="http://www.ros.org/wiki/interactive_markers" target="_blank">interactive marker</a>. This is the part where you can be really creative. It makes selecting a certain area in 3D relative easy. You can therefore adjust your vision system manually while it is still running such as select a certain area as your work space and ignoring other region.</li> 
         <li>You can have multiple vision processes showing vision data in the same RVIZ. You simply have to publish the point cloud or shape you want to show using the ROS publishing method. Visualizing is relatively painless once you get used to it.</li> 
        </ol> 
        <p>Try not to view ROS as an operating system like Windows, Linux. It is more like internet, where RVIZ is just one service like google map, and you can write your own app that queries the map if you use the same communication protocol provided by ROS.</p> 
       </div> 
      </div> 
      <div id="showcomments" class="archive">
       <div class="divider"></div>
       <a href="https://computervisionblog.wordpress.com/2012/11/18/rviz-a-good-reason-to-implement-a-vision-system-in-ros/#comments">? Comment</a>
      </div> 
      <div class="post-261 post type-post status-publish format-standard hentry category-computer-vision category-matlab tag-computer-vision-2 tag-matlab-2 tag-object-recognition tag-sift tag-surf" id="post-261"> 
       <p class="postmetadata"><a href="https://computervisionblog.wordpress.com/tag/computer-vision-2/" rel="tag">computer vision</a>, <a href="https://computervisionblog.wordpress.com/tag/matlab-2/" rel="tag">matlab</a>, <a href="https://computervisionblog.wordpress.com/tag/object-recognition/" rel="tag">Object Recognition</a>, <a href="https://computervisionblog.wordpress.com/tag/sift/" rel="tag">SIFT</a>, <a href="https://computervisionblog.wordpress.com/tag/surf/" rel="tag">SURF</a><br></p> 
       <h2><a href="https://computervisionblog.wordpress.com/2012/01/15/object-matching-method-made-in-the-20th-century/" rel="bookmark">Object matching method made in the 20th&nbsp;century</a></h2> 
       <small>In <a href="https://computervisionblog.wordpress.com/category/computer-vision/" rel="category tag">Computer Vision</a>, <a href="https://computervisionblog.wordpress.com/category/matlab/" rel="category tag">Matlab</a> on <strong>January 15, 2012</strong> at <strong>8:33 pm</strong></small> 
       <div class="entry"> 
        <blockquote>
         <p>written by gooly</p>
        </blockquote> 
        <p><a href="https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg"><img data-attachment-id="262" data-permalink="https://computervisionblog.wordpress.com/2012/01/15/object-matching-method-made-in-the-20th-century/bound8/" data-orig-file="https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=604&amp;h=418" data-orig-size="871,603" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="object recognition" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=604&amp;h=418?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=604&amp;h=418?w=604" class="aligncenter size-full wp-image-262" title="object recognition" src="https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=604&amp;h=418" alt="object recognition using SIFT" width="604" height="418" srcset="https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=604&amp;h=418 604w, https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=150&amp;h=104 150w, https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=300&amp;h=208 300w, https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg?w=768&amp;h=532 768w, https://computervisionblog.files.wordpress.com/2012/01/bound8.jpg 871w" sizes="(max-width: 604px) 100vw, 604px"></a>I just submitted some <a href="http://www.mathworks.com/matlabcentral/fileexchange/34626">Matlab code</a> for object matching, using an old but simple method mentioned in the paper:</p> 
        <p>Lowe, D.G. 1999. Object recognition from local scale-invariant features.<br> In International Conference on Computer Vision, Corfu,<br> Greece, pp. 1150–1157.</p> 
        <p>This is the original famous SIFT paper. Most people know SIFT points for its robustness and scale, rotation invariant, but many might not notice that an object matching method is also mentioned in the paper.</p> 
        <p>This Matlab code is based on that method but uses SURF points instead of SIFT. To run the Matlab code you have to download the SURFmex library first.&nbsp;<a href="http://www.maths.lth.se/matematiklth/personal/petter/surfmex.php">http://www.maths.lth.se/matematiklth/personal/petter/surfmex.php</a><br> Remember to include the SURFmex library by right clicking the folder in Matlab and add subfolders to path.</p> 
        <p>You can then run Demo.m to see the matching result.</p> 
        <p>Demo.m first calls createTargetModel with a target image and an image with the contour of the target in the same image as input.&nbsp;createTargetModel then gathers the information needed for object matching and output it as targetModel.</p> 
        <p>matchTarget is then called with the targetModel and the test image as input. The contour of the target in the test image will then be shown.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2012/01/object.jpg"><img data-attachment-id="267" data-permalink="https://computervisionblog.wordpress.com/2012/01/15/object-matching-method-made-in-the-20th-century/object/" data-orig-file="https://computervisionblog.files.wordpress.com/2012/01/object.jpg?w=604" data-orig-size="350,291" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="object matching" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2012/01/object.jpg?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2012/01/object.jpg?w=604?w=350" class="aligncenter size-full wp-image-267" title="object matching" src="https://computervisionblog.files.wordpress.com/2012/01/object.jpg?w=604" alt="" srcset="https://computervisionblog.files.wordpress.com/2012/01/object.jpg 350w, https://computervisionblog.files.wordpress.com/2012/01/object.jpg?w=150 150w, https://computervisionblog.files.wordpress.com/2012/01/object.jpg?w=300 300w" sizes="(max-width: 350px) 100vw, 350px"></a></p> 
        <p>The algorithm works as follows. First the SURF points of the target image is extracted and stored. &nbsp;In matchTarget.m the SURF points of the test image is also calculated and each of them is matched to the most similar SURF point in the model. By using the scale and orientation of the SURF point descriptor, each matched SURF point pair has a translation from the target image to the test image.</p> 
        <p>Therefore 1 pair of correctly matched SURF points can decide the position, scale and orientation of the target in the test image. However most of the matched pairs aren’t correct, therefore we use all of the pairs to cast votes on what are the correct position, scale and orientation of the target in the test image.</p> 
        <p>The result that has the highest votes are then refined. A rotation matrix and &nbsp;a transition vector is then calculated based on the SURF point pairs in the result.</p> 
        <p>&nbsp;</p> 
        <p>&nbsp;</p> 
       </div> 
      </div> 
      <div id="showcomments" class="archive">
       <div class="divider"></div>
       <a href="https://computervisionblog.wordpress.com/2012/01/15/object-matching-method-made-in-the-20th-century/#comments">? View 3 Comments</a>
      </div> 
      <div class="post-244 post type-post status-publish format-standard hentry category-computer-vision category-paper-talk tag-computer-vision-2 tag-object-recognition" id="post-244"> 
       <p class="postmetadata"><a href="https://computervisionblog.wordpress.com/tag/computer-vision-2/" rel="tag">computer vision</a>, <a href="https://computervisionblog.wordpress.com/tag/object-recognition/" rel="tag">Object Recognition</a><br></p> 
       <h2><a href="https://computervisionblog.wordpress.com/2011/12/11/object-recognition/" rel="bookmark">Object recognition with limited (&lt; 6) training&nbsp;images</a></h2> 
       <small>In <a href="https://computervisionblog.wordpress.com/category/computer-vision/" rel="category tag">Computer Vision</a>, <a href="https://computervisionblog.wordpress.com/category/paper-talk/" rel="category tag">Paper Talk</a> on <strong>December 11, 2011</strong> at <strong>10:51 pm</strong></small> 
       <div class="entry"> 
        <blockquote>
         <p>by gooly</p>
        </blockquote> 
        <p>If you read my last post, you know I am working on a social app; it turned out that the social app didn’t work as we imagined due to some false assumptions we made; so we came up with a slightly different idea and is still testing it. In the mean time, I decided to post some vision work I made .</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2011/12/model2.jpg"><img data-attachment-id="245" data-permalink="https://computervisionblog.wordpress.com/2011/12/11/object-recognition/model2/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=604&amp;h=137" data-orig-size="878,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="object recognition" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=604&amp;h=137?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=604&amp;h=137?w=604" class="aligncenter size-full wp-image-245" title="object recognition" alt="" src="https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=604&amp;h=137" width="604" height="137" srcset="https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=601&amp;h=137 601w, https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=150&amp;h=34 150w, https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=300&amp;h=68 300w, https://computervisionblog.files.wordpress.com/2011/12/model2.jpg?w=768&amp;h=175 768w, https://computervisionblog.files.wordpress.com/2011/12/model2.jpg 878w" sizes="(max-width: 604px) 100vw, 604px"></a></p> 
        <p>The goal of this project is to recognize objects with limited training images even under slightly different angle. Only using a few images has a lot of advantages, specially for researcher that is lazy of collecting images and don’t have the patience to wait for several hours or days of training. The concept is simple, we look at the 4 training images we only have, and try to find what is common. Then we take the common structure and appearance and make them into a model.</p> 
        <p>So first, in order to be rotation and scale invariant we find the SURF points of all training.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2011/12/surf.jpg"><img data-attachment-id="246" data-permalink="https://computervisionblog.wordpress.com/2011/12/11/object-recognition/surf-2/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/12/surf.jpg?w=604" data-orig-size="602,401" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="find surf points" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/12/surf.jpg?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/12/surf.jpg?w=604?w=602" class="aligncenter size-full wp-image-246" title="find surf points" alt="" src="https://computervisionblog.files.wordpress.com/2011/12/surf.jpg?w=604" srcset="https://computervisionblog.files.wordpress.com/2011/12/surf.jpg 602w, https://computervisionblog.files.wordpress.com/2011/12/surf.jpg?w=150 150w, https://computervisionblog.files.wordpress.com/2011/12/surf.jpg?w=300 300w" sizes="(max-width: 602px) 100vw, 602px"></a>&nbsp;Then we find the ones that have similar appearance and also form the same structure.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2011/12/modeltri.png"><img data-attachment-id="247" data-permalink="https://computervisionblog.wordpress.com/2011/12/11/object-recognition/modeltri/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/12/modeltri.png?w=604" data-orig-size="462,351" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="surf model" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/12/modeltri.png?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/12/modeltri.png?w=604?w=462" class="aligncenter size-full wp-image-247" title="surf model" alt="" src="https://computervisionblog.files.wordpress.com/2011/12/modeltri.png?w=604" srcset="https://computervisionblog.files.wordpress.com/2011/12/modeltri.png 462w, https://computervisionblog.files.wordpress.com/2011/12/modeltri.png?w=150 150w, https://computervisionblog.files.wordpress.com/2011/12/modeltri.png?w=300 300w" sizes="(max-width: 462px) 100vw, 462px"></a></p> 
        <p>We build the structure by combining SURF points into a chain of triangles using dynamic programming. And then it is done. For test image, simply match the model to it’s SURF points. The results are fairly good on different objects and different angles. You can download the full paper <a href="https://docs.google.com/file/d/0BzICcXLKjPKnZi1lUW5wNHlOemc/edit?pli=1">here (A Probabilistic Model for Object Matching)</a>.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg"><img data-attachment-id="250" data-permalink="https://computervisionblog.wordpress.com/2011/12/11/object-recognition/box2m6/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg?w=604&amp;h=403" data-orig-size="604,403" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Surf Model" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg?w=604&amp;h=403?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg?w=604&amp;h=403?w=604" class="aligncenter size-full wp-image-250" title="Surf Model" alt="" src="https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg?w=604&amp;h=403" width="604" height="403" srcset="https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg 604w, https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg?w=150&amp;h=100 150w, https://computervisionblog.files.wordpress.com/2011/12/box2m6.jpg?w=300&amp;h=200 300w" sizes="(max-width: 604px) 100vw, 604px"></a><a href="https://computervisionblog.files.wordpress.com/2011/12/box1.jpg"><img data-attachment-id="251" data-permalink="https://computervisionblog.wordpress.com/2011/12/11/object-recognition/box1/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/12/box1.jpg?w=604" data-orig-size="602,401" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="SURF model" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/12/box1.jpg?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/12/box1.jpg?w=604?w=602" class="aligncenter size-full wp-image-251" title="SURF model" alt="" src="https://computervisionblog.files.wordpress.com/2011/12/box1.jpg?w=604" srcset="https://computervisionblog.files.wordpress.com/2011/12/box1.jpg 602w, https://computervisionblog.files.wordpress.com/2011/12/box1.jpg?w=150 150w, https://computervisionblog.files.wordpress.com/2011/12/box1.jpg?w=300 300w" sizes="(max-width: 602px) 100vw, 602px"></a></p> 
       </div> 
      </div> 
      <div id="showcomments" class="archive">
       <div class="divider"></div>
       <a href="https://computervisionblog.wordpress.com/2011/12/11/object-recognition/#comments">? Comment</a>
      </div> 
      <div class="post-84 post type-post status-publish format-standard hentry category-computer-vision category-paper-talk tag-object-recognition" id="post-84"> 
       <p class="postmetadata"><a href="https://computervisionblog.wordpress.com/tag/object-recognition/" rel="tag">Object Recognition</a><br></p> 
       <h2><a href="https://computervisionblog.wordpress.com/2011/04/16/paper-talk-unsupervised-learning-of-probabilistic-grammar-markov-models-for-object-categories/" rel="bookmark">Paper Talk: Unsupervised Learning of Probabilistic Grammar-Markov Models for Object&nbsp;Categories</a></h2> 
       <small>In <a href="https://computervisionblog.wordpress.com/category/computer-vision/" rel="category tag">Computer Vision</a>, <a href="https://computervisionblog.wordpress.com/category/paper-talk/" rel="category tag">Paper Talk</a> on <strong>April 16, 2011</strong> at <strong>9:35 am</strong></small> 
       <div class="entry"> 
        <blockquote>
         <p>written by Gooly<a href="https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg"><br> <img data-attachment-id="85" data-permalink="https://computervisionblog.wordpress.com/2011/04/16/paper-talk-unsupervised-learning-of-probabilistic-grammar-markov-models-for-object-categories/pgmm/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=604&amp;h=323" data-orig-size="891,477" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Got paper?" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=604&amp;h=323?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=604&amp;h=323?w=604" class="aligncenter size-full wp-image-85" title="Got paper?" src="https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=604&amp;h=323" alt="" width="604" height="323" srcset="https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=604&amp;h=323 604w, https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=150&amp;h=80 150w, https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=300&amp;h=161 300w, https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg?w=768&amp;h=411 768w, https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg 891w" sizes="(max-width: 604px) 100vw, 604px"><br> </a>“The triangle is a foundation to an offense.”<br> Bill Cartwright, 3 times NBA Champion<a href="https://computervisionblog.files.wordpress.com/2011/04/pgmm.jpg"><br> </a></p>
        </blockquote> 
        <p>What ever that means, triangle is&nbsp;definitely&nbsp;the foundation of this paper. Combining SIFT points into a chain of triangles allows us to use dynamic programming; the DP algorithm works as follows: after finding several triangles, we add each node to one of the triangles that most fit to create a new triangle for each iteration. &nbsp;See figure below.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg"><img data-attachment-id="90" data-permalink="https://computervisionblog.wordpress.com/2011/04/16/paper-talk-unsupervised-learning-of-probabilistic-grammar-markov-models-for-object-categories/pgmm3/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg?w=604" data-orig-size="555,174" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="chain of triangles" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg?w=604?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg?w=604?w=555" class="aligncenter size-full wp-image-90" title="chain of triangles" src="https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg?w=604" alt="" srcset="https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg 555w, https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg?w=150 150w, https://computervisionblog.files.wordpress.com/2011/04/pgmm3.jpg?w=300 300w" sizes="(max-width: 555px) 100vw, 555px"></a>Since for each node we store the best fit triangle that it can combine, at the next iteration when we want to add the best n5 (see above graph) , we only have to consider the best fit among all n5, all the n4 from last iteration and the n3 which each n4 pick . For a model with m nodes and an image with n nodes to match this is a drop roughly from O(n^m) to O(m*n^2).</p> 
        <p>The fitness of the triangle is a probability defined by both the surf appearance and there location plus orientation compared to the model.</p> 
        <p>The paper also provides an unsupervised way to learn the model by DP. ( which is probably the emphasis of the paper )</p> 
        <p>Some of the paper’s result are shown below.</p> 
        <p><a href="https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg"><img data-attachment-id="91" data-permalink="https://computervisionblog.wordpress.com/2011/04/16/paper-talk-unsupervised-learning-of-probabilistic-grammar-markov-models-for-object-categories/pgmm2/" data-orig-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg?w=604&amp;h=399" data-orig-size="679,449" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="triangle model" data-image-description="" data-medium-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg?w=604&amp;h=399?w=300" data-large-file="https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg?w=604&amp;h=399?w=604" class="aligncenter size-full wp-image-91" title="triangle model" src="https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg?w=604&amp;h=399" alt="" width="604" height="399" srcset="https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg?w=604&amp;h=399 604w, https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg?w=150&amp;h=99 150w, https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg?w=300&amp;h=198 300w, https://computervisionblog.files.wordpress.com/2011/04/pgmm2.jpg 679w" sizes="(max-width: 604px) 100vw, 604px"></a></p> 
       </div> 
      </div> 
      <div id="showcomments" class="archive">
       <div class="divider"></div>
       <a href="https://computervisionblog.wordpress.com/2011/04/16/paper-talk-unsupervised-learning-of-probabilistic-grammar-markov-models-for-object-categories/#comments">? Comment</a>
      </div> 
     </div> 
    </div> 
   </div> 
   <div id="sidebar"> 
    <div class="sleeve"> 
     <ul class="group"> 
      <li id="left_sidebar"> 
       <ul> 
        <li id="top-posts-3" class="widget widget_top-posts"><h3 class="widgettitle">Top Posts &amp; Pages</h3>
         <ul class="widgets-list-layout no-grav"> 
          <li> <a href="https://computervisionblog.wordpress.com/2013/06/01/cats-and-vision-is-vision-acquired-or-innate/" title="Cats and Vision: is vision acquired or innate?" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2013/06/catexperiment.jpg?resize=40%2C40" class="widgets-list-layout-blavatar" alt="Cats and Vision: is vision acquired or innate?" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2013/06/01/cats-and-vision-is-vision-acquired-or-innate/" class="bump-view" data-bump-view="tp"> Cats and Vision: is vision acquired or innate? </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2012/02/10/the-most-cited-papers-in-computer-vision/" title="The most cited papers in Computer Vision" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2012/02/publication.jpg?resize=40%2C40" class="widgets-list-layout-blavatar" alt="The most cited papers in Computer Vision" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2012/02/10/the-most-cited-papers-in-computer-vision/" class="bump-view" data-bump-view="tp"> The most cited papers in Computer Vision </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2011/04/13/matlab-read-all-images-from-a-folder-everything-starts-here/" title="MATLAB: Read all images in a folder; everything starts here " class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://i0.wp.com/www.mathworks.com/images/pixelclear.gif?resize=40%2C40" class="widgets-list-layout-blavatar" alt="MATLAB: Read all images in a folder; everything starts here " data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2011/04/13/matlab-read-all-images-from-a-folder-everything-starts-here/" class="bump-view" data-bump-view="tp"> MATLAB: Read all images in a folder; everything starts here </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2013/06/26/visual-illusion-saccadic-masking/" title="Visual Illusion: Chronostasis and Saccadic Masking" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2013/06/bestbodypaint.jpg?resize=40%2C40" class="widgets-list-layout-blavatar" alt="Visual Illusion: Chronostasis and Saccadic Masking" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2013/06/26/visual-illusion-saccadic-masking/" class="bump-view" data-bump-view="tp"> Visual Illusion: Chronostasis and Saccadic Masking </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2011/04/25/surf-on-images-feature-point-matching/" title="SURF On Images: feature point matching" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2011/04/surf.png?resize=40%2C40" class="widgets-list-layout-blavatar" alt="SURF On Images: feature point matching" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2011/04/25/surf-on-images-feature-point-matching/" class="bump-view" data-bump-view="tp"> SURF On Images: feature point matching </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2016/06/19/the-most-cited-papers-in-computer-vision-and-deep-learning/" title="The most cited papers in computer vision and deep learning" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2016/06/work-2005113-2-flat550x550075f-please-cite-my-papers.jpg?resize=40%2C40" class="widgets-list-layout-blavatar" alt="The most cited papers in computer vision and deep learning" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2016/06/19/the-most-cited-papers-in-computer-vision-and-deep-learning/" class="bump-view" data-bump-view="tp"> The most cited papers in computer vision and deep learning </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2012/06/10/object-recognition-with-point-cloud-library/" title="Object Recognition with Point Cloud Library" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2012/06/pcl-open-source.jpg?resize=40%2C40" class="widgets-list-layout-blavatar" alt="Object Recognition with Point Cloud Library" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2012/06/10/object-recognition-with-point-cloud-library/" class="bump-view" data-bump-view="tp"> Object Recognition with Point Cloud Library </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2012/11/18/rviz-a-good-reason-to-implement-a-vision-system-in-ros/" title="RVIZ: a good reason to implement a vision system in ROS" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2012/11/rviz.png?resize=40%2C40" class="widgets-list-layout-blavatar" alt="RVIZ: a good reason to implement a vision system in ROS" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2012/11/18/rviz-a-good-reason-to-implement-a-vision-system-in-ros/" class="bump-view" data-bump-view="tp"> RVIZ: a good reason to implement a vision system in ROS </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2017/03/20/generative-adversarial-networks/" title="Generative Adversarial Nets: Your Enemy is Your Best Friend?" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2017/03/arithmetic.png?resize=40%2C40" class="widgets-list-layout-blavatar" alt="Generative Adversarial Nets: Your Enemy is Your Best Friend?" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2017/03/20/generative-adversarial-networks/" class="bump-view" data-bump-view="tp"> Generative Adversarial Nets: Your Enemy is Your Best Friend? </a> 
           </div> </li> 
          <li> <a href="https://computervisionblog.wordpress.com/2013/11/20/book-it-openni-cookbook/" title="Book it: OpenNI Cookbook" class="bump-view" data-bump-view="tp"> <img width="40" height="40" src="https://computervisionblog.files.wordpress.com/2013/11/openni-cookbook.jpg?resize=40%2C40" class="widgets-list-layout-blavatar" alt="Book it: OpenNI Cookbook" data-pin-nopin="true"> </a> 
           <div class="widgets-list-layout-links"> 
            <a href="https://computervisionblog.wordpress.com/2013/11/20/book-it-openni-cookbook/" class="bump-view" data-bump-view="tp"> Book it: OpenNI Cookbook </a> 
           </div> </li> 
         </ul> </li> 
       </ul> </li> 
      <li id="middle_sidebar"> 
       <ul> 
        <li id="wp_tag_cloud-2" class="widget wp_widget_tag_cloud"><h3 class="widgettitle"></h3>
         <div style="overflow:hidden">
          <a href="https://computervisionblog.wordpress.com/tag/3d-models/" class="tag-cloud-link tag-link-175006 tag-link-position-1" style="font-size: 10.964705882353pt;" aria-label="3D models (2 items)">3D models</a> 
          <a href="https://computervisionblog.wordpress.com/tag/alogrithm/" class="tag-cloud-link tag-link-8184 tag-link-position-2" style="font-size: 8pt;" aria-label="alogrithm (1 item)">alogrithm</a> 
          <a href="https://computervisionblog.wordpress.com/tag/amazon/" class="tag-cloud-link tag-link-6602 tag-link-position-3" style="font-size: 8pt;" aria-label="amazon (1 item)">amazon</a> 
          <a href="https://computervisionblog.wordpress.com/tag/artificial-intelligence/" class="tag-cloud-link tag-link-12374 tag-link-position-4" style="font-size: 10.964705882353pt;" aria-label="artificial intelligence (2 items)">artificial intelligence</a> 
          <a href="https://computervisionblog.wordpress.com/tag/associating-grasping-with-convolutional-neural-network-features/" class="tag-cloud-link tag-link-544501853 tag-link-position-5" style="font-size: 8pt;" aria-label="Associating Grasping with Convolutional Neural Network Features (1 item)">Associating Grasping with Convolutional Neural Network Features</a> 
          <a href="https://computervisionblog.wordpress.com/tag/book-review/" class="tag-cloud-link tag-link-7215 tag-link-position-6" style="font-size: 8pt;" aria-label="book review (1 item)">book review</a> 
          <a href="https://computervisionblog.wordpress.com/tag/brain/" class="tag-cloud-link tag-link-16283 tag-link-position-7" style="font-size: 15.905882352941pt;" aria-label="brain (5 items)">brain</a> 
          <a href="https://computervisionblog.wordpress.com/tag/calcium-imaging/" class="tag-cloud-link tag-link-6267137 tag-link-position-8" style="font-size: 8pt;" aria-label="calcium imaging (1 item)">calcium imaging</a> 
          <a href="https://computervisionblog.wordpress.com/tag/capri/" class="tag-cloud-link tag-link-48576 tag-link-position-9" style="font-size: 8pt;" aria-label="capri (1 item)">capri</a> 
          <a href="https://computervisionblog.wordpress.com/tag/chrome-extension/" class="tag-cloud-link tag-link-27781379 tag-link-position-10" style="font-size: 8pt;" aria-label="chrome extension (1 item)">chrome extension</a> 
          <a href="https://computervisionblog.wordpress.com/tag/computer-vision-2/" class="tag-cloud-link tag-link-36435491 tag-link-position-11" style="font-size: 22pt;" aria-label="computer vision (13 items)">computer vision</a> 
          <a href="https://computervisionblog.wordpress.com/tag/convolutional-neural-network/" class="tag-cloud-link tag-link-125262826 tag-link-position-12" style="font-size: 12.941176470588pt;" aria-label="convolutional neural network (3 items)">convolutional neural network</a> 
          <a href="https://computervisionblog.wordpress.com/tag/deep-learning/" class="tag-cloud-link tag-link-1131271 tag-link-position-13" style="font-size: 8pt;" aria-label="deep learning (1 item)">deep learning</a> 
          <a href="https://computervisionblog.wordpress.com/tag/google-scholar/" class="tag-cloud-link tag-link-39575 tag-link-position-14" style="font-size: 8pt;" aria-label="google scholar (1 item)">google scholar</a> 
          <a href="https://computervisionblog.wordpress.com/tag/hmp/" class="tag-cloud-link tag-link-62247 tag-link-position-15" style="font-size: 8pt;" aria-label="HMP (1 item)">HMP</a> 
          <a href="https://computervisionblog.wordpress.com/tag/icra2017/" class="tag-cloud-link tag-link-545282339 tag-link-position-16" style="font-size: 8pt;" aria-label="ICRA2017 (1 item)">ICRA2017</a> 
          <a href="https://computervisionblog.wordpress.com/tag/image-query/" class="tag-cloud-link tag-link-76929009 tag-link-position-17" style="font-size: 8pt;" aria-label="image query (1 item)">image query</a> 
          <a href="https://computervisionblog.wordpress.com/tag/image-recognition/" class="tag-cloud-link tag-link-420336 tag-link-position-18" style="font-size: 8pt;" aria-label="image recognition (1 item)">image recognition</a> 
          <a href="https://computervisionblog.wordpress.com/tag/images/" class="tag-cloud-link tag-link-914 tag-link-position-19" style="font-size: 8pt;" aria-label="images (1 item)">images</a> 
          <a href="https://computervisionblog.wordpress.com/tag/kinect/" class="tag-cloud-link tag-link-18608004 tag-link-position-20" style="font-size: 14.588235294118pt;" aria-label="kinect (4 items)">kinect</a> 
          <a href="https://computervisionblog.wordpress.com/tag/local-distance-learning/" class="tag-cloud-link tag-link-326056289 tag-link-position-21" style="font-size: 8pt;" aria-label="local distance learning (1 item)">local distance learning</a> 
          <a href="https://computervisionblog.wordpress.com/tag/machine-learning-2/" class="tag-cloud-link tag-link-34964372 tag-link-position-22" style="font-size: 12.941176470588pt;" aria-label="machine learning (3 items)">machine learning</a> 
          <a href="https://computervisionblog.wordpress.com/tag/matlab-2/" class="tag-cloud-link tag-link-35197863 tag-link-position-23" style="font-size: 12.941176470588pt;" aria-label="matlab (3 items)">matlab</a> 
          <a href="https://computervisionblog.wordpress.com/tag/neruons/" class="tag-cloud-link tag-link-94823534 tag-link-position-24" style="font-size: 8pt;" aria-label="Neruons (1 item)">Neruons</a> 
          <a href="https://computervisionblog.wordpress.com/tag/neural-science-2/" class="tag-cloud-link tag-link-55334838 tag-link-position-25" style="font-size: 8pt;" aria-label="neural science (1 item)">neural science</a> 
          <a href="https://computervisionblog.wordpress.com/tag/object-recognition/" class="tag-cloud-link tag-link-1199747 tag-link-position-26" style="font-size: 17.058823529412pt;" aria-label="Object Recognition (6 items)">Object Recognition</a> 
          <a href="https://computervisionblog.wordpress.com/tag/octave/" class="tag-cloud-link tag-link-580231 tag-link-position-27" style="font-size: 8pt;" aria-label="octave (1 item)">octave</a> 
          <a href="https://computervisionblog.wordpress.com/tag/openni/" class="tag-cloud-link tag-link-47494039 tag-link-position-28" style="font-size: 10.964705882353pt;" aria-label="openni (2 items)">openni</a> 
          <a href="https://computervisionblog.wordpress.com/tag/optogenetics/" class="tag-cloud-link tag-link-22167017 tag-link-position-29" style="font-size: 8pt;" aria-label="Optogenetics (1 item)">Optogenetics</a> 
          <a href="https://computervisionblog.wordpress.com/tag/organizing-publication/" class="tag-cloud-link tag-link-137409991 tag-link-position-30" style="font-size: 8pt;" aria-label="organizing publication (1 item)">organizing publication</a> 
          <a href="https://computervisionblog.wordpress.com/tag/point-cloud/" class="tag-cloud-link tag-link-7889602 tag-link-position-31" style="font-size: 8pt;" aria-label="point cloud (1 item)">point cloud</a> 
          <a href="https://computervisionblog.wordpress.com/tag/point-cloud-library-2/" class="tag-cloud-link tag-link-95747931 tag-link-position-32" style="font-size: 8pt;" aria-label="point cloud library (1 item)">point cloud library</a> 
          <a href="https://computervisionblog.wordpress.com/tag/primesense/" class="tag-cloud-link tag-link-15414327 tag-link-position-33" style="font-size: 8pt;" aria-label="primesense (1 item)">primesense</a> 
          <a href="https://computervisionblog.wordpress.com/tag/publication/" class="tag-cloud-link tag-link-25503 tag-link-position-34" style="font-size: 10.964705882353pt;" aria-label="publication (2 items)">publication</a> 
          <a href="https://computervisionblog.wordpress.com/tag/quadrotor/" class="tag-cloud-link tag-link-18643859 tag-link-position-35" style="font-size: 8pt;" aria-label="quadrotor (1 item)">quadrotor</a> 
          <a href="https://computervisionblog.wordpress.com/tag/robotics/" class="tag-cloud-link tag-link-13426 tag-link-position-36" style="font-size: 8pt;" aria-label="Robotics (1 item)">Robotics</a> 
          <a href="https://computervisionblog.wordpress.com/tag/ros/" class="tag-cloud-link tag-link-30475 tag-link-position-37" style="font-size: 8pt;" aria-label="ros (1 item)">ros</a> 
          <a href="https://computervisionblog.wordpress.com/tag/ros-deep-vision/" class="tag-cloud-link tag-link-544501851 tag-link-position-38" style="font-size: 8pt;" aria-label="ros-deep-vision (1 item)">ros-deep-vision</a> 
          <a href="https://computervisionblog.wordpress.com/tag/rviz/" class="tag-cloud-link tag-link-77878646 tag-link-position-39" style="font-size: 8pt;" aria-label="rviz (1 item)">rviz</a> 
          <a href="https://computervisionblog.wordpress.com/tag/sift/" class="tag-cloud-link tag-link-2434298 tag-link-position-40" style="font-size: 8pt;" aria-label="SIFT (1 item)">SIFT</a> 
          <a href="https://computervisionblog.wordpress.com/tag/sift-feature-point/" class="tag-cloud-link tag-link-55894248 tag-link-position-41" style="font-size: 8pt;" aria-label="SIFT feature point (1 item)">SIFT feature point</a> 
          <a href="https://computervisionblog.wordpress.com/tag/social-network/" class="tag-cloud-link tag-link-40461 tag-link-position-42" style="font-size: 8pt;" aria-label="social network (1 item)">social network</a> 
          <a href="https://computervisionblog.wordpress.com/tag/sparse-coding-2/" class="tag-cloud-link tag-link-236802251 tag-link-position-43" style="font-size: 8pt;" aria-label="sparse coding (1 item)">sparse coding</a> 
          <a href="https://computervisionblog.wordpress.com/tag/stereo-vision/" class="tag-cloud-link tag-link-7005360 tag-link-position-44" style="font-size: 8pt;" aria-label="stereo vision (1 item)">stereo vision</a> 
          <a href="https://computervisionblog.wordpress.com/tag/surf/" class="tag-cloud-link tag-link-13720 tag-link-position-45" style="font-size: 8pt;" aria-label="SURF (1 item)">SURF</a> 
          <a href="https://computervisionblog.wordpress.com/tag/surf-feature-point/" class="tag-cloud-link tag-link-55894246 tag-link-position-46" style="font-size: 12.941176470588pt;" aria-label="SURF feature point (3 items)">SURF feature point</a> 
          <a href="https://computervisionblog.wordpress.com/tag/surfmex/" class="tag-cloud-link tag-link-21332099 tag-link-position-47" style="font-size: 8pt;" aria-label="surfmex (1 item)">surfmex</a> 
          <a href="https://computervisionblog.wordpress.com/tag/virtual-world/" class="tag-cloud-link tag-link-114877 tag-link-position-48" style="font-size: 8pt;" aria-label="virtual world (1 item)">virtual world</a> 
          <a href="https://computervisionblog.wordpress.com/tag/vision/" class="tag-cloud-link tag-link-1912 tag-link-position-49" style="font-size: 12.941176470588pt;" aria-label="vision (3 items)">vision</a> 
          <a href="https://computervisionblog.wordpress.com/tag/vision-library/" class="tag-cloud-link tag-link-8942172 tag-link-position-50" style="font-size: 8pt;" aria-label="vision library (1 item)">vision library</a> 
          <a href="https://computervisionblog.wordpress.com/tag/visual-cortex/" class="tag-cloud-link tag-link-2122877 tag-link-position-51" style="font-size: 8pt;" aria-label="visual cortex (1 item)">visual cortex</a> 
          <a href="https://computervisionblog.wordpress.com/tag/visual-feature/" class="tag-cloud-link tag-link-48874091 tag-link-position-52" style="font-size: 8pt;" aria-label="visual feature (1 item)">visual feature</a> 
          <a href="https://computervisionblog.wordpress.com/tag/visual-illusion/" class="tag-cloud-link tag-link-372480 tag-link-position-53" style="font-size: 10.964705882353pt;" aria-label="Visual Illusion (2 items)">Visual Illusion</a> 
          <a href="https://computervisionblog.wordpress.com/tag/xtion/" class="tag-cloud-link tag-link-29349336 tag-link-position-54" style="font-size: 8pt;" aria-label="xtion (1 item)">xtion</a>
         </div></li>
        <li id="blog-stats-3" class="widget widget_blog-stats"><h3 class="widgettitle">Blog Stats</h3> 
         <ul> 
          <li>245,911 hits</li> 
         </ul> </li>
        <li id="archives-2" class="widget widget_archive"><h3 class="widgettitle">Archives</h3> 
         <ul> 
          <li><a href="https://computervisionblog.wordpress.com/2017/07/">July 2017</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2017/04/">April 2017</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2017/03/">March 2017</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2016/10/">October 2016</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2016/06/">June 2016</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2016/04/">April 2016</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2016/01/">January 2016</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2015/11/">November 2015</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2015/05/">May 2015</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2015/02/">February 2015</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2014/10/">October 2014</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2014/09/">September 2014</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2014/05/">May 2014</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2014/03/">March 2014</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2014/02/">February 2014</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2013/11/">November 2013</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2013/09/">September 2013</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2013/06/">June 2013</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2013/05/">May 2013</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2013/01/">January 2013</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2012/11/">November 2012</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2012/09/">September 2012</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2012/07/">July 2012</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2012/06/">June 2012</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2012/04/">April 2012</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2012/02/">February 2012</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2012/01/">January 2012</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2011/12/">December 2011</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2011/11/">November 2011</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2011/05/">May 2011</a></li> 
          <li><a href="https://computervisionblog.wordpress.com/2011/04/">April 2011</a></li> 
         </ul> </li> 
       </ul> </li> 
      <li id="right_sidebar"> 
       <ul> 
        <li id="twitter_timeline-2" class="widget widget_twitter_timeline"><h3 class="widgettitle">Follow me on Twitter</h3><a class="twitter-timeline" data-width="225" data-height="400" data-theme="light" data-link-color="#f96e5b" data-border-color="#e8e8e8" data-tweet-limit="3" data-lang="EN" data-partner="jetpack" data-widget-id="402060499409305600">My Tweets</a></li>
        <li id="text-2" class="widget widget_text"> 
         <div class="textwidget">
          connect with me on 
          <a href="https://plus.google.com/101317515434715512426?rel=author"> Google+ </a>
         </div> </li> 
       </ul> </li> 
     </ul> 
     <div class="closer"></div> 
    </div> 
   </div> 
  </div> 
  <div id="footer"> 
   <p> <a href="https://wordpress.com/?ref=footer_blog">Blog at WordPress.com.</a> </p> 
  </div>  
  <!--  --> 
  <script type="text/javascript" src="//0.gravatar.com/js/gprofiles.js?ver=201740y"></script> 
  <script type="text/javascript">
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script> 
  <script type="text/javascript" src="https://s1.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1380573781h"></script> 
  <script>
		//initialize and attach hovercards to all gravatars
		jQuery( document ).ready( function( $ ) {

			if (typeof Gravatar === "undefined"){
				return;
			}

			if ( typeof Gravatar.init !== "function" ) {
				return;
			}			

			Gravatar.profile_cb = function( hash, id ) {
				WPGroHo.syncProfileData( hash, id );
			};
			Gravatar.my_hash = WPGroHo.my_hash;
			Gravatar.init( 'body', '#wp-admin-bar-my-account' );
		});
	</script> 
  <div style="display:none"> 
  </div> 
  <div id="carousel-reblog-box"> 
   <form action="#" name="carousel-reblog"> 
    <textarea id="carousel-reblog-content" name="carousel-reblog-content" placeholder="Add your thoughts here... (optional)"></textarea> 
    <label for="carousel-reblog-to-blog-id" id="carousel-reblog-lblogid">Post to</label> 
    <select name="carousel-reblog-to-blog-id" id="carousel-reblog-to-blog-id"> </select> 
    <div class="submit"> 
     <span class="canceltext"><a href="#" class="cancel">Cancel</a></span> 
     <input type="submit" name="carousel-reblog-submit" class="button" id="carousel-reblog-submit" value="Reblog Post"> 
     <input type="hidden" id="carousel-reblog-blog-id" value="22115651"> 
     <input type="hidden" id="carousel-reblog-blog-url" value="https://computervisionblog.wordpress.com"> 
     <input type="hidden" id="carousel-reblog-blog-title" value="the Serious Computer Vision Blog"> 
     <input type="hidden" id="carousel-reblog-post-url" value=""> 
     <input type="hidden" id="carousel-reblog-post-title" value=""> 
    </div> 
    <input type="hidden" id="_wpnonce" name="_wpnonce" value="a108bdcde1">
    <input type="hidden" name="_wp_http_referer" value="/tag/object-recognition/"> 
   </form> 
   <div class="arrow"></div> 
  </div> 
  <link rel="stylesheet" id="all-css-0-2" href="https://s1.wp.com/wp-content/mu-plugins/carousel/jetpack-carousel.css?m=1481571546h" type="text/css" media="all"> 
  <!--[if lte IE 8]>
<link rel='stylesheet' id='jetpack-carousel-ie8fix-css'  href='https://s1.wp.com/wp-content/mu-plugins/carousel/jetpack-carousel-ie8fix.css?m=1412618825h&#038;ver=20121024' type='text/css' media='all' />
<![endif]--> 
  <script type="text/javascript">
/* <![CDATA[ */
var actionbardata = {"siteID":"22115651","siteName":"the Serious Computer Vision Blog","siteURL":"http:\/\/computervisionblog.wordpress.com","icon":"<img alt='' src='https:\/\/secure.gravatar.com\/blavatar\/053089fcaeb6aacc5d7c36792f0319e6?s=50&d=https%3A%2F%2Fs1.wp.com%2Fi%2Flogo%2Fwpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/depo-masthead","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/computervisionblog.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fcomputervisionblog.wordpress.com%2F2015%2F02%2F08%2Flocal-distance-learning-in-object-recognition%2F","themeURL":"","xhrURL":"https:\/\/computervisionblog.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"c71ad1571e","isSingular":"","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"eda7df8374\" \/>","referer":"https:\/\/computervisionblog.wordpress.com\/tag\/object-recognition\/","canFollow":"1","feedID":"1645652","statusMessage":"","customizeLink":"https:\/\/computervisionblog.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fcomputervisionblog.wordpress.com%2Ftag%2Fobject-recognition%2F","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: DePo Masthead","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 314 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/computervisionblog.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fcomputervisionblog.wordpress.com%2F2015%2F02%2F08%2Flocal-distance-learning-in-object-recognition%2F\">Log in now.<\/a>","stats":"Stats"}};
/* ]]> */
</script> 
  <script type="text/javascript">
/* <![CDATA[ */
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/computervisionblog.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"f3ca80e632","display_exif":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","comment_registration":"0","require_name_email":"0","login_url":"https:\/\/computervisionblog.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fcomputervisionblog.wordpress.com%2F2015%2F02%2F08%2Flocal-distance-learning-in-object-recognition%2F","blog_id":"22115651","local_comments_commenting_as":"<fieldset><label for=\"email\">Email<\/label> <input type=\"text\" name=\"email\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-email-field\" \/><\/fieldset><fieldset><label for=\"author\">Name<\/label> <input type=\"text\" name=\"author\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-author-field\" \/><\/fieldset><fieldset><label for=\"url\">Website<\/label> <input type=\"text\" name=\"url\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-url-field\" \/><\/fieldset>","reblog":"Reblog","reblogged":"Reblogged","reblog_add_thoughts":"Add your thoughts here... (optional)","reblogging":"Reblogging...","post_reblog":"Post Reblog","stats_query_args":"blog=22115651&v=wpcom&tz=-7&user_id=0&subd=computervisionblog","is_public":"1","reblog_enabled":""};
/* ]]> */
</script> 
  <script type="text/javascript" src="https://s2.wp.com/_static/??-eJx9T9sOgjAM/SFHgcTLi/FTzNwa7djK3AX07x2JEB4IT21Oz6UHRi9Uzwk5gYmgcSCF/lOZeIDVyWXhbX4SR7DUYYR3xowvydpi2CEbTF6qDu7ECtJIKWEQiRxaYtzRSe2IxUMGcDIWTdlEP2AIpEv4gu05qEQ9Tw7LNrPLLzZPPqVw9MRbuCkFw/c/qjVrI0vJ0OeIdq4rZqBobu7aHOv2dGnObW1++TuJYQ=="></script> 
  <script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script> 
  <script type="text/javascript">
	var skimlinks_pub_id = "725X584219"
	var skimlinks_sitename = "computervisionblog.wordpress.com";
	</script> 
  <script type="text/javascript" defer src="https://s.skimresources.com/js/725X1342.skimlinks.js"></script>
  <script src="//stats.wp.com/w.js?56" type="text/javascript" async defer></script> 
  <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'22115651','blog_tz':'-7','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'22115651','v':'wpcom','tz':'-7','user_id':'0','subd':'computervisionblog'}]);
_stq.push(['extra', {'crypt':'UE5XaGUuOTlwaD85flAmcm1mcmZsaDhkV11YdTdvUG14Q2VDQTR4LlUsLi82dU1mai9BMlVKPytqWkpxbS9HYjlZJUc/aTVFT05kUC0/bUN0Nz9Jd0c9P29JazBKOC1HcUtEXW4zPTk1LFVxd2lvbDk4XSVrME8udlB5RFRGQm1BXzRYXUtuaXpvUGRiM11lUERvLWhraWVja3oyYV1pMmJtM3NfLy9TLGhYaHVCX0tiY10/SDAyMUFyOVFPLDBMRUY4W01MZTRyeSxON1VaMFo3TTR4a1JEW0ZBRGFPK1FuUTl4UCtbWV9UMmImL1d+TFVVMWI9PWEuUkczN3Q0LVo9Rm1NdVdlaGNNb005PWN3aHNfMERwdG1BdThIRUh3TUVpZD1oemotSCt2bCVwTQ=='}]);
_stq.push([ 'clickTrackerInit', '22115651', '0' ]);
	</script> 
  <noscript>
   <img src="https://pixel.wp.com/b.gif?v=noscript" style="height:0px;width:0px;overflow:hidden" alt="">
  </noscript> 
  <script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>   
 </body>
</html>