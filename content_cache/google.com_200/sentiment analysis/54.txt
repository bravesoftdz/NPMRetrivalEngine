<!doctype html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:dc="http://purl.org/dc/terms/" xmlns:doi="http://dx.doi.org/" lang="en" xml:lang="en" itemscope itemtype="http://schema.org/Article" class="no-js">
 <head prefix="og: http://ogp.me/ns#"> 
  <title>Multilingual Twitter Sentiment Classification: The Role of Human Annotators</title> 
  <link rel="stylesheet" href="/plosone/resource/compiled/asset_V7OLX4LSII2Y4DADFWUTOT572TJ6EECJ.css"> 
  <!-- allows for  extra head tags --> 
  <!-- hello --> 
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600"> 
  <link media="print" rel="stylesheet" type="text/css" href="/plosone/resource/css/print.css"> 
  <script type="text/javascript">
        var siteUrlPrefix = "/plosone/";
    </script> 
  <script src="/plosone/resource/compiled/asset_SC5JIUGEUPR4P4P6VBUINUVOVUSU3NRY.js"></script> 
  <link rel="shortcut icon" href="/plosone/resource/img/favicon.ico" type="image/x-icon"> 
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"> 
  <link rel="canonical" href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036"> 
  <meta name="description" content="What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered."> 
  <meta name="keywords" content="Twitter,Support vector machines,Machine learning algorithms,Slovenian people,Albanian people,Machine learning,Bulgarian people,Serbian people"> 
  <meta name="citation_doi" content="10.1371/journal.pone.0155036"> 
  <meta name="citation_author" content="Igor Mozeti?"> 
  <meta name="citation_author_institution" content="Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia"> 
  <meta name="citation_author" content="Miha Gr?ar"> 
  <meta name="citation_author_institution" content="Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia"> 
  <meta name="citation_author" content="Jasmina Smailovi?"> 
  <meta name="citation_author_institution" content="Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia"> 
  <meta name="citation_title" content="Multilingual Twitter Sentiment Classification: The Role of Human Annotators"> 
  <meta itemprop="name" content="Multilingual Twitter Sentiment Classification: The Role of Human Annotators"> 
  <meta name="citation_journal_title" content="PLOS ONE"> 
  <meta name="citation_journal_abbrev" content="PLOS ONE"> 
  <meta name="citation_date" content="May 5, 2016"> 
  <meta name="citation_firstpage" content="e0155036"> 
  <meta name="citation_issue" content="5"> 
  <meta name="citation_volume" content="11"> 
  <meta name="citation_issn" content="1932-6203"> 
  <meta name="citation_publisher" content="Public Library of Science"> 
  <meta name="citation_pdf_url" content="http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0155036&amp;type=printable"> 
  <meta name="dc.identifier" content="10.1371/journal.pone.0155036"> 
  <meta name="twitter:card" content="summary"> 
  <meta name="twitter:site" content="@plosone"> 
  <meta name="twitter:title" content="Multilingual Twitter Sentiment Classification: The Role of Human Annotators"> 
  <meta property="twitter:description" content="What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered."> 
  <meta property="twitter:image" content="http://journals.plos.org/plosone/article/figure/image?id=10.1371/journal.pone.0155036.g012&amp;size=inline"> 
  <meta property="og:type" content="article"> 
  <meta property="og:url" content="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036"> 
  <meta property="og:title" content="Multilingual Twitter Sentiment Classification: The Role of Human Annotators"> 
  <meta property="og:description" content="What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered."> 
  <meta property="og:image" content="http://journals.plos.org/plosone/article/figure/image?id=10.1371/journal.pone.0155036.g012&amp;size=inline"> 
  <meta name="citation_reference" content="citation_title=Sentiment analysis and opinion mining;citation_author=B Liu;citation_journal_title=Synthesis Lectures on Human Language Technologies;citation_volume=5;citation_number=5;citation_issue=1;citation_first_page=1;citation_last_page=167;citation_publication_date=2012;"> 
  <meta name="citation_reference" content="citation_title=Sentiment Analysis: Mining Opinions, Sentiments, and Emotions;citation_author=B Liu;citation_publication_date=2015;citation_publisher=Cambridge University Press"> 
  <meta name="citation_reference" content="citation_title=Human language reveals a universal positivity bias;citation_author=PS Dodds;citation_author=EM Clark;citation_author=S Desu;citation_author=MR Frank;citation_author=AJ Reagan;citation_author=JR Williams;citation_journal_title=Proceedings of the National Academy of Sciences;citation_volume=112;citation_number=112;citation_issue=8;citation_first_page=2389;citation_last_page=2394;citation_publication_date=2015;"> 
  <meta name="citation_reference" content="citation_title=SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining;citation_author=S Baccianella;citation_author=A Esuli;citation_author=F Sebastiani;citation_journal_title=LREC;citation_volume=vol. 10;citation_number=10;citation_first_page=2200;citation_last_page=2204;citation_publication_date=2010;"> 
  <meta name="citation_reference" content="citation_title=Sentiment analysis in Twitter;citation_author=E Martínez-Cámara;citation_author=MT Martín-Valdivia;citation_author=LA Urena-López;citation_author=AR Montejo-Ráez;citation_journal_title=Natural Language Engineering;citation_volume=20;citation_number=20;citation_issue=1;citation_first_page=1;citation_last_page=28;citation_publication_date=2014;"> 
  <meta name="citation_reference" content="Kolchyna, O, Souza, TTP, Treleaven, PC, Aste, T. Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination. arXiv preprint arXiv:150700955. 2015;."> 
  <meta name="citation_reference" content="citation_title=Content Analysis, An Introduction to Its Methodology;citation_author=K Krippendorff;citation_publication_date=2012;citation_publisher=Sage Publications"> 
  <meta name="citation_reference" content="citation_title=Emotional dynamics in the age of misinformation;citation_author=F Zollo;citation_author=P Kralj Novak;citation_author=M Del Vicario;citation_author=A Bessi;citation_author=I Mozeti?;citation_author=A Scala;citation_journal_title=PLoS ONE;citation_volume=10;citation_number=10;citation_issue=9;citation_first_page=e0138740;citation_publication_date=2015;"> 
  <meta name="citation_reference" content="citation_title=The effects of Twitter sentiment on stock price returns;citation_author=G Ranco;citation_author=D Aleksovski;citation_author=G Caldarelli;citation_author=M Gr?ar;citation_author=I Mozeti?;citation_journal_title=PLoS ONE;citation_volume=10;citation_number=10;citation_issue=9;citation_first_page=e0138441;citation_publication_date=2015;"> 
  <meta name="citation_reference" content="citation_title=Sentiment leaning of influential communities in social networks;citation_author=B Sluban;citation_author=J Smailovi?;citation_author=S Battiston;citation_author=I Mozeti?;citation_journal_title=Computational Social Networks;citation_volume=2;citation_number=2;citation_issue=9;citation_first_page=1;citation_last_page=21;citation_publication_date=2015;"> 
  <meta name="citation_reference" content="citation_title=Sentiment of emojis;citation_author=P Kralj Novak;citation_author=J Smailovi?;citation_author=B Sluban;citation_author=I Mozeti?;citation_journal_title=PLoS ONE;citation_volume=10;citation_number=10;citation_issue=12;citation_first_page=e0144296;citation_publication_date=2015;"> 
  <meta name="citation_reference" content="citation_title=Bootstrapping: A Nonparametric Approach to Statistical Inference;citation_author=CZ Mooney;citation_author=RD Duval;citation_publication_date=1993;citation_publisher=Sage Publications"> 
  <meta name="citation_reference" content="citation_title=The Nature of Statistical Learning Theory;citation_author=VN Vapnik;citation_publication_date=1995;citation_publisher=Springer"> 
  <meta name="citation_reference" content="citation_title=Artificial Intelligence: A Modern Approach;citation_author=S Russell;citation_author=P Norvig;citation_publication_date=2003;citation_publisher=Prentice Hall"> 
  <meta name="citation_reference" content="citation_title=A comparison of alternative tests of significance for the problem of m rankings;citation_author=M Friedman;citation_journal_title=Annals of Mathematical Statistics;citation_volume=11;citation_number=11;citation_first_page=86;citation_last_page=92;citation_publication_date=1940;"> 
  <meta name="citation_reference" content="citation_title=Distribution-free multiple comparisons;citation_author=PB Nemenyi;citation_publication_date=1963;citation_publisher=Princeton University"> 
  <meta name="citation_reference" content="Saif H, Fernández M, He Y, Alani H. Evaluation datasets for Twitter sentiment analysis: A survey and a new dataset, the STS-Gold. In: 1st Intl. Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI (ESSEM); 2013."> 
  <meta name="citation_reference" content="dos Santos CN, Gatti M. Deep convolutional neural networks for sentiment analysis of short texts. In: Proc. 25th Intl. Conf. on Computational Linguistics (COLING), Dublin, Ireland; 2014."> 
  <meta name="citation_reference" content="dos Santos CN. Think positive: Towards Twitter sentiment analysis from scratch. In: Proc. 8th Intl. Workshop on Semantic Evaluation (SemEval); 2014. p. 647–651."> 
  <meta name="citation_reference" content="Tang D, Wei F, Qin B, Liu T, Zhou M. Coooolll: A deep learning system for Twitter sentiment classification. In: Proc. 8th Intl. Workshop on Semantic Evaluation (SemEval); 2014. p. 208–212."> 
  <meta name="citation_reference" content="Ghosh A, Li G, Veale T, Rosso P, Shutova E, Barnden J, et al. SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter. In: Proc. 9th Intl. Workshop on Semantic Evaluation (SemEval); 2015. p. 470–478."> 
  <meta name="citation_reference" content="citation_title=Using hashtags to capture fine emotion categories from tweets;citation_author=SM Mohammad;citation_author=S Kiritchenko;citation_journal_title=Computational Intelligence;citation_volume=31;citation_number=31;citation_issue=2;citation_first_page=301;citation_last_page=326;citation_publication_date=2015;"> 
  <meta name="citation_reference" content="citation_title=Information Retrieval;citation_author=CJ Van Rijsbergen;citation_publication_date=1979;citation_publisher=Butterworth"> 
  <meta name="citation_reference" content="citation_title=Sentiment analysis of short informal texts;citation_author=S Kiritchenko;citation_author=X Zhu;citation_author=SM Mohammad;citation_journal_title=Journal of Artificial Intelligence Research;citation_first_page=723;citation_last_page=762;citation_publication_date=2014;"> 
  <meta name="citation_reference" content="citation_title=Evaluation methods for ordinal classificatio;citation_inbook_title=Advances in Artificial Intelligence;citation_author=L Gaudette;citation_author=N Japkowicz;citation_first_page=207;citation_last_page=210;citation_publication_date=2009;citation_publisher=Springer"> 
  <meta name="citation_reference" content="Go A, Bhayani R, Huang L. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford. 2009;p. 1–12."> 
  <meta name="citation_reference" content="Rosenthal S, Nakov P, Kiritchenko S, Mohammad SM, Ritter A, Stoyanov V. SemEval-2015 task 10: Sentiment Analysis in Twitter. In: Proc. 9th Intl. Workshop on Semantic Evaluation (SemEval); 2015. p. 451–463."> 
  <meta name="citation_reference" content="Haldenwang N, Vornberger O. Sentiment Uncertainty and Spam in Twitter Streams and Its Implications for General Purpose Realtime Sentiment Analysis. In: Proc. Intl. Conf. of the German Society for Computational Linguistics and Language Technology (GSCL); 2015. p. 157–159."> 
  <meta name="citation_reference" content="citation_title=Semantic patterns for sentiment analysis of Twitter;citation_inbook_title=The Semantic Web (ISWC);citation_author=H Saif;citation_author=Y He;citation_author=M Fernandez;citation_author=H Alani;citation_first_page=324;citation_last_page=340;citation_publication_date=2014;citation_publisher=Springer"> 
  <meta name="citation_reference" content="Parikh R, Movassate M. Sentiment analysis of user-generated Twitter updates using various classification techniques. CS224N Final Report. 2009;p. 1–18."> 
  <meta name="citation_reference" content="citation_title=Twitter as a Corpus for Sentiment Analysis and Opinion Mining;citation_inbook_title=LREC;citation_author=A Pak;citation_author=P Paroubek;citation_volume=vol. 10;citation_number=10;citation_first_page=1320;citation_last_page=1326;citation_publication_date=2010;"> 
  <meta name="citation_reference" content="Asiaee T A, Tepper M, Banerjee A, Sapiro G. If you are happy and you know it… tweet. In: Proc. 21st ACM Intl. Conf. on Information and Knowledge Management. ACM; 2012. p. 1602–1606."> 
  <meta name="citation_reference" content="citation_title=Semantic sentiment analysis of Twitter;citation_inbook_title=The Semantic Web (ISWC). vol. 7649 of Lecture Notes in Computer Science;citation_author=H Saif;citation_author=Y He;citation_author=H Alani;citation_first_page=508;citation_last_page=524;citation_publication_date=2012;citation_publisher=Springer"> 
  <meta name="citation_reference" content="Barbosa L, Feng J. Robust sentiment detection on Twitter from biased and noisy data. In: Proc. 23rd Intl. Conf. on Computational Linguistics: Posters. ACL; 2010. p. 36–44."> 
  <meta name="citation_reference" content="Jiang L, Yu M, Zhou M, Liu X, Zhao T. Target-dependent Twitter sentiment classification. In: Proc. 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. ACL; 2011. p. 151–160."> 
  <meta name="citation_reference" content="Davidov D, Tsur O, Rappoport A. Enhanced sentiment learning using Twitter hashtags and smileys. In: Proc. 23rd Intl. Conf. Computational Linguistics: Posters. ACL; 2010. p. 241–249."> 
  <meta name="citation_reference" content="Kouloumpis E, Wilson T, Moore J. Twitter sentiment analysis: The good the bad and the OMG! In: Proc. Intl. Conf. on Weblogs and Social Media (ICWSM). vol. 11; 2011. p. 538–541."> 
  <meta name="citation_reference" content="citation_title=A comparison of methods for multiclass Support Vector Machines;citation_author=C Hsu;citation_author=C Lin;citation_journal_title=IEEE Transactions on Neural Networks;citation_volume=13;citation_number=13;citation_first_page=415;citation_last_page=425;citation_publication_date=2002;"> 
  <meta name="citation_reference" content="Gr?ar M. Mining text-enriched heterogeneous information networks. PhD thesis, Jožef Stefan International Postgraduate School, Ljubljana, Slovenia; 2015."> 
  <meta name="citation_reference" content="Smailovi? J. Sentiment analysis in streams of microblogging posts. PhD thesis, Jožef Stefan International Postgraduate School, Ljubljana, Slovenia; 2014."> 
  <meta name="citation_reference" content="Smailovi? J, Kranjc J, Gr?ar M, Žnidarši? M, Mozeti? I. Monitoring the Twitter sentiment during the Bulgarian elections. In: Proc. IEEE Intl. Conf. on Data Science and Advanced Analytics. IEEE; 2015. p. 1–10."> 
  <meta name="citation_reference" content="citation_title=On the Use of Cross-validation for Time Series Predictor Evaluation;citation_author=C Bergmeir;citation_author=JM Benítez;citation_journal_title=Information Sciences;citation_volume=191;citation_number=191;citation_first_page=192;citation_last_page=213;citation_publication_date=2012;"> 
  <meta name="citation_reference" content="Martineau J, Finin T. Delta TFIDF: An improved feature space for sentiment analysis. In: Proc. 3rd AAAI Intl. Conf. on Weblogs and Social Media (ICWSM); 2009. p. 258–261."> 
  <meta name="citation_reference" content="citation_title=Statistical comparison of classifiers over multiple data sets;citation_author=J Demšar;citation_journal_title=Journal of Machine Learning Research;citation_volume=7;citation_number=7;citation_first_page=1;citation_last_page=30;citation_publication_date=2006;"> 
  <!-- DoubleClick overall ad setup script --> 
  <script type="text/javascript">
  var googletag = googletag || {};
  googletag.cmd = googletag.cmd || [];
  (function() {
    var gads = document.createElement('script');
    gads.async = true;
    gads.type = 'text/javascript';
    var useSSL = 'https:' == document.location.protocol;
    gads.src = (useSSL ? 'https:' : 'http:') +
        '//www.googletagservices.com/tag/js/gpt.js';
    var node = document.getElementsByTagName('script')[0];
    node.parentNode.insertBefore(gads, node);
  })();
</script> 
  <!-- DoubleClick ad slot setup script --> 
  <script id="doubleClickSetupScript" type="text/javascript">
    googletag.cmd.push(function() {
  googletag.defineSlot('/75507958/PONE_728x90_ATF', [728, 90], 'div-gpt-ad-1458247671871-0').addService(googletag.pubads());
  googletag.defineSlot('/75507958/PONE_160x600_BTF', [160, 600], 'div-gpt-ad-1458247671871-1').addService(googletag.pubads());
      googletag.pubads().enableSingleRequest();
      googletag.enableServices();
    });
  </script> 
  <script type="text/javascript">
    var WombatConfig = WombatConfig || {};
    WombatConfig.resourcePath = "/plosone/resource/";
    WombatConfig.imgPath = "/plosone/resource/img/";
    WombatConfig.journalKey = "PLoSONE";
    WombatConfig.figurePath = "/plosone/article/figure/image";
    WombatConfig.figShareInstitutionString = "plos";
    WombatConfig.doiResolverPrefix = "http://dx.plos.org/";
</script> 
  <script type="text/javascript">
  var WombatConfig = WombatConfig || {};
  WombatConfig.metrics = WombatConfig.metrics || {};
  WombatConfig.metrics.referenceUrl      = "http://lagotto.io/plos";
  WombatConfig.metrics.googleScholarUrl  = "http://scholar.google.com/scholar";
  WombatConfig.metrics.googleScholarCitationUrl  = WombatConfig.metrics.googleScholarUrl + "?hl=en&lr=&cites=";
  WombatConfig.metrics.crossrefUrl  = "http://www.crossref.org";
</script>
  <script src="//code.jquery.com/jquery-2.1.4.min.js"></script> 
  <script>window.jQuery || document.write('<script src="/plosone/resource/js/vendor/jquery-2.1.4.min.js""><\/script>')</script> 
  <script type="text/javascript" src="https://widgets.figshare.com/static/figshare.js"></script> 
  <!--For Google Tag manager to be able to track site information  --> 
  <script>

  dataLayer = [{
    'mobileSite': 'false',
    'desktopSite': 'true'
  }];

</script> 
 </head> 
 <body class="article plosone"> 
  <!-- Google Tag Manager --> 
  <noscript>
   <iframe src="//www.googletagmanager.com/ns.html?id=GTM-TP26BH" height="0" width="0" style="display:none;visibility:hidden"></iframe>
  </noscript> 
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TP26BH');</script> 
  <noscript>
   <iframe src="//www.googletagmanager.com/ns.html?id=GTM-MQQMGF" height="0" width="0" style="display:none;visibility:hidden"></iframe>
  </noscript> 
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MQQMGF');</script> 
  <!-- End Google Tag Manager --> 
  <header> 
   <div id="topslot" class="head-top"> 
    <div class="center"> 
     <div class="title">
      Advertisement
     </div> 
     <!-- DoubleClick Ad Zone --> 
     <div class="advertisement" id="div-gpt-ad-1458247671871-0" style="width:728px; height:90px;"> 
      <script type="text/javascript">
      googletag.cmd.push(function() { googletag.display('div-gpt-ad-1458247671871-0'); });
    </script> 
     </div> 
    </div> 
   </div> 
   <div id="user" class="nav"> 
    <ul class="nav-user"> 
     <li><a href="http://www.plos.org">plos.org</a></li> 
     <li><a href="https://community.plos.org/registration/new">create account</a></li> 
     <li class="highlighted"><a href="/plosone/user/secure/login?page=%2Fplosone%2Farticle%3Fid%3D10.1371%2Fjournal.pone.0155036">sign in</a></li> 
    </ul> 
   </div> 
   <div id="pagehdr"> 
    <nav class="nav-main"> 
     <h1 class="logo"> <a href="/plosone/.">PLOS ONE</a> </h1> 
     <section class="top-bar-section"> 
      <ul class="nav-elements"> 
       <li class="multi-col-parent menu-section-header has-dropdown" id="publish"> Publish 
        <div class="dropdown mega "> 
         <ul class="multi-col" id="publish-dropdown-list"> 
          <li class="menu-section-header " id="submissions"> <span class="menu-section-header-title"> Submissions </span> 
           <ul class="menu-section " id="submissions-dropdown-list"> 
            <li> <a href="/plosone/s/getting-started">Getting Started</a> </li> 
            <li> <a href="/plosone/s/submission-guidelines">Submission Guidelines</a> </li> 
            <li> <a href="/plosone/s/figures">Figures</a> </li> 
            <li> <a href="/plosone/s/tables">Tables</a> </li> 
            <li> <a href="/plosone/s/supporting-information">Supporting Information</a> </li> 
            <li> <a href="/plosone/s/latex">LaTeX</a> </li> 
            <li> <a href="/plosone/s/revising-your-manuscript">Revising Your Manuscript</a> </li> 
            <li> <a href="/plosone/s/submit-now">Submit Now</a> </li> 
           </ul> </li> 
          <li class="menu-section-header " id="policies"> <span class="menu-section-header-title"> Policies </span> 
           <ul class="menu-section " id="policies-dropdown-list"> 
            <li> <a href="/plosone/s/best-practices-in-research-reporting">Best Practices in Research Reporting</a> </li> 
            <li> <a href="/plosone/s/human-subjects-research">Human Subjects Research</a> </li> 
            <li> <a href="/plosone/s/animal-research">Animal Research</a> </li> 
            <li> <a href="/plosone/s/competing-interests">Competing Interests</a> </li> 
            <li> <a href="/plosone/s/disclosure-of-funding-sources">Disclosure of Funding Sources</a> </li> 
            <li> <a href="/plosone/s/licenses-and-copyright">Licenses and Copyright</a> </li> 
            <li> <a href="/plosone/s/data-availability">Data Availability</a> </li> 
            <li> <a href="/plosone/s/materials-and-software-sharing">Materials and Software Sharing</a> </li> 
            <li> <a href="/plosone/s/ethical-publishing-practice">Ethical Publishing Practice</a> </li> 
            <li> <a href="/plosone/s/authorship">Authorship</a> </li> 
            <li> <a href="/plosone/s/downloads-and-translations">Downloads and Translations</a> </li> 
           </ul> </li> 
          <li class="menu-section-header " id="manuscript-review-and-publication"> <span class="menu-section-header-title"> Manuscript Review and Publication </span> 
           <ul class="menu-section " id="manuscript-review-and-publication-dropdown-list"> 
            <li> <a href="/plosone/s/criteria-for-publication">Criteria for Publication</a> </li> 
            <li> <a href="/plosone/s/editorial-and-peer-review-process">Editorial and Peer Review Process</a> </li> 
            <li> <a href="/plosone/s/reviewer-guidelines">Reviewer Guidelines</a> </li> 
            <li> <a href="/plosone/s/accepted-manuscripts">Accepted Manuscripts</a> </li> 
            <li> <a href="/plosone/s/corrections-and-retractions">Corrections and Retractions</a> </li> 
            <li> <a href="/plosone/s/comments">Comments</a> </li> 
            <li> <a href="/plosone/s/article-level-metrics">Article-Level Metrics</a> </li> 
           </ul> </li> 
         </ul> 
         <div class="calloutcontainer"> 
          <h3 class="callout-headline">Submit Your Manuscript</h3> 
          <div class="action-contain"> 
           <p class="callout-content"> Discover a faster, simpler path to publishing in a high-quality journal. <em>PLOS ONE</em> promises fair, rigorous peer review, broad scope, and wide readership – a perfect fit for your research every time. </p> 
           <p class="button-contain special"> <a class="button button-default" href="/plosone/static/publish"> Learn More </a> <a class="button-link" href="http://www.editorialmanager.com/pone/default.asp"> Submit Now </a> </p> 
          </div> 
          <!-- opens in siteMenuCalloutDescription --> 
         </div> 
        </div> </li> 
       <li class="menu-section-header has-dropdown " id="about"> <span class="menu-section-header-title"> About </span> 
        <ul class="menu-section dropdown " id="about-dropdown-list"> 
         <li> <a href="/plosone/static/publish">Why Publish with PLOS ONE</a> </li> 
         <li> <a href="/plosone/s/journal-information">Journal Information</a> </li> 
         <li> <a href="/plosone/s/staff-editors">Staff Editors</a> </li> 
         <li> <a href="/plosone/static/editorial-board">Editorial Board</a> </li> 
         <li> <a href="/plosone/s/section-editors">Section Editors</a> </li> 
         <li> <a href="/plosone/s/advisory-groups">Advisory Groups</a> </li> 
         <li> <a href="/plosone/s/publishing-information">Publishing Information</a> </li> 
         <li> <a href="/plosone/s/publication-fees">Publication Fees</a> </li> 
         <li> <a href="/plosone/s/press-and-media">Press and Media</a> </li> 
         <li> <a href="/plosone/s/contact">Contact</a> </li> 
        </ul> </li> 
       <li data-js-tooltip-hover="trigger" class="subject-area menu-section-header"> Browse </li> 
       <li id="navsearch" class="head-search"> 
        <form name="searchForm" action="/plosone/search" method="get"> 
         <fieldset> 
          <legend>Search</legend> 
          <label for="search">Search</label> 
          <input id="search" type="text" name="q" placeholder="Search" required> 
          <button id="headerSearchButton" type="submit"><span class="search-icon"></span></button> 
         </fieldset> 
         <input type="hidden" name="filterJournals" value="PLoSONE"> 
        </form> <a id="advSearch" href="/plosone/search"> advanced search </a> </li> 
      </ul> 
     </section> 
    </nav> 
   </div> 
  </header>
  <section id="taxonomyContainer"> 
   <div id="taxonomy-browser" class="areas" data-search-url="/plosone/browse"> 
    <div class="wrapper"> 
     <div class="taxonomy-header">
       Browse Subject Areas 
      <div id="subjInfo">
       ?
      </div> 
      <div id="subjInfoText"> 
       <p>Click through the PLOS taxonomy to find articles in your field.</p> 
       <p>For more information about PLOS Subject Areas, click <a href="/plosone/s/help-using-this-site#loc-subject-areas">here</a>. </p> 
      </div> 
     </div> 
     <div class="levels"> 
      <div class="levels-container cf"> 
       <div class="levels-position"></div> 
      </div> 
      <a href="#" class="prev"></a> 
      <a href="#" class="next active"></a> 
     </div> 
    </div> 
    <div class="taxonomy-browser-border-bottom"></div> 
   </div>
  </section> 
  <main> 
   <div class="set-grid"> 
    <header class="title-block"> 
     <ul id="almSignposts" class="signposts"> 
      <li id="loadingMetrics"> <p>Loading metrics</p> </li> 
     </ul> 
     <script type="text/template" id="signpostsGeneralErrorTemplate">
  <li id="metricsError">Article metrics are unavailable at this time. Please try again later.</li>
</script> 
     <script type="text/template" id="signpostsNewArticleErrorTemplate">
  <li></li><li></li><li id="tooSoon">Article metrics are unavailable for recently published articles.</li>
</script> 
     <script type="text/template" id="signpostsTemplate">
    <li id="almSaves">
      <%= s.numberFormat(saveCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#savedHeader">Save</a>
        <p class="saves-tip" data-js-tooltip-hover="target"><a href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#savedHeader">Total Mendeley and CiteULike bookmarks.</a></p>
      </div>
    </li>

    <li id="almCitations">
      <%= s.numberFormat(citationCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#citedHeader">Citation</a>
        <p class="citations-tip" data-js-tooltip-hover="target"><a href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#citedHeader">Paper's citation count computed by Scopus.</a></p>
      </div>
    </li>

    <li id="almViews">
      <%= s.numberFormat(viewCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#viewedHeader">View</a>
        <p class="views-tip" data-js-tooltip-hover="target"><a href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#viewedHeader">Sum of PLOS and PubMed Central page views and downloads.</a></p>
      </div>
    </li>

    <li id="almShares">
      <%= s.numberFormat(shareCount, 0) %>
      <div class="tools" data-js-tooltip-hover="trigger">
        <a class="metric-term" href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#discussedHeader">Share</a>
        <p class="shares-tip" data-js-tooltip-hover="target"><a href="/plosone/article/metrics?id=10.1371/journal.pone.0155036#discussedHeader">Sum of Facebook and Twitter activity.</a></p>
      </div>
    </li>
</script> 
     <div class="article-meta"> 
      <div class="classifications"> 
       <p class="license-short" id="licenseShort">Open Access</p> 
       <p class="peer-reviewed" id="peerReviewed">Peer-reviewed</p> 
       <div class="article-type"> 
        <p class="type-article" id="artType">Research Article</p> 
       </div> 
      </div> 
     </div> 
     <div class="article-title-etc"> 
      <div class="title-authors"> 
       <h1 id="artTitle">
        <!--?xml version="1.0" encoding="UTF-8"?-->Multilingual Twitter Sentiment Classification: The Role of Human Annotators</h1> 
       <ul class="author-list clearfix" data-js-tooltip="tooltip_container" id="author-list"> 
        <li data-js-tooltip="tooltip_trigger"> <a data-author-id="0" class="author-name"> Igor Mozeti? <span class="email"> </span>,</a> 
         <div id="author-meta-0" class="author-info" data-js-tooltip="tooltip_target"> 
          <p id="authCorresponding-0"> <span class="email">* E-mail:</span> <a href="mailto:Igor.Mozetic@ijs.si">Igor.Mozetic@ijs.si</a> (IM); <a href="mailto:Jasmina.Smailovic@ijs.si">Jasmina.Smailovic@ijs.si</a> (JS)</p> 
          <p id="authAffiliations-0"><span class="type">Affiliation</span> Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia </p> 
          <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose0"> ? </a> 
         </div> </li> 
        <li data-js-tooltip="tooltip_trigger"> <a data-author-id="1" class="author-name"> Miha Gr?ar,</a> 
         <div id="author-meta-1" class="author-info" data-js-tooltip="tooltip_target"> 
          <p id="authAffiliations-1"><span class="type">Affiliation</span> Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia </p> 
          <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose1"> ? </a> 
         </div> </li> 
        <li data-js-tooltip="tooltip_trigger"> <a data-author-id="2" class="author-name"> Jasmina Smailovi? <span class="email"> </span></a> 
         <div id="author-meta-2" class="author-info" data-js-tooltip="tooltip_target"> 
          <p id="authCorresponding-2"> <span class="email">* E-mail:</span> <a href="mailto:Igor.Mozetic@ijs.si">Igor.Mozetic@ijs.si</a> (IM); <a href="mailto:Jasmina.Smailovic@ijs.si">Jasmina.Smailovic@ijs.si</a> (JS)</p> 
          <p id="authAffiliations-2"><span class="type">Affiliation</span> Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia </p> 
          <a data-js-tooltip="tooltip_close" class="close" id="tooltipClose2"> ? </a> 
         </div> </li> 
       </ul> 
      </div> 
      <div id="floatTitleTop" data-js-floater="title_author" class="float-title"> 
       <div class="set-grid"> 
        <div class="float-title-inner"> 
         <h1>
          <!--?xml version="1.0" encoding="UTF-8"?-->Multilingual Twitter Sentiment Classification: The Role of Human Annotators</h1> 
         <ul id="floatAuthorList" data-js-floater="floated_authors"> 
          <li data-float-index="1">Igor Mozeti?,&nbsp; </li> 
          <li data-float-index="2">Miha Gr?ar,&nbsp; </li> 
          <li data-float-index="3">Jasmina Smailovi? </li> 
         </ul> 
        </div> 
        <div class="logo-close" id="titleTopCloser"> 
         <img src="/plosone/resource/img/logo.plos.95.png" alt="PLOS"> 
         <div class="close-floater" title="close">
          x
         </div> 
        </div> 
       </div> 
      </div> 
      <ul class="date-doi"> 
       <li id="artPubDate">Published: May 5, 2016</li> 
       <li id="artDoi"> <a href="https://doi.org/10.1371/journal.pone.0155036">https://doi.org/10.1371/journal.pone.0155036</a> </li> 
      </ul> 
     </div> 
     <div> 
     </div> 
    </header> 
    <section class="article-body"> 
     <ul class="article-tabs"> 
      <li class="tab-title active" id="tabArticle"> <a href="/plosone/article?id=10.1371/journal.pone.0155036" class="article-tab-1">Article</a> </li> 
      <li class="tab-title " id="tabAuthors"> <a href="/plosone/article/authors?id=10.1371/journal.pone.0155036" class="article-tab-2">Authors</a> </li> 
      <li class="tab-title " id="tabMetrics"> <a href="/plosone/article/metrics?id=10.1371/journal.pone.0155036" class="article-tab-3">Metrics</a> </li> 
      <li class="tab-title " id="tabComments"> <a href="/plosone/article/comments?id=10.1371/journal.pone.0155036" class="article-tab-4">Comments</a> </li> 
      <li class="tab-title " id="tabRelated"> <a href="/plosone/article/related?id=10.1371/journal.pone.0155036" class="article-tab-5">Related Content</a> </li> 
     </ul> 
     <div class="article-container"> 
      <div id="nav-article"> 
       <ul class="nav-secondary"> 
        <li class="nav-comments" id="nav-comments"> <a href="article/comments?id=10.1371/journal.pone.0155036">Reader Comments (0)</a> </li> 
        <li class="nav-media" id="nav-media" data-doi="10.1371/journal.pone.0155036"> <a href="/plosone/article/related?id=10.1371/journal.pone.0155036"> Media Coverage <span id="media-coverage-count"></span> </a> </li> 
        <li id="nav-figures"><a href="#" data-doi="10.1371/journal.pone.0155036">Figures</a></li> 
       </ul> 
      </div> 
      <div id="figure-lightbox-container"></div> 
      <script id="figure-lightbox-template" type="text/template">
  <div id="figure-lightbox" class="reveal-modal full" data-reveal aria-hidden="true"
       role="dialog">
    <div class="lb-header">
      <h1 id="lb-title"><%= articleTitle %></h1>

      <div id="lb-authors">
        <% authorList.split(',').forEach(function (author) { %>
        <span><%= author.trim() %></span>
        <% }) %>
      </div>

      <div class="lb-close" title="close">&nbsp;</div>
    </div>
    <div class="img-container">
      <div class="loader"> <i class="fa-spinner"></i> </div>
      <img class="main-lightbox-image" src=""/>
      <aside id="figures-list">
        <% figureList.each(function (ix, figure) { %>
        <div class="change-img" data-doi="<%= figure.getAttribute('data-doi') %>">
          <img class="aside-figure" src="/plosone/article/figure/image?size=inline&id=<%= figure.getAttribute('data-doi') %>" />
        </div>
        <% }) %>
        <div class="dummy-figure">
        </div>
      </aside>
    </div>
    <div id="lightbox-footer">

      <div id="btns-container" class="lightbox-row <% if(figureList.length <= 1) { print('one-figure-only') } %>">
        <div class="fig-btns-container reset-zoom-wrapper left">
          <span class="fig-btn reset-zoom-btn">Reset zoom</span>
        </div>
        <div class="zoom-slider-container">
          <div class="range-slider-container">
            <span id="lb-zoom-min"></span>
            <div class="range-slider round" data-slider data-options="start: 20; end: 200; initial: 20;">
              <span class="range-slider-handle" role="slider" tabindex="0"></span>
              <span class="range-slider-active-segment"></span>
              <input type="hidden">
            </div>
            <span id="lb-zoom-max"></span>
          </div>
        </div>
        <% if(figureList.length > 1) { %>
        <div class="fig-btns-container">
          <span class="fig-btn all-fig-btn"><i class="icon icon-all"></i> All Figures</span>
          <span class="fig-btn next-fig-btn"><i class="icon icon-next"></i> Next</span>
          <span class="fig-btn prev-fig-btn"><i class="icon icon-prev"></i> Previous</span>
        </div>
        <% } %>
      </div>
      <div id="image-context">
      </div>
    </div>
  </div>
</script> 
      <script id="image-context-template" type="text/template">
  <div class="footer-text">
    <div id="figure-description-wrapper">
      <div id="view-more-wrapper" style="<% descriptionExpanded? print('display:none;') : '' %>">
        <span id="figure-title"><%= title %></span>
        <p id="figure-description">
          <%= description %>&nbsp;&nbsp;
        </p>
        <span id="view-more">show more<i class="icon-arrow-right"></i></span>

      </div>
      <div id="view-less-wrapper" style="<% descriptionExpanded? print('display:inline-block;') : '' %>" >
        <span id="figure-title"><%= title %></span>
        <p id="full-figure-description">
          <%= description %>&nbsp;&nbsp;
          <span id="view-less">show less<i class="icon-arrow-left"></i></span>
        </p>
      </div>
    </div>
  </div>
  <div id="show-context-container">
    <a class="btn show-context" href="<%= showInContext(strippedDoi) %>">Show in Context</a>
  </div>
  <div id="download-buttons">
    <h3>Download:</h3>
    <div class="item">
      <a href="/plosone/article/figure/image?size=original&download=&id=<%= doi %>" title="original image">
        <span class="download-btn">TIFF</span>
      </a>
      <span class="file-size"><%= fileSizes.original %></span>
    </div>
    <div class="item">
      <a href="/plosone/article/figure/image?size=large&download=&id=<%= doi %>" title="large image">
        <span class="download-btn">PNG</span>
      </a>
      <span class="file-size"><%= fileSizes.large %></span>
    </div>
    <div class="item">
      <a href="/plosone/article/figure/powerpoint?id=<%= doi %>" title="PowerPoint slide">
        <span class="download-btn">PPT</span>
      </a>
    </div>

  </div>
</script> 
      <div class="article-content"> 
       <div id="figure-carousel-section"> 
        <h2>Figures</h2> 
        <div id="figure-carousel"> 
         <div class="carousel-wrapper"> 
          <div class="slider"> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g001"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g001" alt="Fig 1"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g002"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g002" alt="Fig 2"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g003"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g003" alt="Fig 3"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g004"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g004" alt="Fig 4"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g005"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g005" alt="Fig 5"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g006"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g006" alt="Fig 6"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g007"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g007" alt="Fig 7"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g008"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g008" alt="Fig 8"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g009"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g009" alt="Fig 9"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g010"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g010" alt="Fig 10"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.t001"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.t001" alt="Table 1"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.t002"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.t002" alt="Table 2"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.t003"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.t003" alt="Table 3"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.t004"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.t004" alt="Table 4"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.t005"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.t005" alt="Table 5"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g011"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g011" alt="Fig 11"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.t006"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.t006" alt="Table 6"> 
           </div> 
           <div class="carousel-item lightbox-figure" data-doi="10.1371/journal.pone.0155036.g012"> 
            <img src="/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0155036.g012" alt="Fig 12"> 
           </div> 
          </div> 
         </div> 
         <div class="carousel-control"> 
          <span class="button previous"></span> 
          <span class="button next"></span> 
         </div> 
         <div class="carousel-page-buttons"> 
         </div> 
        </div> 
       </div> 
       <div class="article-text" id="artText"> 
        <div class="abstract toc-section">
         <a id="abstract0" name="abstract0" data-toc="abstract0" class="link-target" title="Abstract"></a>
         <h2>Abstract</h2>
         <a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1" class="link-target"></a>
         <p>What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered.</p> 
        </div> 
        <div class="articleinfo">
         <p><strong>Citation: </strong>Mozeti? I, Gr?ar M, Smailovi? J (2016) Multilingual Twitter Sentiment Classification: The Role of Human Annotators. PLoS ONE 11(5): e0155036. https://doi.org/10.1371/journal.pone.0155036</p>
         <p><strong>Editor: </strong>Matjaz Perc, University of Maribor, SLOVENIA </p>
         <p><strong>Received: </strong>February 25, 2016; <strong>Accepted: </strong>April 22, 2016; <strong>Published: </strong> May 5, 2016</p>
         <p><strong>Copyright: </strong> © 2016 Mozeti? et al. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
         <p><strong>Data Availability: </strong>All data are available as 15 language files, in csv format, in a public language resource repository CLARIN.SI at <a href="http://hdl.handle.net/11356/1054">http://hdl.handle.net/11356/1054</a>.</p>
         <p><strong>Funding: </strong>This work was supported in part by the European Union projects SIMPOL (no. 610704), MULTIPLEX (no. 317532) and DOLFINS (no. 640772), and by the Slovenian ARRS programme Knowledge Technologies (no. P2-103).</p>
         <p><strong>Competing interests: </strong> Most of the tweets, except English, were collected during a joint project with Gama System (<a href="http://www.gama-system.si">http://www.gama-system.si</a>), using their PerceptionAnalytics platform (<a href="http://www.perceptionanalytics.net">http://www.perceptionanalytics.net</a>). The Twitter sentiment annotations were supported by the Goldfinch platform, provided by Sowa Labs (<a href="http://www.sowalabs.com">http://www.sowalabs.com</a>) for free. There are no patents, products in development or marketed products to declare. This does not alter the authors’ adherence to all the PLOS ONE policies on sharing data and materials.</p>
        </div> 
        <div id="section1" class="section toc-section">
         <a id="sec001" name="sec001" data-toc="sec001" class="link-target" title="Introduction"></a>
         <h2>Introduction</h2>
         <a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1" class="link-target"></a>
         <p>Sentiment analysis is a form of shallow semantic analysis of texts. Its goal is to extract opinions, emotions or attitudes towards different objects of interest [<a href="#pone.0155036.ref001" class="ref-tip">1</a>, <a href="#pone.0155036.ref002" class="ref-tip">2</a>]. For example, one might be interested in consumers opinion about products, voters attitude towards political parties, or investors expectations about stocks. From the first approaches in 2000s, sentiment analysis gained considerable attention with massive growth of the web and social media. Different forms of textual information are becoming easily accessible (e.g., news, blogs, reviews, Facebook comments, Twitter posts, etc.), and different approaches to sentiment analysis were developed.</p> 
         <a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2" class="link-target"></a>
         <p>There are two prevailing approaches to large-scale sentiment analysis: (i) lexicon-based and (ii) machine learning. In the first case, the sentiment in the text is computed from the set of sentiment-bearing words identified in the text. In the second case, a sentiment classification model is constructed first, from a large set of sentiment labeled texts, and then applied to the stream of unlabelled texts. The model has the form of a function that maps features extracted from the text into sentiment labels (which typically have discrete values: negative, neutral, or positive). In both approaches, one needs a considerable involvement of humans, at least initially. Humans have to label their perception of the sentiment expressed either in individual words or in short texts. This sentiment labeling is language-, domain- and often even topic-specific.</p> 
         <a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3" class="link-target"></a>
         <p>An example of a lexicon-based approach that involves a massive human sentiment labeling of words is described by Dodds et al. [<a href="#pone.0155036.ref003" class="ref-tip">3</a>]. They collected around 5 million human sentiment assessments of 10,000 common words, each in 10 languages and labeled 50 times. Another well-known sentiment lexicon is SentiWordNet [<a href="#pone.0155036.ref004" class="ref-tip">4</a>], constructed semi-automatically for over 100,000 words, but limited to English only.</p> 
         <a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4" class="link-target"></a>
         <p>In this paper we analyze a set of over 1.6 million Twitter posts, in 13 European languages, labeled for sentiment by human annotators. The labeled tweets are used as training data to train sentiment classifiers for different languages. An overview of the state-of-the-art of Twitter sentiment analysis is given in [<a href="#pone.0155036.ref005" class="ref-tip">5</a>]. A more recent overview of the lexicon-based and machine learning methods, and their combination, is in [<a href="#pone.0155036.ref006" class="ref-tip">6</a>].</p> 
         <a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5" class="link-target"></a>
         <p>We focus on the quantity and quality of the labeled tweets, and their impact on the performance of sentiment classifiers. The quality of the labeled tweets is estimated from the agreement between human annotators. The main hypothesis of the paper is that the annotators agreement provides an upper bound for the classifier performance.</p> 
         <a id="article1.body1.sec1.p6" name="article1.body1.sec1.p6" class="link-target"></a>
         <p>There are several more specific research questions we address:</p> 
         <ol class="order"> 
          <li>Are the sentiment classes ordered?</li> 
          <li>Which evaluation measures are appropriate to quantify and compare the labeled data quality and classifiers performance?</li> 
          <li>How to estimate the quality of the training data?</li> 
          <li>How to compare and select appropriate classifiers?</li> 
          <li>What are acceptable levels of the annotators agreement?</li> 
          <li>How many labeled Twitter posts are needed for training a sentiment classifier?</li> 
         </ol>
         <a id="article1.body1.sec1.p7" name="article1.body1.sec1.p7" class="link-target"></a>
         <p>In the paper we present three lines of experiments and results. One is related to manual annotation of Twitter posts and estimations of their quality and dataset properties. Another is about training sentiment classifiers, their performance and comparisons. The third line compares the labeled data quality with the classifier performance and provides support for our main hypothesis.</p> 
         <a id="article1.body1.sec1.p8" name="article1.body1.sec1.p8" class="link-target"></a>
         <p>The paper is organized as follows. In the Results and Discussion section we provide the main results on the comparison of the annotators agreement and classifiers performance. We briefly outline the main evaluation measure used and the datasets analyzed. The evaluation procedures and methods are just sketched, to facilitate the discussion of the results—all the details are in the Methods section. The main emphasis is on an in-depth analysis of the datasets. We consider their evolution through time, as new tweets get annotated, and how the performance of the classifiers varies with time. We also discuss the effects of different distributions of the training and application datasets.</p> 
         <a id="article1.body1.sec1.p9" name="article1.body1.sec1.p9" class="link-target"></a>
         <p>Conclusions provide answers to the research questions addressed, and give short- and long-term directions of future research.</p> 
         <a id="article1.body1.sec1.p10" name="article1.body1.sec1.p10" class="link-target"></a>
         <p>The Methods section provides all the details about the first two lines of experiments and results, specifically about the data, annotations, and sentiment classifiers. We define four evaluation measures, common in the fields of inter-rater agreement and machine learning. The measures are used to compute the self- and inter-annotator agreements for all the datasets. From these results we derive evidence that human annotators perceive the sentiment classes as ordered. We present the related work on methods used for the Twitter sentiment classification, and publicly available labeled datasets. We compare the performance of six selected classifiers by applying a standard statistical test. We give the necessary details of the evaluation procedure and the standard Twitter pre-processing steps.</p> 
         <a id="article1.body1.sec1.p11" name="article1.body1.sec1.p11" class="link-target"></a>
         <p>In the following subsection we give an overview of the related work on automated sentiment classification of Twitter posts. We summarize the published labeled sets used for training the classification models, and the machine learning methods applied for training. Most of the related work is limited to English texts only.</p> 
         <a id="article1.body1.sec1.p12" name="article1.body1.sec1.p12" class="link-target"></a>
         <p><strong>Contributions.</strong> We provide a large corpus of sentiment labeled tweets, in different languages and of varying quality. The collected set of over 1.6 million manually labeled tweets is by far the largest dataset reported in the literature, and we make it publicly available. We expect the corpus to be a fruitful and realistic test-bed for various classification algorithms. We apply four evaluation measures and show that two of them are more appropriate to evaluate sentiment classifiers. Additionally, the same measures are used to evaluate the quality of training data, thus providing the means to monitor the annotation process. We do not address various options in Twitter pre-processing and feature selection. Instead, we use the same standard parameter settings to get an unbiased comparison of six sentiment classifiers.</p> 
        </div> 
        <div id="section2" class="section toc-section">
         <a id="sec002" name="sec002" data-toc="sec002" class="link-target" title="Results and Discussion"></a>
         <h2>Results and Discussion</h2>
         <a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1" class="link-target"></a>
         <p>In this paper we analyze a large set of sentiment labeled tweets. We assume a sentiment label takes one of three possible values: <em>negative</em>, <em>neutral</em>, or <em>positive</em>. The analysis sheds light on two aspects of the data: the quality of human labeling of the tweets, and the performance of the sentiment classification models constructed from the same data. The main idea behind this analysis is to use the same evaluation measures to estimate both, the quality of human annotations and the quality of classification models. We argue that the performance of a classification model is primarily limited by the quality of the labeled data. This, in turn, can be estimated by the agreement between the human annotators.</p> 
         <a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2" class="link-target"></a>
         <p><strong>Evaluation measures.</strong> The researchers in the fields of inter-rater agreement and machine learning typically employ different evaluation measures. We report all the results in terms of four selected measures which we deem appropriate for the three-valued sentiment classification task (the details are in the Evaluation measures subsection in Methods). In this section, however, the results are summarized only in terms of Krippendorff’s Alpha-reliability (<em>Alpha</em>) [<a href="#pone.0155036.ref007" class="ref-tip">7</a>], to highlight the main conclusions. <em>Alpha</em> is a generalization of several specialized agreement measures. When annotators agree perfectly or when a model perfectly classifies the data, <em>Alpha</em> = 1. When the level of agreement equals the agreement by chance, <em>Alpha</em> = 0. There are several instances of <em>Alpha</em>. All the results are reported here are in terms of <em>Alpha</em><sub><em>int</em></sub> (interval) which takes into account the ordering of sentiment values and assigns higher penalty to more extreme disagreements. The justification for this choice is in the subsection on Ordering of sentiment values in Methods.</p> 
         <a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3" class="link-target"></a>
         <p><strong>Datasets.</strong> We analyze two corpora of data. The first consists of 13 language datasets, with over 1.6 million annotated tweets, by far the largest sentiment corpus made publicly available. The languages covered are: <strong>Albanian, Bulgarian, English, German, Hungarian, Polish, Portuguese, Russian, Ser/Cro/Bos</strong> (a joint set of Serbian, Croatian, and Bosnian tweets, the languages difficult to distinguish on Twitter), <strong>Slovak, Slovenian, Spanish</strong>, and <strong>Swedish</strong>.</p> 
         <a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4" class="link-target"></a>
         <p>The second corpus of data comes from four applications of sentiment classification, which we already published. These tweets are domain-specific and provide novel insights and lessons learned when analyzed with the same methods as the language datasets. The application datasets are: <strong>Facebook(it)</strong>—the Facebook comments on conspiracy theories in Italian, to study the emotional dynamics [<a href="#pone.0155036.ref008" class="ref-tip">8</a>], <strong>DJIA30</strong>—tweets about the Dow Jones stocks, to analyze the effects of Twitter sentiment on their price movements [<a href="#pone.0155036.ref009" class="ref-tip">9</a>], <strong>Environment</strong>—tweets about environmental issues, to compare the sentiment leaning of different communities [<a href="#pone.0155036.ref010" class="ref-tip">10</a>], and <strong>Emojis</strong>—a subset of the tweets from the above 13 language datasets which contain emojis, used to derive the emoji sentiment lexicon [<a href="#pone.0155036.ref011" class="ref-tip">11</a>]. The details about the datasets in terms of their size, sentiment distribution, and the time of the posts are in the Datasets subsection in Methods.</p> 
         <div id="section1" class="section toc-section">
          <a id="sec003" name="sec003" class="link-target" title="The limits of performance"></a> 
          <h3>The limits of performance</h3> 
          <a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1" class="link-target"></a>
          <p>Determining sentiment expressed in a tweet is not an easy task, and depends on subjective judgment of human annotators. Annotators often disagree between themselves, and even an individual is not always consistent with her/himself. There are several reasons for disagreements, such as: inherent difficulty of the task (e.g., estimating the “sentiment” about the future stock movement), different vocabularies used in different domains (e.g., financial markets vs. environmental issues), topic drift in time (e.g., events which abruptly shift the topic of discussions on Twitter), or simply a poor quality of the annotator’s work. In the data we analyze, we observe all the above issues, try to identify them by computational means, and draw lessons how the annotation process should be conducted in the future.</p> 
          <a id="article1.body1.sec2.sec1.p2" name="article1.body1.sec2.sec1.p2" class="link-target"></a>
          <p><strong>Annotator agreements.</strong> During the manual sentiment labeling of tweets, a fraction of tweets (about 15%) was intentionally duplicated to be annotated twice, either by the same annotator or by two different annotators (see details in the Datasets subsection in Methods). From multiple annotations of the same annotator we compute the <strong>self-agreement</strong>, and from multiple annotations by different annotators we compute the <strong>inter-annotator agreement</strong> (abbreviated as inter-agreement). The confidence intervals for the agreements are estimated by bootstrapping [<a href="#pone.0155036.ref012" class="ref-tip">12</a>]. The detailed results are in the Annotator agreements subsection in Methods. It turns out that the self-agreement is a good measure to identify low quality annotators, and that the inter-annotator agreement provides a good estimate of the objective difficulty of the task, unless it is too low.</p> 
          <a id="article1.body1.sec2.sec1.p3" name="article1.body1.sec2.sec1.p3" class="link-target"></a>
          <p><strong>Model evaluation.</strong> To manually label over 1.6 million tweets requires a considerable effort. The purpose of this effort is to use the labeled data to built sentiment classification models for each of the 13 languages. A classification model can then be applied to unlabeled data in various application scenarios, as was the case with our four application datasets.</p> 
          <a id="article1.body1.sec2.sec1.p4" name="article1.body1.sec2.sec1.p4" class="link-target"></a>
          <p>A classification model can be build by any suitable supervised machine learning method. To evaluate the model, a standard approach in machine learning is to use 10-fold cross-validation. The whole labeled set is partitioned into 10 folds, one is set apart for testing, and the remaining nine are used to train the model and evaluate it on the test fold. The process is repeated 10 times until each fold is used for testing exactly once. The reported evaluation results are the average of 10 tests, and the confidence intervals are estimated from standard deviations.</p> 
          <a id="article1.body1.sec2.sec1.p5" name="article1.body1.sec2.sec1.p5" class="link-target"></a>
          <p>We constructed and evaluated six different classification models for each labeled language dataset. The results for the application datasets are extracted from the original papers. Our classifiers are all based on Support Vector Machines (SVM) [<a href="#pone.0155036.ref013" class="ref-tip">13</a>], and for reference we also constructed a Naive Bayes classifier [<a href="#pone.0155036.ref014" class="ref-tip">14</a>]. Detailed results are in the Classification models performance subsection in Methods. When comparing the classifiers’ performance with the Friedman-Nemenyi test [<a href="#pone.0155036.ref015" class="ref-tip">15</a>, <a href="#pone.0155036.ref016" class="ref-tip">16</a>], it turns out that there is no statistically significant difference between most of them (see the Friedman-Nemenyi test subsection in Methods). For subsequent analyses and comparisons, we selected the TwoPlaneSVMbin classifier that is always in the group of top classifiers according to two most relevant evaluation measures.</p> 
          <a id="article1.body1.sec2.sec1.p6" name="article1.body1.sec2.sec1.p6" class="link-target"></a>
          <p><strong>Comparative analyses.</strong> The main results of this paper are summarized in <a href="#pone-0155036-g001">Fig 1</a>. It shows a comparison of the self-agreement, the inter-annotator agreement, and the TwoPlaneSVMbin classifier performance, for the 13 language datasets and the four application datasets.</p> 
          <a class="link-target" id="pone-0155036-g001" name="pone-0155036-g001"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g001">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g001" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g001"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g001" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g001"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g001"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g001"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g001"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g001"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g001"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 1. </span> Comparison of annotators self-agreement (green), the inter-annotator agreement (blue), and an automated sentiment classifier (TwoPlaneSVMbin, red) in terms of Krippendorff’s 
            <em>Alpha</em>.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec1.fig1.caption1.p1" name="article1.body1.sec2.sec1.fig1.caption1.p1" class="link-target"></a></p>
           <p>On the left-hand side are the 13 language datasets, and on the right-hand side the four application datasets. The datasets are ordered by decreasing self-agreement. The error bars indicate estimated 95% confidence intervals.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g001"> https://doi.org/10.1371/journal.pone.0155036.g001</a></p>
          </div>
          <a id="article1.body1.sec2.sec1.p7" name="article1.body1.sec2.sec1.p7" class="link-target"></a>
          <p>The <strong>self-agreement</strong> for most of the datasets is above 0.6. The exceptions, Albanian and Spanish, indicate low quality annotators which should be eliminated from further considerations. In the applications corpus, the Emojis dataset is the only one with the self-agreement lower than the inter-annotator agreement, due to a high number of low quality Spanish annotations included. The other three application datasets have relatively high self-agreement (0.7–0.9, due to more carefully selected annotators), and higher variability (due to a lower number of tweets annotated twice, 2–4% only).</p> 
          <a id="article1.body1.sec2.sec1.p8" name="article1.body1.sec2.sec1.p8" class="link-target"></a>
          <p>The <strong>inter-annotator agreement</strong> varies a lot, and is always lower than the self-agreement, except for Emojis. The high inter-annotator agreement for Facebook(it) is consistent with the high self-agreement. Values below 0.2 (Albanian and Spanish) indicate low quality annotators, consistent with the low self-agreement. Values in the range between 0.3–0.4 (Ser/Cro/Bos, Bulgarian, and German) indicate a problem with the annotation process, and are discussed in more detail in the next subsection.</p> 
          <a id="article1.body1.sec2.sec1.p9" name="article1.body1.sec2.sec1.p9" class="link-target"></a>
          <p>The <strong>classifier performance</strong> is typically in the range between 0.4–0.6. Notable exceptions are Albanian and Spanish, with the performance barely above random, but very close to the inter-annotator agreement. More interesting are the datasets with a relatively low performance, around 0.4, that cannot be explained by low quality annotations alone: Ser/Cro/Bos, Bulgarian, German, Portuguese, and Environment. They are analyzed in the next subsections.</p> 
          <a id="article1.body1.sec2.sec1.p10" name="article1.body1.sec2.sec1.p10" class="link-target"></a>
          <p>The main hypothesis of this paper is that the inter-annotator agreement approximates an upper bound for a classifier performance. In <a href="#pone-0155036-g001">Fig 1</a> we observe three such cases where the classifier performance, in the range 0.4–0.6, approaches its limit: Polish, Slovenian, and DJIA30. There are also three cases where there still appears a gap between the classifier performance and the inter-annotator agreement: English, Facebook(it), and Environment. In order to confirm the hypothesis, we analyze the evolution of the classifiers performance through time and check if the performance is still improving or was the plateau already reached. This is not always possible: There are datasets where only one annotator was engaged and for which there is no inter-annotator agreement (Russian, Swedish, Hungarian, Slovak, and Portuguese). For them we can only draw analogies with the multiply annotated datasets and speculate about the conclusions.</p> 
          <a id="article1.body1.sec2.sec1.p11" name="article1.body1.sec2.sec1.p11" class="link-target"></a>
          <p>In the next two subsection we first analyze the language datasets, and then the four application datasets.</p> 
         </div> 
         <div id="section2" class="section toc-section">
          <a id="sec004" name="sec004" class="link-target" title="Language datasets analyses"></a> 
          <h3>Language datasets analyses</h3> 
          <a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1" class="link-target"></a>
          <p>To label the 1.6 million tweets in the 13 languages, 83 native speakers were engaged, and an estimated effort of 38 person-months was spent. Can one reduce the efforts and focus them on more problematic datasets instead? It seems, for example, that the annotation of over 200,000 Polish tweets was an overkill. Worse, the annotation of over 250,000 Spanish tweets was largely a waste of efforts, due to the poor annotation quality.</p> 
          <a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2" class="link-target"></a>
          <p>We perform a post-hoc analysis of the 13 language datasets by measuring the performance of the sentiment classifiers through time. We emulate the evolution of the performance by feeding increasingly large labeled sets into the classifier training process. The labeled sets are ordered by the post time of the tweets, so one can detect potential topic shifts during the Twitter discussions. At each stage, the labeled set is increased by 10,000 tweets, and the set accumulated so far is used for training and testing the classifier. After each stage, the evaluation by 10-fold cross-validation is performed and the results are reported in the following charts. The final stage, when all the labeled sets are exhausted, corresponds to the results reported in <a href="#pone-0155036-g001">Fig 1</a>. In subsequent figures, the x-axis denotes labeled sets increases by 10,000 tweets, the y-axis denotes the TwoPlaneSVMbin classifier performance measured by <em>Alpha</em>, and the error bars are the 95% confidence intervals estimated from 10-fold cross-validations. The inter-annotator agreement is represented by a blue line—it is constant and is computed from all the available data.</p> 
          <a id="article1.body1.sec2.sec2.p3" name="article1.body1.sec2.sec2.p3" class="link-target"></a>
          <p>We identify five cases, characterized by different relations between the classifier performance and the inter-annotator agreement: (i) a performance gap still exists, (ii) a performance limit is approached, (iii) low inter-annotator agreement, (iv) topic shift, and (v) very low annotation quality.</p> 
          <a id="article1.body1.sec2.sec2.p4" name="article1.body1.sec2.sec2.p4" class="link-target"></a>
          <p><strong>Performance gap still exists.</strong> <a href="#pone-0155036-g002">Fig 2</a> (chart on the left) shows the evolution of the <strong>English</strong> classifier performance, as it is fed increasingly large training sets. On top (in blue) is the inter-annotator agreement line (<em>Alpha</em> = 0.613). The classifier’s <em>Alpha</em> is increasing from the initial 0.422 to 0.516, but is still considerably below the inter-annotator agreement. Despite the relatively large training set (around 90,000 labeled tweets) there is still a performance gap and even more annotations are needed to approach the inter-annotator agreement.</p> 
          <a class="link-target" id="pone-0155036-g002" name="pone-0155036-g002"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g002">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g002" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g002"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g002" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g002"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g002"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g002"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g002"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g002"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g002"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 2. </span> The English (left) and Russian (right) datasets.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig1.caption1.p1" name="article1.body1.sec2.sec2.fig1.caption1.p1" class="link-target"></a></p>
           <p>For English, there is still a gap (<em>Alpha</em> = 0.097) between the classifier (in red) and the inter-annotator agreement (in blue).</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g002"> https://doi.org/10.1371/journal.pone.0155036.g002</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p5" name="article1.body1.sec2.sec2.p5" class="link-target"></a>
          <p>We observe a similar pattern with the <strong>Russian</strong> (<a href="#pone-0155036-g002">Fig 2</a>, chart on the right) and <strong>Slovak</strong> datasets (not shown). The inter-annotator agreement is unknown, but the classifier’s performance is still increasing from the initial <em>Alpha</em> of 0.403 to 0.490 for Russian, and from the initial 0.408 to 0.460 for Slovak. The size of the labeled sets for Russian is around 90,000, for Slovak around 60,000, and we argue that more training data is needed to further improve the performance.</p> 
          <a id="article1.body1.sec2.sec2.p6" name="article1.body1.sec2.sec2.p6" class="link-target"></a>
          <p><strong>Performance limit approached.</strong> A different pattern from the above can be observed in <a href="#pone-0155036-g003">Fig 3</a> for the <strong>Polish</strong> dataset. After a slow improvement of the classifier’s performance, the peak is reached at around 150,000 labeled tweets, and afterwards the performance remains stable and is even slightly decreasing. The maximum <em>Alpha</em> is 0.536, close to the inter-annotator agreement of 0.571. At the same point, at 150,000 tweets, another performance measure, <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e001" class="inline-graphic"></span>, also peaks at its maximum value, even above the corresponding inter-annotator agreement. These results suggest that beyond a certain point, when the classifier’s performance is “close enough” to the inter-annotator agreement, it does not pay off to further label tweets by sentiment. This is valid, however, only until a considerably new topic occurs.</p> 
          <a class="link-target" id="pone-0155036-g003" name="pone-0155036-g003"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g003">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g003" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g003"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g003" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g003"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g003"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g003"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g003"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g003"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g003"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 3. </span> The Polish dataset.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig2.caption1.p1" name="article1.body1.sec2.sec2.fig2.caption1.p1" class="link-target"></a></p>
           <p>The classifier’s peak performance (in red, <em>Alpha</em> = 0.536) is at 150,000 labeled tweets.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g003"> https://doi.org/10.1371/journal.pone.0155036.g003</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p7" name="article1.body1.sec2.sec2.p7" class="link-target"></a>
          <p>Similar conclusions can be drawn for the <strong>Slovenian</strong> dataset (<a href="#pone-0155036-g004">Fig 4</a>, chart on the left). The classifier’s performance reaches its peak earlier, at 70,000 tweets, with the maximum <em>Alpha</em> of 0.459, as well as the maximum <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e002" class="inline-graphic"></span>. <em>Alpha</em> is close to the inter-annotator agreement of 0.485, and <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e003" class="inline-graphic"></span> even exceeds the corresponding agreement. However, notice that the inter-annotator agreement for Slovenian is almost 10% points lower than for Polish.</p> 
          <a class="link-target" id="pone-0155036-g004" name="pone-0155036-g004"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g004">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g004" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g004"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g004" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g004"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g004"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g004"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g004"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g004"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g004"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 4. </span> The Slovenian (left) and Bulgarian (right) datasets.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig3.caption1.p1" name="article1.body1.sec2.sec2.fig3.caption1.p1" class="link-target"></a></p>
           <p>The Slovenian classifier peak is at 70,000 tweets (<em>Alpha</em> = 0.459). The Bulgarian classifier peak is at 40,000 tweets (<em>Alpha</em> = 0.378).</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g004"> https://doi.org/10.1371/journal.pone.0155036.g004</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p8" name="article1.body1.sec2.sec2.p8" class="link-target"></a>
          <p>We observe a similar pattern for the <strong>Bulgarian</strong> dataset (<a href="#pone-0155036-g004">Fig 4</a>, chart on the right). The classifier’s peak performance is reached even earlier, at 40,000 tweets (<em>Alpha</em> is 0.378), but the inter-annotator agreement is also considerably lower, more than 10% points below the Slovenian (<em>Alpha</em> is 0.367). In such cases, when the inter-annotator agreement is “too low” (our estimate is when <em>Alpha</em> &lt; 0.4), the inter-annotator agreement is a poor estimator of the difficulty of the task, and should not be used as a performance approximation. Instead, one could analyze the reasons for the disagreements, as we do with cases in the following paragraphs.</p> 
          <a id="article1.body1.sec2.sec2.p9" name="article1.body1.sec2.sec2.p9" class="link-target"></a>
          <p><strong>Low inter-annotator agreement.</strong> The inter-annotator agreement for the <strong>German</strong> dataset is low, <em>Alpha</em> is 0.344. The classifier’s performance is higher already with the initial small datasets, and soon starts dropping (<a href="#pone-0155036-g005">Fig 5</a>, chart on the left). It turns out that over 90% of the German tweets were labeled by two annotators only, dubbed annotator A and B. The annotation quality of the two annotators is very different, the self-agreement <em>Alpha</em> for the annotator A is 0.590, and for the annotator B is 0.760. We consider the German tweets labeled by A and B separately (<a href="#pone-0155036-g005">Fig 5</a>, charts in the middle and on the right). The lower quality A dataset reaches its maximum at 30,000 tweets, while the performance of the higher quality B dataset is still increasing. There was also a relatively high disagreement between the two annotators which resulted in a low classifier’s performance. A conclusions drawn from this dataset, as well as from the Bulgarian, is that one should constantly monitor the self- and inter-annotator agreements, and promptly notify the annotators as soon as the agreements drop too low.</p> 
          <a class="link-target" id="pone-0155036-g005" name="pone-0155036-g005"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g005">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g005" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g005"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g005" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g005"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g005"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g005"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g005"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g005"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g005"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 5. </span> The German datasets.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig4.caption1.p1" name="article1.body1.sec2.sec2.fig4.caption1.p1" class="link-target"></a></p>
           <p>The complete dataset (left), and separate datasets labeled by the two main annotators (middle and right).</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g005"> https://doi.org/10.1371/journal.pone.0155036.g005</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p10" name="article1.body1.sec2.sec2.p10" class="link-target"></a>
          <p> <a href="#pone-0155036-g006">Fig 6</a> gives the results on the joint <strong>Ser/Cro/Bos</strong> dataset. We observe a low inter-annotator agreement (<em>Alpha</em> is 0.329) and a high variability of the classifier’s performance.</p> 
          <a class="link-target" id="pone-0155036-g006" name="pone-0155036-g006"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g006">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g006" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g006"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g006" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g006"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g006"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g006"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g006"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g006"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g006"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 6. </span> Joint Serbian/Croatian/Bosnian dataset.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig5.caption1.p1" name="article1.body1.sec2.sec2.fig5.caption1.p1" class="link-target"></a></p>
           <p>There is an oscillation in performance and high variability.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g006"> https://doi.org/10.1371/journal.pone.0155036.g006</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p11" name="article1.body1.sec2.sec2.p11" class="link-target"></a>
          <p>The three languages, Serbian, Croatian, and Bosnian, are very similar and difficult to distinguish in short Twitter posts. However, we argue that the reason for poor performance is not in mixing the three languages, but in different annotation quality. <strong>Serbian</strong> (73,783 tweets) was annotated by 11 annotators, where two of them account for over 40% of the annotations. All the inter-annotator agreement measures come from the Serbian only (1,880 tweets annotated twice by different annotators, <em>Alpha</em> is 0.329), and there are very few tweets annotated twice by the same annotator (182 tweets only, <em>Alpha</em> for the self-agreement is 0.205). In contrast, all the Croatian and Bosnian tweets were annotated by a single annotator, and we have reliable self-agreement estimates. There are 97,291 <strong>Croatian</strong> tweets, 13,290 annotated twice, and the self-agreement <em>Alpha</em> is 0.781. There are 44,583 <strong>Bosnian</strong> tweets, 6,519 annotated twice, and the self-agreement <em>Alpha</em> is 0.722. We can conclude that the annotation quality of the Croatian and Bosnian tweets is considerably higher than of the Serbian. If we construct separate sentiment classifiers for each language we observe very different performance (see <a href="#pone-0155036-g007">Fig 7</a>). The Serbian classifier reaches the inter-annotator agreement (albeit low) at 70,000 tweets. The Croatian classifier has much higher performance, and reaches it maximum at 50,000 tweets (<em>Alpha</em> is 0.590). The performance of the Bosnian classifier is also higher, and is still increasing at 40,000 tweets (<em>Alpha</em> is 0.494). The individual classifiers are “well-behaved” in contrast to the joint Ser/Cro/Bos model in <a href="#pone-0155036-g006">Fig 6</a>. In retrospect, we can conclude that datasets with no overlapping annotations and different annotation quality are better not merged.</p> 
          <a class="link-target" id="pone-0155036-g007" name="pone-0155036-g007"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g007">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g007" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g007"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g007" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g007"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g007"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g007"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g007"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g007"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g007"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 7. </span> Separate Serbian (left), Croatian (middle), and Bosnian (right) datasets.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig6.caption1.p1" name="article1.body1.sec2.sec2.fig6.caption1.p1" class="link-target"></a></p>
           <p>Here, the lower quality Serbian set has no adverse effects on the higher quality Croatian and Bosnian sets.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g007"> https://doi.org/10.1371/journal.pone.0155036.g007</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p12" name="article1.body1.sec2.sec2.p12" class="link-target"></a>
          <p><strong>Topic shift.</strong> There is no inter-annotator agreement for the <strong>Portuguese</strong> dataset because only one annotator was engaged. However, the classifier shows interesting performance variability (<a href="#pone-0155036-g008">Fig 8</a>). After an initial peak is reached at 50,000 tweets (<em>Alpha</em> is 0.394), there is a considerable drop and a very high variability of performance. Inspection of the tweets (the set of 10,000 tweets added to the first 50,000 tweets at stage 6) revealed that at the beginning of November 2013, the Portuguese government approved additional austerity measures, affecting mainly public sector, to avoid the second international bailout. This provoked a flood of negative reactions on social media, in particular on Twitter, and a considerable shift of focus and sentiment of Twitter discussions. The classification model could not react immediately to the topic shift, and it took additional 100,000 tweets to accommodate the new topics, and the model to approach the peak performance (<em>Alpha</em> is 0.391 for the complete dataset).</p> 
          <a class="link-target" id="pone-0155036-g008" name="pone-0155036-g008"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g008">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g008" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g008"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g008" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g008"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g008"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g008"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g008"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g008"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g008"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 8. </span> The Portuguese dataset.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig7.caption1.p1" name="article1.body1.sec2.sec2.fig7.caption1.p1" class="link-target"></a></p>
           <p>There are two peaks (at 50,000 tweets, <em>Alpha</em> = 0.394, and at 160,000 tweets, <em>Alpha</em> = 0.391), and a large drop in between, due to a topic shift.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g008"> https://doi.org/10.1371/journal.pone.0155036.g008</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p13" name="article1.body1.sec2.sec2.p13" class="link-target"></a>
          <p><strong>Very low annotation quality.</strong> What happens with the classifier’s performance when the annotation quality is low? <a href="#pone-0155036-g009">Fig 9</a> shows the evolution of performance for the <strong>Spanish</strong> dataset. We observe high variability and consistent drop in performance. Most (over 95%) of the Spanish tweets were annotated by one annotator, and out of them, 40,116 tweets were annotated twice. Therefore we have a reliable estimate of the low quality of her/his annotations since the self-agreement <em>Alpha</em> is only 0.244. 2,194 tweets were annotated twice by two annotators and, not surprisingly, the inter-annotator agreement is ever lower, <em>Alpha</em> is 0.120.</p> 
          <a class="link-target" id="pone-0155036-g009" name="pone-0155036-g009"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g009">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g009" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g009"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g009" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g009"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g009"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g009"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g009"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g009"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g009"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 9. </span> The Spanish dataset.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec2.fig8.caption1.p1" name="article1.body1.sec2.sec2.fig8.caption1.p1" class="link-target"></a></p>
           <p>There is a consistent drop of performance and high variability.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g009"> https://doi.org/10.1371/journal.pone.0155036.g009</a></p>
          </div>
          <a id="article1.body1.sec2.sec2.p14" name="article1.body1.sec2.sec2.p14" class="link-target"></a>
          <p>We observe a similar performance drop for the <strong>Albanian</strong> dataset (not shown). The main annotator (who annotated over 22% of the Albanian tweets) has self-agreement <em>Alpha</em> only 0.269 (computed from 1,963 tweets annotated twice). The inter-annotator agreement <em>Alpha</em> is only 0.126.</p> 
          <a id="article1.body1.sec2.sec2.p15" name="article1.body1.sec2.sec2.p15" class="link-target"></a>
          <p>Such poorly labeled data is useless for training sentiment classifiers. However, the lesson learned is that the annotators should be monitored throughout the annotation process, that the low quality annotators (identified by a low self-agreement) should be excluded, and that the low inter-annotator agreements should be promptly investigated.</p> 
         </div> 
         <div id="section3" class="section toc-section">
          <a id="sec005" name="sec005" class="link-target" title="Application datasets analyses"></a> 
          <h3>Application datasets analyses</h3> 
          <a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1" class="link-target"></a>
          <p>The purpose of building sentiment classification models is to apply them in particular domains, e.g., to monitor elections or to predict stock prices. The models are build from labeled data (where the sentiment is given) and applied to unlabeled data (where the sentiment is to be predicted). The models are also evaluated on the labeled data (typically by 10-fold cross-validation) and the estimated performance can be extended to the application if the labeled data is representative, i.e., drawn from the same distribution as the application data. In the context of Twitter sentiment classification this means that the labeled tweets have to be not only language-, but also domain-specific.</p> 
          <a id="article1.body1.sec2.sec3.p2" name="article1.body1.sec2.sec3.p2" class="link-target"></a>
          <p>In the previous subsection we analyzed the classifiers performance on the labeled datasets and in relation to the annotator agreements. The potential improvements can be achieved by providing additional training data, by improving the inter-annotator agreements, and by excluding low quality annotators. In this subsection we also consider the relation between the training and application dataset distributions.</p> 
          <a id="article1.body1.sec2.sec3.p3" name="article1.body1.sec2.sec3.p3" class="link-target"></a>
          <p>There are four applications where we already applied and published Twitter sentiment classification to different domains. Details about the sizes and distributions of the labeled and application datasets are in the Datasets subsection in Methods. Sentiment distribution is captured by the <em>sentiment score</em> which is computed as the mean of a discrete probability distribution—details are in [<a href="#pone.0155036.ref011" class="ref-tip">11</a>]. Here we briefly analyze and suggest possible improvements with reference to the results in <a href="#pone-0155036-g001">Fig 1</a>.</p> 
          <a id="article1.body1.sec2.sec3.p4" name="article1.body1.sec2.sec3.p4" class="link-target"></a>
          <p><strong>Facebook(it) [<a href="#pone.0155036.ref008" class="ref-tip">8</a>].</strong> This is the only domain that is not limited to Twitter, but where the same sentiment classification methodology was applied to Facebook comments, in Italian. There was over 1 million Facebook comments collected, and a sample of about 20,000 was labeled for sentiment. The sentiment distribution in both sets is similar. The self-agreement and inter-annotator agreement are both high, however, there is a gap between the inter-annotator agreement (<em>Alpha</em> is 0.673) and the classifier’s performance (<em>Alpha</em> is 0.562). Based on the lessons from the language datasets, we speculate that 20,000 training examples is not enough, and that additional Facebook comments have to be labeled to approach the inter-annotator agreement.</p> 
          <a id="article1.body1.sec2.sec3.p5" name="article1.body1.sec2.sec3.p5" class="link-target"></a>
          <p><strong>DJIA30 [<a href="#pone.0155036.ref009" class="ref-tip">9</a>].</strong> This domain deals with English tweets, but very specific for financial markets. The sentiment labeling requires considerable domain knowledge about specific financial terminology. There were over 1.5 million tweets about the Dow Jones stocks collected, and a sample of about 100,000 was annotated for sentiment. The sentiment distribution in both sets is very similar. The annotators self-agreement is high, but the inter-annotator agreement is relatively low (<em>Alpha</em> is 0.438), and the classifier even slightly exceeds it. Also, in the period from June 2013 to September 2014, a relatively small fraction of tweets was annotated twice (5,934), so the agreement estimates are less reliable. These considerations were taken into account in the subsequent period: from June 2014 to May 2015 altogether 19,720 tweets were annotated twice, and the inter-annotator agreement improved for 10% points (new <em>Alpha</em> is 0.482).</p> 
          <a id="article1.body1.sec2.sec3.p6" name="article1.body1.sec2.sec3.p6" class="link-target"></a>
          <p><strong>Environment [<a href="#pone.0155036.ref010" class="ref-tip">10</a>].</strong> This domain deals with sentiment leaning towards various environmental issues (like climate change, fossil fuels, fracking, etc.)—not so well-defined problem. Consequently, the self-agreement and inter-annotator agreement are relatively low in comparison to the Facebook(it) dataset. Still, there is a gap between the inter-annotator agreement (<em>Alpha</em> is 0.510) and the classifier’s performance (<em>Alpha</em> is 0.397). The training set consists of only about 20,000 labeled tweets, and in analogy to the language datasets and Facebook(it) we conclude that additional tweets have to be labeled to improve the classifier performance.</p> 
          <a id="article1.body1.sec2.sec3.p7" name="article1.body1.sec2.sec3.p7" class="link-target"></a>
          <p>However, there is another issue. There were altogether over 3 million tweets collected, and sentiment distribution in the training set is considerably different from the application set (sentiment scores are ?0.137 and +0.015, respectively; see <a href="#pone-0155036-g010">Fig 10</a>). The sampling was done just in the initial phases of the Twitter acquisition and is not representative of the whole application dataset.</p> 
          <a class="link-target" id="pone-0155036-g010" name="pone-0155036-g010"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g010">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g010" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g010"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g010" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g010"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g010"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g010"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g010"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g010"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g010"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 10. </span> The sentiment distribution of the environmental tweets in the training and application sets.
           </div>
           <p class="caption_target"><a id="article1.body1.sec2.sec3.fig1.caption1.p1" name="article1.body1.sec2.sec3.fig1.caption1.p1" class="link-target"></a></p>
           <p>Negative tweets are denoted by red, neutral by yellow, and positive by green color. The grey bar denotes the sentiment score (the mean) of each dataset.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g010"> https://doi.org/10.1371/journal.pone.0155036.g010</a></p>
          </div>
          <a id="article1.body1.sec2.sec3.p8" name="article1.body1.sec2.sec3.p8" class="link-target"></a>
          <p>We conducted an additional experiment to demonstrate the effects of different training and application sets. We applied the general English language sentiment classification model from the previous subsection, trained on all 90,000 English tweets, to the labeled environmental tweets. The classifier’s performance (<em>Alpha</em> is 0.243) is considerably lower in comparison to the environment-specific model (<em>Alpha</em> is 0.397) which was trained on only 20,000 domain-specific tweets. The same holds for the <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e004" class="inline-graphic"></span> measure. Detailed evaluation results are in the Classification models performance subsection in Methods. This result confirms our thesis that Twitter sentiment classification is sensitive to domain of application and that sentiment labeling has to be domain-specific.</p> 
          <a id="article1.body1.sec2.sec3.p9" name="article1.body1.sec2.sec3.p9" class="link-target"></a>
          <p>Note also that the general English classifier has higher accuracy (<em>Acc</em> is 0.604) than the environment-specific model (<em>Acc</em> is 0.556). Our conclusion is that this is a clear indication that accuracy is a misleading evaluation measure for the ordered three-class sentiment classification problem.</p> 
          <a id="article1.body1.sec2.sec3.p10" name="article1.body1.sec2.sec3.p10" class="link-target"></a>
          <p><strong>Emojis [<a href="#pone.0155036.ref011" class="ref-tip">11</a>].</strong> There is no automated sentiment classification with the Emojis dataset. From the 13 language datasets which consist in total of over 1.6 labeled tweets, we selected only the tweets that contain emojis, about 70,000 in total. The goal was to attribute the sentiment to emojis, based on the sentiment of all the tweets in which they occur. <a href="#pone-0155036-g001">Fig 1</a> shows that Emojis is the only dataset where the self-agreement (<em>Alpha</em> is 0.544) is lower than the inter-annotator agreement (<em>Alpha</em> is 0.597). The reason for this anomaly is a large share of Spanish tweets with emojis (about 20,000) that have very low self-agreement (<em>Alpha</em> is 0.245). If we remove them from the Emojis set, the self-agreement increases considerably (new <em>Alpha</em> is 0.720), while the inter-annotators agreement remains almost unchanged (new <em>Alpha</em> is 0.598). This reconfirms our conclusion that low quality annotators have to be excluded and their annotations removed from the datasets.</p> 
         </div> 
        </div> 
        <div id="section3" class="section toc-section">
         <a id="sec006" name="sec006" data-toc="sec006" class="link-target" title="Conclusions"></a>
         <h2>Conclusions</h2>
         <a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1" class="link-target"></a>
         <p>We present an analysis of over 1.6 million sentiment annotated Twitter posts, by far the largest set made publicly available until now. The labeled datasets are used to train sentiment classification models, and our analysis focuses on four main aspects: quality, quantity and sampling of the training data, and performance of the classifiers. Our main conclusion is that the choice of a particular classifier type is not so important, but that the training data has a major impact on the results.</p> 
         <a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2" class="link-target"></a>
         <p>There are several specific research questions we address:</p> 
         <ol class="order"> 
          <li>What is the nature and proper formalization of the sentiment classification problem, in particular, are the sentiment values ordered or not? We show that there is strong evidence that the sentiment values, <em>negative</em>, <em>neutral</em>, and <em>positive</em>, are perceived as ordered by human annotators (see subsection on Ordering of sentiment values in Methods).</li> 
          <li>Which evaluation measures should be used to properly quantify the data quality and classifiers performance? In all the experiment, we compute values for four evaluation measures (<em>Acc</em>±1, <em>Acc</em>, <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e005" class="inline-graphic"></span>, and <em>Alpha</em>). Since there is evidence that sentiment values are ordered, <em>Alpha</em> and <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e006" class="inline-graphic"></span> are the most appropriate as they take the ordering into account.</li> 
          <li>How to estimate the quality of the training data? We propose to invest an extra effort to label a portion of tweets twice, and then to compute the annotator self-agreement and the inter-annotator agreement. The self-agreement yields a useful indication when to exclude low quality annotators, and the inter-annotator agreement approximates an upper bound on the performance of sentiment classifiers.</li> 
          <li>How to select the most appropriate classifier? Our results show that there are no statistically significant differences between the top classifiers. As a consequence, one should better direct the efforts into higher training data quality.</li> 
          <li>What are acceptable levels of annotators agreement? On the basis of the 17 datasets analyzed, we propose the following rule-of-thumb: for self-agreement, <em>Alpha</em> &gt; 0.6, and for the inter-annotator agreement, <em>Alpha</em> &gt; 0.4.</li> 
          <li>How many posts should be labeled with sentiment for training? We cannot provide conclusive answers here. It seems that 20,000 high-quality annotations already provide reasonable performance. The peak performance depends on the inter-annotator agreement and we estimate that around 100,000 annotations are needed. However, more important than sheer quantity is the quality, and domain- and topic-specific coverage of the posts, as demonstrated on several use-cases.</li> 
         </ol>
         <a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3" class="link-target"></a>
         <p>This gives the following directions for the short-term future work. The annotation process has to be redesigned to allow for systematic monitoring of the annotation quality. In particular, more than one annotator per language/domain has to be engaged. We propose an increased overhead of posts to be labeled twice, from 15% to 20%, both by individuals as well as by two different annotators. The posts to be labeled multiple times could be based on their “importance” as measured by their retweet count [<a href="#pone.0155036.ref010" class="ref-tip">10</a>], for example. The self- and the inter-annotator agreements have to be continuously monitored and warnings issued when they drop below the selected thresholds. Extreme disagreements (as measured by <em>Acc</em>±1) should be promptly directed to a “master” annotator who has to resolve the disagreement and issue a proper annotation together with a brief guideline. After each batch of a few thousand annotations, a classification model should be trained and its performance evaluated. This would help in monitoring the progress towards the inter-annotator agreement as well as in detecting possible abrupt topic shifts.</p> 
         <a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4" class="link-target"></a>
         <p>There is a number of open research questions to be addressed. One is how to combine the lexicon-based and machine learning approaches to sentiment classification. In [<a href="#pone.0155036.ref006" class="ref-tip">6</a>], authors already showed that the combination of both outperforms the individual approaches. However, sentiment lexicons are rarely available for languages other than English and require considerable efforts to construct. For several languages, one could use the data published by Dodds et al. [<a href="#pone.0155036.ref003" class="ref-tip">3</a>]. For the languages covered in this study, one can construct a basic sentiment lexicon from the annotated tweets, in the analogy to derivation of the emoji sentiment lexicon [<a href="#pone.0155036.ref011" class="ref-tip">11</a>].</p> 
         <a id="article1.body1.sec3.p5" name="article1.body1.sec3.p5" class="link-target"></a>
         <p>Another research direction, with the potential of considerable performance improvements, is the construction and selection of informative features from short Twitter posts. In this study we apply a number of standard text pre-processing steps to extract just the textual features and eliminate noise in tweets. However, there is a lot of additional information on Twitter to be exploited. For example, the importance of tweets (estimated by the retweet count, for example), the influence and reliability of Twitter users (estimated by their followers, retweets, and correlations to the real-world events), and the network features (e.g., neighbourhood and centrality) that can be attributed to the users, and indirectly to their tweets. We expect that proper considerations of the broader context in which the tweets are posted can provide for a major leap in quality and predictive potential of the Twitter sentiment classifiers.</p> 
         <a id="article1.body1.sec3.p6" name="article1.body1.sec3.p6" class="link-target"></a>
         <p>Finally, since the analysis of opinions expressed in social media is an active and evolving research area, we plan to keep up with the newest trends, such as performing entity-based sentiment analysis [<a href="#pone.0155036.ref017" class="ref-tip">17</a>], applying deep learning techniques [<a href="#pone.0155036.ref018" class="ref-tip">18</a>–<a href="#pone.0155036.ref020" class="ref-tip">20</a>], analyzing figurative language (e.g., irony or sarcasm) [<a href="#pone.0155036.ref021" class="ref-tip">21</a>], and detecting different types of emotions (e.g., joy, sadness or anger) [<a href="#pone.0155036.ref022" class="ref-tip">22</a>]. The most interesting direction seems to be a shift from the basic sentiment categories (negative, neutral, and positive) of the whole tweet, to the finer-grained emotions about a discussed entity or topic.</p> 
        </div> 
        <div id="section4" class="section toc-section">
         <a id="sec007" name="sec007" data-toc="sec007" class="link-target" title="Methods"></a>
         <h2>Methods</h2> 
         <div id="section1" class="section toc-section">
          <a id="sec008" name="sec008" class="link-target" title="Ethics statement"></a> 
          <h3>Ethics statement</h3> 
          <a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1" class="link-target"></a>
          <p>The tweets were collected through the public Twitter API and are subject to the Twitter terms and conditions. The human annotators were engaged for the purpose of sentiment labeling, and were aware that their annotations will be used to construct the sentiment classification models, and to estimate the annotator self-agreement and the inter-annotator agreement.</p> 
         </div> 
         <div id="section2" class="section toc-section">
          <a id="sec009" name="sec009" class="link-target" title="Datasets"></a> 
          <h3>Datasets</h3> 
          <a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1" class="link-target"></a>
          <p>In this study we analyze two corpora of data (see <a href="#pone-0155036-t001">Table 1</a>). The first corpus is a collection of tweets, in 13 European languages, posted between April 2013 and February 2015. The tweets, except English, were collected during a joint project with Gama System (<a href="http://www.gama-system.si">http://www.gama-system.si</a>), using their PerceptionAnalytics platform (<a href="http://www.perceptionanalytics.net">http://www.perceptionanalytics.net</a>). The tweets were acquired through Twitter Search API, by specifying the geolocations of the largest cities. For English tweets, we used Twitter Streaming API (a random sample of 1% of all the public tweets), and filtered out the English posts.</p> 
          <a class="link-target" id="pone-0155036-t001" name="pone-0155036-t001"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.t001">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.t001" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.t001"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.t001" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t001"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t001"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t001"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t001"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t001"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t001"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Table 1. </span> The number and distribution of sentiment annotated posts, and the time period of the posts.
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec2.table-wrap1.caption1.p1" name="article1.body1.sec4.sec2.table-wrap1.caption1.p1" class="link-target"></a></p>
           <p>The top part of the table refers to the 13 language datasets, and the bottom to the four application datasets.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.t001"> https://doi.org/10.1371/journal.pone.0155036.t001</a></p>
          </div>
          <a id="article1.body1.sec4.sec2.p2" name="article1.body1.sec4.sec2.p2" class="link-target"></a>
          <p>83 native speakers (except for English) were engaged to manually label with sentiment over 1.6 million of the collected tweets. The annotation process was supported by the Goldfinch platform (provided by Sowa Labs, <a href="http://www.sowalabs.com">http://www.sowalabs.com</a>), designed specifically for sentiment annotation of short texts (such as Twitter posts, Facebook comments, etc.). The annotators were instructed to label each tweet as either <em>negative</em>, <em>neutral</em>, or <em>positive</em>, by estimating the emotional attitude of the user who posted the tweet. Tweets that were skipped or excluded are not considered in this study.</p> 
          <a id="article1.body1.sec4.sec2.p3" name="article1.body1.sec4.sec2.p3" class="link-target"></a>
          <p>The second corpus of data are four application datasets, used in different application scenarios and already published [<a href="#pone.0155036.ref008" class="ref-tip">8</a>–<a href="#pone.0155036.ref011" class="ref-tip">11</a>].</p> 
          <a id="article1.body1.sec4.sec2.p4" name="article1.body1.sec4.sec2.p4" class="link-target"></a>
          <p>The datasets in <a href="#pone-0155036-t001">Table 1</a> are used to analyze the annotator agreements, and to build the sentiment classification models. The classification models build from three out of four application datasets were actually applied to much larger sets of unlabeled data, to predict the sentiment. Details are in <a href="#pone-0155036-t002">Table 2</a>. For each of the three application domains we also show the difference between the application and training phase in terms of the sentiment score (the mean of a discrete probability distribution, see [<a href="#pone.0155036.ref011" class="ref-tip">11</a>] for details). For the Emojis dataset, no sentiment classification model was trained—the tweets with emojis were just extracted from the above corpus of 13 language datasets.</p> 
          <a class="link-target" id="pone-0155036-t002" name="pone-0155036-t002"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.t002">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.t002" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.t002"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.t002" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t002"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t002"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t002"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t002"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t002"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t002"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Table 2. </span> Sentiment distributions of the application datasets as predicted by the sentiment classifiers.
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec2.table-wrap2.caption1.p1" name="article1.body1.sec4.sec2.table-wrap2.caption1.p1" class="link-target"></a></p>
           <p>The rightmost column shows the sentiment score (the mean) of the application and training datasets (the later from <a href="#pone-0155036-t001">Table 1</a>), respectively.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.t002"> https://doi.org/10.1371/journal.pone.0155036.t002</a></p>
          </div>
          <a id="article1.body1.sec4.sec2.p5" name="article1.body1.sec4.sec2.p5" class="link-target"></a>
          <p> <a href="#pone-0155036-t003">Table 3</a> gives the details of the number of posts annotated twice, by the same annotator or by two different annotators.</p> 
          <a class="link-target" id="pone-0155036-t003" name="pone-0155036-t003"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.t003">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.t003" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.t003"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.t003" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t003"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t003"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t003"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t003"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t003"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t003"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Table 3. </span> The number of annotators, and the number and fraction of posts annotated twice.
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec2.table-wrap3.caption1.p1" name="article1.body1.sec4.sec2.table-wrap3.caption1.p1" class="link-target"></a></p>
           <p>The self-agreement column gives the number of posts annotated twice by the same annotator, and the inter-agreement column the posts annotated twice by two different annotators.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.t003"> https://doi.org/10.1371/journal.pone.0155036.t003</a></p>
          </div>
          <a id="article1.body1.sec4.sec2.p6" name="article1.body1.sec4.sec2.p6" class="link-target"></a>
          <p>The 13 language datasets are publicly available for further analyses. Actually, our analysis reveales that it is better to partition the Ser/Cro/Bos dataset into the three constituent languages, therefore we provide the sentiment annotation data for the 15 languages. The data is available as 15 language files, in the csv format, in a public language resource repository <span class="small-caps">clarin</span>.<span class="small-caps">si</span> at <a href="http://hdl.handle.net/11356/1054">http://hdl.handle.net/11356/1054</a>. For each language and for each labeled tweet, there is the tweet ID (as provided and required by Twitter), the sentiment label (<em>negative</em>, <em>neutral</em>, or <em>positive</em>), and the annotator ID (anonymized). From this data, one can compute the annotator agreement measures, construct the “gold standard” training data, and train the classifiers for different languages.</p> 
         </div> 
         <div id="section3" class="section toc-section">
          <a id="sec010" name="sec010" class="link-target" title="Evaluation measures"></a> 
          <h3>Evaluation measures</h3> 
          <a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1" class="link-target"></a>
          <p>In general, the agreement can be estimated between any two methods of generating data. One of the main ideas of this work is to use the same measures to estimate the agreement between the human annotators as well as the agreement between the results of automated classification and the “gold standard”. There are different measures of agreement, and to get robust estimates we apply four well-known measures from the fields of inter-rater agreement and machine learning.</p> 
          <a id="article1.body1.sec4.sec3.p2" name="article1.body1.sec4.sec3.p2" class="link-target"></a>
          <p><strong>Krippendorff’s Alpha-reliability</strong> (<em>Alpha</em>) [<a href="#pone.0155036.ref007" class="ref-tip">7</a>] is a generalization of several specialized agreement measures. It works for any number of annotators, and is applicable to different variable types and metrics (e.g., nominal, ordered, interval, etc.). <em>Alpha</em> is defined as follows: <a name="pone.0155036.e007" id="pone.0155036.e007" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e007" class="inline-graphic"></span> where <em>D</em><sub><em>o</em></sub> is the observed disagreement between annotators, and <em>D</em><sub><em>e</em></sub> is a disagreement, expected by chance. When annotators agree perfectly, <em>Alpha</em> = 1, and when the level of agreement equals the agreement by chance, <em>Alpha</em> = 0. The two disagreement measures are defined as follows: <a name="pone.0155036.e008" id="pone.0155036.e008" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e008" class="inline-graphic"></span> <a name="pone.0155036.e009" id="pone.0155036.e009" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e009" class="inline-graphic"></span> The arguments, <em>N</em>,<em>N</em>(<em>c</em>,<em>c</em>?),<em>N</em>(<em>c</em>), and <em>N</em>(<em>c</em>?), refer to the frequencies in a coincidence matrix, defined below. <em>?</em>(<em>c</em>,<em>c</em>?) is a difference function between the values of <em>c</em> and <em>c</em>?, and depends on the metric properties of the variable. <em>c</em> (and <em>c</em>?) is a discrete sentiment variable with three possible values: <em>negative</em> (?), <em>neutral</em> (0), or <em>positive</em> (+). We consider two options: either the sentiment variable <em>c</em> is nominal or ordered. This gives rise to two instance of <em>Alpha</em>, <em>Alpha</em><sub><em>nom</em></sub> (nominal, when <em>c</em> is unordered) and <em>Alpha</em><sub><em>int</em></sub> (interval, when <em>c</em> is ordered), corresponding to two difference functions <em>?</em>: <a name="pone.0155036.e010" id="pone.0155036.e010" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e010" class="inline-graphic"></span> <a name="pone.0155036.e011" id="pone.0155036.e011" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e011" class="inline-graphic"></span> Note that in the case of the <em>interval</em> difference function, <em>?</em> assigns a disagreement of 1 between the <em>neutral</em> and the <em>negative</em> or <em>positive</em> sentiment, and a disagreement of 2 between the extremes, i.e., the <em>negative</em> and <em>positive</em> sentiment. The corresponding disagreements <em>D</em><sub><em>o</em></sub> and <em>D</em><sub><em>e</em></sub> between the extreme classes are then four times larger than between the neighbouring classes.</p> 
          <a id="article1.body1.sec4.sec3.p3" name="article1.body1.sec4.sec3.p3" class="link-target"></a>
          <p>A coincidence matrix tabulates all pairable values of <em>c</em> from two annotators into a <em>k</em>-by-<em>k</em> square matrix, where <em>k</em> is the number of possible values of <em>c</em>. In the case of sentiment annotations, we have a 3-by-3 coincidence matrix. The diagonal contains all the perfect matches, and the matrix is symmetrical around the diagonal. A coincidence matrix has the following general form: <img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e029" class="inline-graphic"> In our case, <em>c</em> and <em>c</em>? range over the three possible sentiment values. In a coincidence matrix, each labeled unit is entered twice, once as a (<em>c</em>,<em>c</em>?) pair, and once as a (<em>c</em>?,<em>c</em>) pair. <em>N</em>(<em>c</em>,<em>c</em>?) is the number of units labeled by the values <em>c</em> and <em>c</em>? by different annotators, <em>N</em>(<em>c</em>) and <em>N</em>(<em>c</em>?) are the totals for each value, and <em>N</em> is the grand total.</p> 
          <a id="article1.body1.sec4.sec3.p4" name="article1.body1.sec4.sec3.p4" class="link-target"></a>
          <p>The computed values of <em>Alpha</em> are subject to sampling variability, determined by an unknown sampling distribution. The sampling distribution can be approximated by bootstrapping [<a href="#pone.0155036.ref012" class="ref-tip">12</a>]. In our case, we set the number of bootstrap samples to 1,000, and estimate the 95% confidence interval of true <em>Alpha</em>.</p> 
          <a id="article1.body1.sec4.sec3.p5" name="article1.body1.sec4.sec3.p5" class="link-target"></a>
          <p><strong>F score</strong> (<span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e012" class="inline-graphic"></span>) is an instance of a well-known effectiveness measure in information retrieval [<a href="#pone.0155036.ref023" class="ref-tip">23</a>]. We use an instance specifically designed to evaluate the 3-class sentiment classifiers [<a href="#pone.0155036.ref024" class="ref-tip">24</a>]. <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e013" class="inline-graphic"></span> is defined as follows: <a name="pone.0155036.e014" id="pone.0155036.e014" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e014" class="inline-graphic"></span> <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e015" class="inline-graphic"></span> implicitly takes into account the ordering of sentiment values, by considering only the <em>negative</em> (?) and <em>positive</em> (+) labels. The middle, <em>neutral</em>, label is taken into account only indirectly. In general, <em>F</em><sub>1</sub>(<em>c</em>) is a harmonic mean of precision and recall for class <em>c</em>. In the case of a coincidence matrix, which is symmetric, the ‘precision’ and ‘recall’ are equal, and thus <em>F</em><sub>1</sub>(<em>c</em>) degenerates into: <a name="pone.0155036.e016" id="pone.0155036.e016" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e016" class="inline-graphic"></span> In terms of the annotator agreement, <em>F</em><sub>1</sub>(<em>c</em>) is the fraction of equally labeled tweets out of all the tweets with label <em>c</em>.</p> 
          <a id="article1.body1.sec4.sec3.p6" name="article1.body1.sec4.sec3.p6" class="link-target"></a>
          <p><strong>Accuracy</strong> (<em>Acc</em>) is a common, and the simplest, measure of performance of the model which measures the agreement between the model and the “gold standard”. <em>Acc</em> is defined in terms of the observed disagreement <em>D</em><sub><em>o</em></sub>: <a name="pone.0155036.e017" id="pone.0155036.e017" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e017" class="inline-graphic"></span> <em>Acc</em> is simply the fraction of the diagonal elements of the coincidence matrix. Note that it does not account for the (dis)agreement by chance, nor for the ordering of the sentiment values.</p> 
          <a id="article1.body1.sec4.sec3.p7" name="article1.body1.sec4.sec3.p7" class="link-target"></a>
          <p><strong>Accuracy within 1</strong> (<em>Acc</em>±1) is a special case of <em>accuracy within n</em> [<a href="#pone.0155036.ref025" class="ref-tip">25</a>]. It assumes ordered classes and extends the range of predictions considered correct to the <em>n</em> neighbouring class values. In our case, <em>Acc</em>±1 considers as incorrect only misclassifications from <em>negative</em> to <em>positive</em> and vice-versa: <a name="pone.0155036.e018" id="pone.0155036.e018" class="link-target"></a><span class="equation"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e018" class="inline-graphic"></span> Note that it is easy to maximize <em>Acc</em>±1 by simply classifying all the examples as <em>neutral</em>; then <em>Acc</em>±1 = 1.</p> 
          <a id="article1.body1.sec4.sec3.p8" name="article1.body1.sec4.sec3.p8" class="link-target"></a>
          <p>The four agreement measures are always computed from the same coincidence matrix. In the case of the annotator agreements, the coincidence matrix is formed from the pairs of sentiment labels assigned to a tweet by different annotators (or the same when she/he annotated the tweet several times). In the case of a classification model, an entry in the coincidence matrix is a pair of labels, one from the model prediction, and the other from the “gold standard”. Experiments show that a typical ordering of the agreement results is: <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e019" class="inline-graphic"></span>.</p> 
         </div> 
         <div id="section4" class="section toc-section">
          <a id="sec011" name="sec011" class="link-target" title="The annotator agreements"></a> 
          <h3>The annotator agreements</h3> 
          <a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1" class="link-target"></a>
          <p> <a href="#pone-0155036-t004">Table 4</a> gives the results of the annotator agreements in terms of the four evaluation measures. The self-agreement is computed from the tweets annotated twice by the same annotator, and the inter-annotator agreement from the tweets annotated twice by two different annotators, where possible. The 95% confidence intervals for <em>Alpha</em> are computed from 1,000 bootstrap samples.</p> 
          <a class="link-target" id="pone-0155036-t004" name="pone-0155036-t004"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.t004">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.t004" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.t004"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.t004" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t004"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t004"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t004"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t004"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t004"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t004"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Table 4. </span> The self- and inter-annotator agreement measures.
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec4.table-wrap1.caption1.p1" name="article1.body1.sec4.sec4.table-wrap1.caption1.p1" class="link-target"></a></p>
           <p>The 95% confidence intervals for <em>Alpha</em> are computed by bootstrapping. Albanian and Spanish (in bold) have very low agreement values.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.t004"> https://doi.org/10.1371/journal.pone.0155036.t004</a></p>
          </div>
          <a id="article1.body1.sec4.sec4.p2" name="article1.body1.sec4.sec4.p2" class="link-target"></a>
          <p>Note that the Albanian and Spanish datasets have very low <em>Alpha</em> agreement values. All the results for <em>Alpha</em>, reported here and throughout the paper, refer to the <em>Alpha</em><sub><em>int</em></sub> instance, for the reasons outlined in the next subsection.</p> 
         </div> 
         <div id="section5" class="section toc-section">
          <a id="sec012" name="sec012" class="link-target" title="Ordering of sentiment values"></a> 
          <h3>Ordering of sentiment values</h3> 
          <a id="article1.body1.sec4.sec5.p1" name="article1.body1.sec4.sec5.p1" class="link-target"></a>
          <p>Should the sentiment classes <em>negative</em> (?), <em>neutral</em> (0), and <em>positive</em> (+) be treated as nominal (categorical, unordered) or ordered? One can use the agreement measures to estimate how are the three classes perceived by the human annotators.</p> 
          <a id="article1.body1.sec4.sec5.p2" name="article1.body1.sec4.sec5.p2" class="link-target"></a>
          <p>First, lets compare the agreements in terms of two variants of <em>Alpha</em>: <em>Alpha</em><sub><em>int</em></sub> (interval) and <em>Alpha</em><sub><em>nom</em></sub> (nominal). The difference between the two measures is that <em>Alpha</em><sub><em>int</em></sub> assigns four times higher cost to extreme disagreements (between the negative and positive classes) than <em>Alpha</em><sub><em>nom</em></sub>. A measure which yields higher agreements hints at the nature of sentiment class ordering as perceived by humans. The results in <a href="#pone-0155036-t005">Table 5</a>, column two, show that <em>Alpha</em><sub><em>int</em></sub> always yields higher agreement than <em>Alpha</em><sub><em>nom</em></sub>, except for Spanish. We compute the average relative agreement gains by ignoring the Albanian and Spanish datasets (which have poor annotation quality), and Emojis (which are already subsumed by the 13 language datasets). We observe that the average agreement is 18% higher with <em>Alpha</em><sub><em>int</em></sub> than with <em>Alpha</em><sub><em>nom</em></sub>. This gives a strong indication that the sentiment classes are perceived as ordered by the annotators.</p> 
          <a class="link-target" id="pone-0155036-t005" name="pone-0155036-t005"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.t005">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.t005" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.t005"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.t005" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t005"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t005"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t005"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t005"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t005"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t005"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Table 5. </span> Differences between the three sentiment classes (?,0,+).
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec5.table-wrap1.caption1.p1" name="article1.body1.sec4.sec5.table-wrap1.caption1.p1" class="link-target"></a></p>
           <p>The differences are measured in terms of <em>Alpha</em>, for the union of self- and inter-annotator agreements. The second column shows the relative difference between the <em>Alpha</em><sub><em>int</em></sub> (interval) and <em>Alpha</em><sub><em>nom</em></sub> (nominal) agreement measures. The third and fourth columns show the distances of the negative (?) and positive (+) class to the neutral class (0), respectively, normalized with the distance between them. The last row is the average difference, but without the low quality Albanian and Spanish, and the subsumed Emojis datasets (in bold). Only the numbers in bold do not support the thesis that sentiment classes are ordered.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.t005"> https://doi.org/10.1371/journal.pone.0155036.t005</a></p>
          </div>
          <a id="article1.body1.sec4.sec5.p3" name="article1.body1.sec4.sec5.p3" class="link-target"></a>
          <p>Second, we can use the agreement as a proxy to measure the “distance” between the sentiment classes. Lets assume that the difficulty of distinguishing between the extreme classes (?, +), as measured by <em>Alpha</em>, is normalized to 1. If it is more difficult to distinguish between the neutral (0) and each extreme (? or +) then the normalized agreement will be lower than 1, otherwise it will be greater than 1. The results in <a href="#pone-0155036-t005">Table 5</a>, columns three and four, indicate that for almost all the datasets the normalized agreement is lower than 1. The only exceptions are Slovak and Spanish. If we ignore the Albanian, Spanish, and Emojis we observe the following average differences: (i) it is 27% (1?0.73) more difficult to distinguish between the <em>negative</em> (?) and <em>neutral</em> (0) than between the <em>negative</em> (?) and <em>positive</em> (+); and (ii) it is 35% (1?0.65) more difficult to distinguish between the <em>positive</em> (+) and <em>neutral</em> (0) than between the <em>positive</em> (+) and <em>negative</em> (?).</p> 
          <a id="article1.body1.sec4.sec5.p4" name="article1.body1.sec4.sec5.p4" class="link-target"></a>
          <p>The above results support our hypothesis that the sentiment values are ordered: <em>negative</em> ? <em>neutral</em> ? <em>positive</em>. This has an implication on the selection of an appropriate performance measure and a classification model. The performance measure should take the class ordering into account, therefore our selection of <em>Alpha</em><sub><em>int</em></sub> over <em>Alpha</em><sub><em>nom</em></sub> is justified. In this respect, <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e025" class="inline-graphic"></span> would also be appropriate, and it actually shows high correlation to <em>Alpha</em><sub><em>int</em></sub>. The choice of an appropriate classification model is discussed in the next two subsections.</p> 
         </div> 
         <div id="section6" class="section toc-section">
          <a id="sec013" name="sec013" class="link-target" title="Related sentiment classification approaches"></a> 
          <h3>Related sentiment classification approaches</h3> 
          <a id="article1.body1.sec4.sec6.p1" name="article1.body1.sec4.sec6.p1" class="link-target"></a>
          <p>In this subsection we give an overview of the related work on automated sentiment classification of Twitter posts. We summarize the published labeled sets used for training the classification models, and the machine learning methods applied for training. Most of the related work is limited to English texts only.</p> 
          <a id="article1.body1.sec4.sec6.p2" name="article1.body1.sec4.sec6.p2" class="link-target"></a>
          <p>To train a sentiment classifier, one needs a fairly large training dataset of tweets already labeled with sentiment. One can rely on a proxy, e.g., emoticons used in the tweets to determine the intended sentiment [<a href="#pone.0155036.ref026" class="ref-tip">26</a>], however, high quality labeling requires engagement of human annotators.</p> 
          <a id="article1.body1.sec4.sec6.p3" name="article1.body1.sec4.sec6.p3" class="link-target"></a>
          <p>There exist several publicly available and manually labeled Twitter datasets. They vary in the number of examples from several hundreds to several thousands, but to the best of our knowledge, none exceeds 20,000 entries. Saif et al. [<a href="#pone.0155036.ref017" class="ref-tip">17</a>] describe eight Twitter sentiment datasets and also introduce a new one which contains separate sentiment labels for tweets and entities. Rosenthal et al. [<a href="#pone.0155036.ref027" class="ref-tip">27</a>] provide statistics for several of the 2013–2015 SemEval datasets. Haldenwang and Vornberger [<a href="#pone.0155036.ref028" class="ref-tip">28</a>] present a publicly available collection of Twitter posts, which were labeled not only with the positive or negative sentiment, but also as uncertain or spam. Finally, several Twitter sentiment datasets are publicly available in CrowdFlower’s “Data for Everyone” collection.</p> 
          <a id="article1.body1.sec4.sec6.p4" name="article1.body1.sec4.sec6.p4" class="link-target"></a>
          <p>There are several supervised machine learning algorithms suitable to train sentiment classifiers from sentiment labeled tweets. For example, in the SemEval-2015 competition, for the task on Sentiment Analysis on Twitter [<a href="#pone.0155036.ref027" class="ref-tip">27</a>], the most often used algorithms are Support Vector Machines (SVM), Maximum Entropy, Conditional Random Fields, and linear regression. In other cases, frequently used are also Naive Bayes, k-Nearest-Neighbor, and even Decision Trees. In the following we cite several relevant papers, and report, where available, the comparison in performance between the algorithms used.</p> 
          <a id="article1.body1.sec4.sec6.p5" name="article1.body1.sec4.sec6.p5" class="link-target"></a>
          <p>Go et al. [<a href="#pone.0155036.ref026" class="ref-tip">26</a>] employ the keyword-based approach, Naive Bayes, Maximum Entropy, and SVM, and show that the best performing algorithm is Maximum Entropy. The authors in [<a href="#pone.0155036.ref029" class="ref-tip">29</a>] show that Maximum Entropy outperforms Naive Bayes. In contrast, the authors in [<a href="#pone.0155036.ref030" class="ref-tip">30</a>] report that Naive Bayes performs considerably better than Maximum Entropy. Pak and Paroubek [<a href="#pone.0155036.ref031" class="ref-tip">31</a>] show that Naive Bayes outperforms the SVM and Conditional Random Fields algorithms. Asiaee et al. [<a href="#pone.0155036.ref032" class="ref-tip">32</a>] employ a dictionary learning approach, weighted SVM, k-Nearest-Neighbor, and Naive Bayes—Naive Bayes and its weighted variant are among the best performing algorithms. Saif et al. [<a href="#pone.0155036.ref033" class="ref-tip">33</a>] employ Naive Bayes for predicting sentiment in tweets.</p> 
          <a id="article1.body1.sec4.sec6.p6" name="article1.body1.sec4.sec6.p6" class="link-target"></a>
          <p>Often, SVM is shown as the best performing classifier for Twitter sentiment. For example, [<a href="#pone.0155036.ref034" class="ref-tip">34</a>] test several algorithms implemented in Weka, and SVM performed best. The authors in [<a href="#pone.0155036.ref006" class="ref-tip">6</a>] test the Naive Bayes, Decision Trees, and SVM algorithms, and find that the best performing algorithm is SVM. Preliminary results reported in [<a href="#pone.0155036.ref024" class="ref-tip">24</a>] show that linear SVM yields better performance than the Maximum Entropy classifier. Jiang et al. [<a href="#pone.0155036.ref035" class="ref-tip">35</a>] employ SVM models for subjectivity and polarity classification of Twitter posts. Davidov et al. [<a href="#pone.0155036.ref036" class="ref-tip">36</a>] employ k-Nearest-Neighbor. Kouloumpis et al. [<a href="#pone.0155036.ref037" class="ref-tip">37</a>] employ AdaBoost.MH, and also test SVMs, but the performance results of SVMs are lower. Recently, researchers also applied deep learning for Twitter sentiment classification [<a href="#pone.0155036.ref018" class="ref-tip">18</a>–<a href="#pone.0155036.ref020" class="ref-tip">20</a>].</p> 
          <a id="article1.body1.sec4.sec6.p7" name="article1.body1.sec4.sec6.p7" class="link-target"></a>
          <p>A wide range of machine learning algorithms is used, and apparently there is no consensus on which one to choose for the best performance. Different studies use different datasets, focus on different use cases, and use incompatible evaluation measures. There are additional factors with considerable impact on the performance, such as the natural language pre-processing of tweets, and formation of appropriate features. Typically, features are based on the bag-of-words presentation of tweets, but there are many subtle choices to be made.</p> 
         </div> 
         <div id="section7" class="section toc-section">
          <a id="sec014" name="sec014" class="link-target" title="Classification models performance"></a> 
          <h3>Classification models performance</h3> 
          <a id="article1.body1.sec4.sec7.p1" name="article1.body1.sec4.sec7.p1" class="link-target"></a>
          <p>As discussed in the previous subsection, there are many supervised machine learning algorithms suitable for training sentiment classification models. Variants of Support Vector Machine (SVM) [<a href="#pone.0155036.ref013" class="ref-tip">13</a>] are often used, because they are well suited for large-scale text categorization tasks, are robust on large feature spaces, and perform well. The basic SVM is a two-class, binary classifier. In the training phase, SVM constructs a hyperplane in a high-dimensional vector space that separates one class from the other. During the classification, the side of the hyperplane then determines the class. A binary SVM can be extended into multi-class and regression classifiers [<a href="#pone.0155036.ref038" class="ref-tip">38</a>]. For this study we implemented five extensions of the basic SVM; some of them take the sentiment class ordering explicitly into account. All the SVM algorithms, and several others, including Naive Bayes [<a href="#pone.0155036.ref014" class="ref-tip">14</a>], are implemented in the open-source LATINO library [<a href="#pone.0155036.ref039" class="ref-tip">39</a>] (a light-weight set of software components for building text mining applications, available at <a href="https://github.com/latinolib">https://github.com/latinolib</a>).</p> 
          <a id="article1.body1.sec4.sec7.p2" name="article1.body1.sec4.sec7.p2" class="link-target"></a>
          <p><strong>NeutralZoneSVM</strong> is an extension of the basic two-class SVM and assumes that neutral tweets are “between” the negative and positive tweets. The classifier is trained just on the negative and positive tweets. During the classification, the side of the hyperplane determines the sentiment class (negative or positive). However, tweets which are “too close” to the hyperplane are considered neutral. Various realizations of “too close” are described in [<a href="#pone.0155036.ref040" class="ref-tip">40</a>, <a href="#pone.0155036.ref041" class="ref-tip">41</a>].</p> 
          <a id="article1.body1.sec4.sec7.p3" name="article1.body1.sec4.sec7.p3" class="link-target"></a>
          <p><strong>TwoPlaneSVM</strong> assumes the ordering of sentiment classes and implements ordinal classification [<a href="#pone.0155036.ref025" class="ref-tip">25</a>]. It consists of two SVM classifiers: One classifier is trained to separate the negative tweets from the neutral-or-positives; the other separates the negative-or-neutrals from the positives. The result is a classifier with two hyperplanes (nearly parallel for all practical cases) which separates the vector space into three subspaces: negative, neutral, and positive. During classification, the distances from both hyperplanes determine the predicted class.</p> 
          <a id="article1.body1.sec4.sec7.p4" name="article1.body1.sec4.sec7.p4" class="link-target"></a>
          <p><strong>TwoPlaneSVMbin</strong> is a refinement of the TwoPlaneSVM classifier. It partitions the space around both hyperplanes into bins, and computes the distribution of the training examples in individual bins. During classification, the distances from both hyperplanes determine the appropriate bin, but the class is determined as the majority class in the bin. Additionally, the classifier can also provide the confidence of the predicted class.</p> 
          <a id="article1.body1.sec4.sec7.p5" name="article1.body1.sec4.sec7.p5" class="link-target"></a>
          <p><strong>CascadingSVM</strong> also consists of two SVM classifiers, but does not assume that the classes are ordered. Instead, the first classifier separates the neutral tweets (“objective”) from the union of negatives and positives (“subjective”). The second classifier in the cascade then considers only the “subjective” tweets and separates the negatives from positives.</p> 
          <a id="article1.body1.sec4.sec7.p6" name="article1.body1.sec4.sec7.p6" class="link-target"></a>
          <p><strong>ThreePlaneSVM</strong> treats the three sentiment classes as nominal, unordered. It consists of three binary classifiers in the one-vs-one setting: the first separates negatives from neutrals, the second neutrals from positives, and the third negatives from positives. The three independent classifiers partition the vector space into eight subspaces. In analogy to the TwoPlaneSVMbin, the distribution of the training examples in each subspace determines the majority class to be predicted during classification.</p> 
          <a id="article1.body1.sec4.sec7.p7" name="article1.body1.sec4.sec7.p7" class="link-target"></a>
          <p><strong>NaiveBayes</strong> is a well-know supervised machine learning algorithm, and is included here for reference. It is a probabilistic classifier based on the Bayes theorem, and does not assume ordering of the sentiment classes.</p> 
          <a id="article1.body1.sec4.sec7.p8" name="article1.body1.sec4.sec7.p8" class="link-target"></a>
          <p>All the above algorithms were applied to the 13 language datasets and evaluated by 10-fold cross-validation. Standard 10-fold cross-validation randomly partitions the whole labeled set into 10 equal folds. One is set apart for testing, the remaining nine are used to train the model, and the train-test procedure is run over all 10 folds. Cross-validation is <em>stratified</em> when the partitioning is not completely random, but each fold has roughly the same class distribution. With time-ordered data, as is the Twitter stream, one should also consider <em>blocked</em> form of cross-validation [<a href="#pone.0155036.ref042" class="ref-tip">42</a>], where there is no randomization, and each fold is a block of consecutive tweets. There are also other evaluation procedures suitable for time-ordered data, different than cross-validation, like ordered sub-sampling, but this is beyond the scope of the paper. In this study we applied blocked, stratified, 10-fold cross-validation in all the experiments.</p> 
          <a id="article1.body1.sec4.sec7.p9" name="article1.body1.sec4.sec7.p9" class="link-target"></a>
          <p>The Twitter data is first pre-processed by standard text processing methods, i.e., tokenization, stemming/lemmatization (if available for a specific language), unigram and bigram construction, and elimination of terms that do not appear at least 5 times in a dataset. The Twitter specific pre-processing is then applied, i.e, replacing URLs, Twitter usernames and hashtags with common tokens, adding emoticon features for different types of emoticons in tweets, handling of repetitive letters, etc. The feature vectors are constructed by the Delta TF-IDF weighting scheme [<a href="#pone.0155036.ref043" class="ref-tip">43</a>].</p> 
          <a id="article1.body1.sec4.sec7.p10" name="article1.body1.sec4.sec7.p10" class="link-target"></a>
          <p>Evaluation results, in terms of <em>Alpha</em>, are summarized in <a href="#pone-0155036-g011">Fig 11</a>. The classifiers are ordered by their average performance rank across the 13 datasets. More detailed results, in terms of all four evaluation measures, and also including the application datasets, are in <a href="#pone-0155036-t006">Table 6</a>. Note that the sizes of the training datasets are lower than the numbers of annotated tweets in <a href="#pone-0155036-t001">Table 1</a>. Namely, tweets annotated several times are first merged into single training examples, thus forming the “gold standard” for training and testing. If all the annotations are the same, the assigned label is obvious. If the annotations differ, the following merging rules are applied: <em>neutral</em> and <em>negative</em> ? <em>negative</em>; <em>neutral</em> and <em>positive</em> ? <em>positive</em>; and <em>negative</em> and <em>positive</em> ? <em>neutral</em>.</p> 
          <a class="link-target" id="pone-0155036-g011" name="pone-0155036-g011"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g011">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g011" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g011"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g011" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g011"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g011"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g011"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g011"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g011"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g011"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 11. </span> Comparison of six classification models in terms of 
            <em>Alpha</em>.
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec7.fig1.caption1.p1" name="article1.body1.sec4.sec7.fig1.caption1.p1" class="link-target"></a></p>
           <p>Results are the average of 10-fold cross-validations.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g011"> https://doi.org/10.1371/journal.pone.0155036.g011</a></p>
          </div>
          <a class="link-target" id="pone-0155036-t006" name="pone-0155036-t006"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.t006">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.t006" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.t006"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.t006" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t006"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.t006"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t006"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.t006"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t006"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.t006"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Table 6. </span> Evaluation results of the sentiment classifiers.
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec7.table-wrap1.caption1.p1" name="article1.body1.sec4.sec7.table-wrap1.caption1.p1" class="link-target"></a></p>
           <p>The dataset sizes in the second column, used for training the classifiers, are the result of merging multiple annotated tweets (there was no merging for the Facebook(it) and DJIA30 datasets). The 95% confidence intervals for <em>Alpha</em> are estimated from 10-fold cross-validations. The last row is an evaluation of the general English language model (trained from the English dataset in row 3) on the Environment dataset.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.t006"> https://doi.org/10.1371/journal.pone.0155036.t006</a></p>
          </div>
         </div> 
         <div id="section8" class="section toc-section">
          <a id="sec015" name="sec015" class="link-target" title="The Friedman-Nemenyi test"></a> 
          <h3>The Friedman-Nemenyi test</h3> 
          <a id="article1.body1.sec4.sec8.p1" name="article1.body1.sec4.sec8.p1" class="link-target"></a>
          <p>Are there significant differences between the six classifiers, in terms of their performance? The results depend on the evaluation measure used, but generally the top classifiers are not distinguishable.</p> 
          <a id="article1.body1.sec4.sec8.p2" name="article1.body1.sec4.sec8.p2" class="link-target"></a>
          <p>A standard statistical method for testing the significant differences between multiple classifiers [<a href="#pone.0155036.ref044" class="ref-tip">44</a>] is the well-known ANOVA and its non-parametric counterpart, the Friedman test [<a href="#pone.0155036.ref015" class="ref-tip">15</a>]. The Friedman test ranks the classifiers for each dataset separately. The best performing classifier is assigned rank 1, the second best rank 2, etc. When there are ties, average ranks are assigned. The Friedman test then compares the average ranks of the classifiers. The null hypothesis is that all the classifiers are equivalent and so their ranks should be equal. If the null hypothesis is rejected, one proceeds with a post-hoc test.</p> 
          <a id="article1.body1.sec4.sec8.p3" name="article1.body1.sec4.sec8.p3" class="link-target"></a>
          <p>If one wants to compare a control classifier to other classifiers, the Bonferroni-Dunn post-hoc test is used. In our case, however, all the classifiers are compared to each other, and the weaker Nemenyi test [<a href="#pone.0155036.ref016" class="ref-tip">16</a>] is used. The Nemenyi test computes the critical distance between any pair of classifiers. The performance of the two classifiers is significantly different if the corresponding average ranks differ by at least the critical distance.</p> 
          <a id="article1.body1.sec4.sec8.p4" name="article1.body1.sec4.sec8.p4" class="link-target"></a>
          <p> <a href="#pone-0155036-g012">Fig 12</a> gives the results of the Friedman-Nemenyi test for the six classifiers trained in this study. We focus on two evaluation measures that take the ordering of sentiment classes into account: <em>Alpha</em> and <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e027" class="inline-graphic"></span>. There are two classifiers which are in the group of top indistinguishable classifiers in both cases: ThreePlaneSVM (ranked 3rd) and TwoPlaneSVMbin (ranked 4th and 1st). We decided to interpret and discuss all the results in this paper using the TwoPlaneSVMbin classifier, since it is explicitly designed for ordered classes.</p> 
          <a class="link-target" id="pone-0155036-g012" name="pone-0155036-g012"></a>
          <div class="figure" data-doi="10.1371/journal.pone.0155036.g012">
           <div class="img-box">
            <a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0155036.g012" data-doi="info:doi/10.1371/journal.pone.0155036" data-uri="info:doi/10.1371/journal.pone.0155036.g012"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0155036.g012" alt="thumbnail" class="thumbnail"></a>
            <div class="expand"></div>
           </div>
           <div class="figure-inline-download">
             Download: 
            <ul>
             <li>
              <div class="definition-label">
               <a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g012"> PPT </a>
              </div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0155036.g012"> PowerPoint slide </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g012"> PNG </a>
              </div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0155036.g012"> larger image </a></li>
             <li>
              <div class="definition-label">
               <a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g012"> TIFF </a>
              </div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0155036.g012"> original image </a></li>
            </ul>
           </div>
           <div class="figcaption">
            <span>Fig 12. </span> Results of the Friedman-Nemenyi test of classifiers ranking.
           </div>
           <p class="caption_target"><a id="article1.body1.sec4.sec8.fig1.caption1.p1" name="article1.body1.sec4.sec8.fig1.caption1.p1" class="link-target"></a></p>
           <p>The six classifiers are compared in terms of their ranking using two evaluation measures, <em>Alpha</em> (left) and <span class="inline-formula"><img src="article/file?type=thumbnail&amp;id=info:doi/10.1371/journal.pone.0155036.e028" class="inline-graphic"></span> (right). The ranks of classifiers within the critical distance (2.09) are not statistically significantly different.</p> 
           <p></p>
           <p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0155036.g012"> https://doi.org/10.1371/journal.pone.0155036.g012</a></p>
          </div>
         </div> 
        </div> 
        <div class="section toc-section">
         <a id="ack" name="ack" data-toc="ack" title="Acknowledgments" class="link-target"></a>
         <h2>Acknowledgments</h2> 
         <a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1" class="link-target"></a>
         <p>We acknowledge Gama System (<a href="http://www.gama-system.si">http://www.gama-system.si</a>) who collected most of the tweets (except English), and Sowa Labs (<a href="http://www.sowalabs.com">http://www.sowalabs.com</a>) for providing the Goldfinch platform for sentiment annotations. Special thanks go to Sašo Rutar who implemented several classification algorithms and evaluation procedures in the LATINO library for text mining (<a href="https://github.com/latinolib">https://github.com/latinolib</a>). We thank Mojca Mikac for computing the Krippendorff’s <em>Alpha</em> confidence intervals, and Dragi Kocev for help with the Friedman-Nemenyi test.</p> 
        </div>
        <div class="contributions toc-section">
         <a id="authcontrib" name="authcontrib" data-toc="authcontrib" title="Author Contributions"></a>
         <h2>Author Contributions</h2>
         <p>Conceived and designed the experiments: IM JS MG. Performed the experiments: JS IM. Analyzed the data: IM JS. Wrote the paper: IM JS MG.</p>
        </div>
        <div class="toc-section">
         <a id="references" name="references" class="link-target" data-toc="references" title="References"></a>
         <h2>References</h2>
         <ol class="references">
          <li id="ref1"><span class="order">1. </span><a name="pone.0155036.ref001" id="pone.0155036.ref001" class="link-target"></a> Liu B. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies. 2012;5(1):1–167. 
           <ul class="reflinks" data-doi="10.2200/S00416ED1V01Y201204HLT016">
            <li><a href="https://doi.org/10.2200/S00416ED1V01Y201204HLT016" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Liu%5Bauthor%5D+AND+Sentiment+analysis+and+opinion+mining" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Sentiment+analysis+and+opinion+mining&amp;author=Liu&amp;publication_year=2012" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref2"><span class="order">2. </span><a name="pone.0155036.ref002" id="pone.0155036.ref002" class="link-target"></a> Liu B. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions. Cambridge University Press; 2015. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref3"><span class="order">3. </span><a name="pone.0155036.ref003" id="pone.0155036.ref003" class="link-target"></a> Dodds PS, Clark EM, Desu S, Frank MR, Reagan AJ, Williams JR, et al. Human language reveals a universal positivity bias. Proceedings of the National Academy of Sciences. 2015;112(8):2389–2394. Available from: <a href="http://dx.doi.org/10.1073/pnas.1411678112">http://dx.doi.org/10.1073/pnas.1411678112</a>. 
           <ul class="reflinks" data-doi="http://dx.doi.org/10.1073/pnas.1411678112">
            <li><a href="https://doi.org/http://dx.doi.org/10.1073/pnas.1411678112" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Dodds%5Bauthor%5D+AND+Human+language+reveals+a+universal+positivity+bias" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Human+language+reveals+a+universal+positivity+bias&amp;author=Dodds&amp;publication_year=2015" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref4"><span class="order">4. </span><a name="pone.0155036.ref004" id="pone.0155036.ref004" class="link-target"></a> Baccianella S, Esuli A, Sebastiani F. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In: LREC. vol. 10; 2010. p. 2200–2204. 
           <ul class="reflinks">
            <li><a href="#" data-author="Baccianella" data-cit="%0ABaccianellaS%2C%20EsuliA%2C%20SebastianiF.%20SentiWordNet%203.0%3A%20An%20enhanced%20lexical%20resource%20for%20sentiment%20analysis%20and%20opinion%20mining.%20In%3A%20LREC.%20vol.%2010%3B%202010.%20p.%202200%E2%80%932204." data-title="SentiWordNet%203.0%3A%20An%20enhanced%20lexical%20resource%20for%20sentiment%20analysis%20and%20opinion%20mining" target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Baccianella%5Bauthor%5D+AND+SentiWordNet+3.0%3A+An+enhanced+lexical+resource+for+sentiment+analysis+and+opinion+mining" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=SentiWordNet+3.0%3A+An+enhanced+lexical+resource+for+sentiment+analysis+and+opinion+mining&amp;author=Baccianella&amp;publication_year=2010" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref5"><span class="order">5. </span><a name="pone.0155036.ref005" id="pone.0155036.ref005" class="link-target"></a> Martínez-Cámara E, Martín-Valdivia MT, Urena-López LA, Montejo-Ráez AR. Sentiment analysis in Twitter. Natural Language Engineering. 2014;20(1):1–28. 
           <ul class="reflinks">
            <li><a href="#" data-author="Mart%C3%ADnez-C%C3%A1mara" data-cit="%0AMart%C3%ADnez-C%C3%A1maraE%2C%20Mart%C3%ADn-ValdiviaMT%2C%20Urena-L%C3%B3pezLA%2C%20Montejo-R%C3%A1ezAR.%20Sentiment%20analysis%20in%20Twitter.%20Natural%20Language%20Engineering.%202014%3B20%281%29%3A1%E2%80%9328." data-title="Sentiment%20analysis%20in%20Twitter" target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Mart%C3%ADnez-C%C3%A1mara%5Bauthor%5D+AND+Sentiment+analysis+in+Twitter" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Sentiment+analysis+in+Twitter&amp;author=Mart%C3%ADnez-C%C3%A1mara&amp;publication_year=2014" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref6"><span class="order">6. </span><a name="pone.0155036.ref006" id="pone.0155036.ref006" class="link-target"></a> Kolchyna, O, Souza, TTP, Treleaven, PC, Aste, T. Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination. arXiv preprint arXiv:150700955. 2015;. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref7"><span class="order">7. </span><a name="pone.0155036.ref007" id="pone.0155036.ref007" class="link-target"></a> Krippendorff K. Content Analysis, An Introduction to Its Methodology. 3rd ed. Thousand Oaks, CA: Sage Publications; 2012. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref8"><span class="order">8. </span><a name="pone.0155036.ref008" id="pone.0155036.ref008" class="link-target"></a> Zollo F, Kralj Novak P, Del Vicario M, Bessi A, Mozeti? I, Scala A, et al. Emotional dynamics in the age of misinformation. PLoS ONE. 2015;10(9):e0138740. Available from: <a href="http://dx.doi.org/10.1371/journal.pone.0138740">http://dx.doi.org/10.1371/journal.pone.0138740</a>. pmid:26422473 
           <ul class="reflinks" data-doi="http://dx.doi.org/10.1371/journal.pone.0138740">
            <li><a href="https://doi.org/http://dx.doi.org/10.1371/journal.pone.0138740" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Zollo%5Bauthor%5D+AND+Emotional+dynamics+in+the+age+of+misinformation" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Emotional+dynamics+in+the+age+of+misinformation&amp;author=Zollo&amp;publication_year=2015" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref9"><span class="order">9. </span><a name="pone.0155036.ref009" id="pone.0155036.ref009" class="link-target"></a> Ranco G, Aleksovski D, Caldarelli G, Gr?ar M, Mozeti? I. The effects of Twitter sentiment on stock price returns. PLoS ONE. 2015;10(9):e0138441. Available from: <a href="http://dx.doi.org/10.1371/journal.pone.0138441">http://dx.doi.org/10.1371/journal.pone.0138441</a>. pmid:26390434 
           <ul class="reflinks" data-doi="http://dx.doi.org/10.1371/journal.pone.0138441">
            <li><a href="https://doi.org/http://dx.doi.org/10.1371/journal.pone.0138441" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Ranco%5Bauthor%5D+AND+The+effects+of+Twitter+sentiment+on+stock+price+returns" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=The+effects+of+Twitter+sentiment+on+stock+price+returns&amp;author=Ranco&amp;publication_year=2015" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref10"><span class="order">10. </span><a name="pone.0155036.ref010" id="pone.0155036.ref010" class="link-target"></a> Sluban B, Smailovi? J, Battiston S, Mozeti? I. Sentiment leaning of influential communities in social networks. Computational Social Networks. 2015;2(9):1–21. Available from: <a href="http://dx.doi.org/10.1186/s40649-015-0016-5">http://dx.doi.org/10.1186/s40649-015-0016-5</a>. 
           <ul class="reflinks" data-doi="http://dx.doi.org/10.1186/s40649-015-0016-5">
            <li><a href="https://doi.org/http://dx.doi.org/10.1186/s40649-015-0016-5" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sluban%5Bauthor%5D+AND+Sentiment+leaning+of+influential+communities+in+social+networks" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Sentiment+leaning+of+influential+communities+in+social+networks&amp;author=Sluban&amp;publication_year=2015" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref11"><span class="order">11. </span><a name="pone.0155036.ref011" id="pone.0155036.ref011" class="link-target"></a> Kralj Novak P, Smailovi? J, Sluban B, Mozeti? I. Sentiment of emojis. PLoS ONE. 2015;10(12):e0144296. Available from: <a href="http://dx.doi.org/10.1186/s40649-015-0016-5">http://dx.doi.org/10.1186/s40649-015-0016-5</a>. pmid:26641093 
           <ul class="reflinks" data-doi="http://dx.doi.org/10.1186/s40649-015-0016-5">
            <li><a href="https://doi.org/http://dx.doi.org/10.1186/s40649-015-0016-5" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Kralj+Novak%5Bauthor%5D+AND+Sentiment+of+emojis" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Sentiment+of+emojis&amp;author=Kralj+Novak&amp;publication_year=2015" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref12"><span class="order">12. </span><a name="pone.0155036.ref012" id="pone.0155036.ref012" class="link-target"></a> Mooney CZ, Duval RD. Bootstrapping: A Nonparametric Approach to Statistical Inference. Thousand Oaks, CA: Sage Publications; 1993. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref13"><span class="order">13. </span><a name="pone.0155036.ref013" id="pone.0155036.ref013" class="link-target"></a> Vapnik VN. The Nature of Statistical Learning Theory. Springer; 1995. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref14"><span class="order">14. </span><a name="pone.0155036.ref014" id="pone.0155036.ref014" class="link-target"></a> Russell S, Norvig P. Artificial Intelligence: A Modern Approach. 2nd ed. Prentice Hall; 2003. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref15"><span class="order">15. </span><a name="pone.0155036.ref015" id="pone.0155036.ref015" class="link-target"></a> Friedman M. A comparison of alternative tests of significance for the problem of m rankings. Annals of Mathematical Statistics. 1940;11:86–92. 
           <ul class="reflinks">
            <li><a href="#" data-author="Friedman" data-cit="%0AFriedmanM.%20A%20comparison%20of%20alternative%20tests%20of%20significance%20for%20the%20problem%20of%20m%20rankings.%20Annals%20of%20Mathematical%20Statistics.%201940%3B11%3A86%E2%80%9392." data-title="A%20comparison%20of%20alternative%20tests%20of%20significance%20for%20the%20problem%20of%20m%20rankings" target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Friedman%5Bauthor%5D+AND+A+comparison+of+alternative+tests+of+significance+for+the+problem+of+m+rankings" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=A+comparison+of+alternative+tests+of+significance+for+the+problem+of+m+rankings&amp;author=Friedman&amp;publication_year=1940" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref16"><span class="order">16. </span><a name="pone.0155036.ref016" id="pone.0155036.ref016" class="link-target"></a> Nemenyi PB. Distribution-free multiple comparisons. Princeton University; 1963. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref17"><span class="order">17. </span><a name="pone.0155036.ref017" id="pone.0155036.ref017" class="link-target"></a>Saif H, Fernández M, He Y, Alani H. Evaluation datasets for Twitter sentiment analysis: A survey and a new dataset, the STS-Gold. In: 1st Intl. Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI (ESSEM); 2013. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref18"><span class="order">18. </span><a name="pone.0155036.ref018" id="pone.0155036.ref018" class="link-target"></a>dos Santos CN, Gatti M. Deep convolutional neural networks for sentiment analysis of short texts. In: Proc. 25th Intl. Conf. on Computational Linguistics (COLING), Dublin, Ireland; 2014. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref19"><span class="order">19. </span><a name="pone.0155036.ref019" id="pone.0155036.ref019" class="link-target"></a>dos Santos CN. Think positive: Towards Twitter sentiment analysis from scratch. In: Proc. 8th Intl. Workshop on Semantic Evaluation (SemEval); 2014. p. 647–651. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref20"><span class="order">20. </span><a name="pone.0155036.ref020" id="pone.0155036.ref020" class="link-target"></a>Tang D, Wei F, Qin B, Liu T, Zhou M. Coooolll: A deep learning system for Twitter sentiment classification. In: Proc. 8th Intl. Workshop on Semantic Evaluation (SemEval); 2014. p. 208–212. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref21"><span class="order">21. </span><a name="pone.0155036.ref021" id="pone.0155036.ref021" class="link-target"></a>Ghosh A, Li G, Veale T, Rosso P, Shutova E, Barnden J, et al. SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter. In: Proc. 9th Intl. Workshop on Semantic Evaluation (SemEval); 2015. p. 470–478. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref22"><span class="order">22. </span><a name="pone.0155036.ref022" id="pone.0155036.ref022" class="link-target"></a> Mohammad SM, Kiritchenko S. Using hashtags to capture fine emotion categories from tweets. Computational Intelligence. 2015;31(2):301–326. 
           <ul class="reflinks" data-doi="10.1111/coin.12024">
            <li><a href="https://doi.org/10.1111/coin.12024" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Mohammad%5Bauthor%5D+AND+Using+hashtags+to+capture+fine+emotion+categories+from+tweets" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Using+hashtags+to+capture+fine+emotion+categories+from+tweets&amp;author=Mohammad&amp;publication_year=2015" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref23"><span class="order">23. </span><a name="pone.0155036.ref023" id="pone.0155036.ref023" class="link-target"></a> Van Rijsbergen CJ. Information Retrieval. Butterworth; 1979. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref24"><span class="order">24. </span><a name="pone.0155036.ref024" id="pone.0155036.ref024" class="link-target"></a> Kiritchenko S, Zhu X, Mohammad SM. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research. 2014;p. 723–762. 
           <ul class="reflinks">
            <li><a href="#" data-author="Kiritchenko" data-cit="%0AKiritchenkoS%2C%20ZhuX%2C%20MohammadSM.%20Sentiment%20analysis%20of%20short%20informal%20texts.%20Journal%20of%20Artificial%20Intelligence%20Research.%202014%3Bp.%20723%E2%80%93762." data-title="Sentiment%20analysis%20of%20short%20informal%20texts" target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Kiritchenko%5Bauthor%5D+AND+Sentiment+analysis+of+short+informal+texts" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Sentiment+analysis+of+short+informal+texts&amp;author=Kiritchenko&amp;publication_year=2014" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref25"><span class="order">25. </span><a name="pone.0155036.ref025" id="pone.0155036.ref025" class="link-target"></a> Gaudette L, Japkowicz N. Evaluation methods for ordinal classification. In: Advances in Artificial Intelligence. Springer; 2009. p. 207–210. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref26"><span class="order">26. </span><a name="pone.0155036.ref026" id="pone.0155036.ref026" class="link-target"></a>Go A, Bhayani R, Huang L. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford. 2009;p. 1–12. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref27"><span class="order">27. </span><a name="pone.0155036.ref027" id="pone.0155036.ref027" class="link-target"></a>Rosenthal S, Nakov P, Kiritchenko S, Mohammad SM, Ritter A, Stoyanov V. SemEval-2015 task 10: Sentiment Analysis in Twitter. In: Proc. 9th Intl. Workshop on Semantic Evaluation (SemEval); 2015. p. 451–463. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref28"><span class="order">28. </span><a name="pone.0155036.ref028" id="pone.0155036.ref028" class="link-target"></a>Haldenwang N, Vornberger O. Sentiment Uncertainty and Spam in Twitter Streams and Its Implications for General Purpose Realtime Sentiment Analysis. In: Proc. Intl. Conf. of the German Society for Computational Linguistics and Language Technology (GSCL); 2015. p. 157–159. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref29"><span class="order">29. </span><a name="pone.0155036.ref029" id="pone.0155036.ref029" class="link-target"></a> Saif H, He Y, Fernandez M, Alani H. Semantic patterns for sentiment analysis of Twitter. In: The Semantic Web (ISWC). Springer; 2014. p. 324–340. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref30"><span class="order">30. </span><a name="pone.0155036.ref030" id="pone.0155036.ref030" class="link-target"></a>Parikh R, Movassate M. Sentiment analysis of user-generated Twitter updates using various classification techniques. CS224N Final Report. 2009;p. 1–18. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref31"><span class="order">31. </span><a name="pone.0155036.ref031" id="pone.0155036.ref031" class="link-target"></a> Pak A, Paroubek P. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In: LREC. vol. 10; 2010. p. 1320–1326. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref32"><span class="order">32. </span><a name="pone.0155036.ref032" id="pone.0155036.ref032" class="link-target"></a>Asiaee T A, Tepper M, Banerjee A, Sapiro G. If you are happy and you know it… tweet. In: Proc. 21st ACM Intl. Conf. on Information and Knowledge Management. ACM; 2012. p. 1602–1606. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref33"><span class="order">33. </span><a name="pone.0155036.ref033" id="pone.0155036.ref033" class="link-target"></a> Saif H, He Y, Alani H. Semantic sentiment analysis of Twitter. In: The Semantic Web (ISWC). vol. 7649 of Lecture Notes in Computer Science. Springer; 2012. p. 508–524. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref34"><span class="order">34. </span><a name="pone.0155036.ref034" id="pone.0155036.ref034" class="link-target"></a>Barbosa L, Feng J. Robust sentiment detection on Twitter from biased and noisy data. In: Proc. 23rd Intl. Conf. on Computational Linguistics: Posters. ACL; 2010. p. 36–44. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref35"><span class="order">35. </span><a name="pone.0155036.ref035" id="pone.0155036.ref035" class="link-target"></a>Jiang L, Yu M, Zhou M, Liu X, Zhao T. Target-dependent Twitter sentiment classification. In: Proc. 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. ACL; 2011. p. 151–160. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref36"><span class="order">36. </span><a name="pone.0155036.ref036" id="pone.0155036.ref036" class="link-target"></a>Davidov D, Tsur O, Rappoport A. Enhanced sentiment learning using Twitter hashtags and smileys. In: Proc. 23rd Intl. Conf. Computational Linguistics: Posters. ACL; 2010. p. 241–249. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref37"><span class="order">37. </span><a name="pone.0155036.ref037" id="pone.0155036.ref037" class="link-target"></a>Kouloumpis E, Wilson T, Moore J. Twitter sentiment analysis: The good the bad and the OMG! In: Proc. Intl. Conf. on Weblogs and Social Media (ICWSM). vol. 11; 2011. p. 538–541. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref38"><span class="order">38. </span><a name="pone.0155036.ref038" id="pone.0155036.ref038" class="link-target"></a> Hsu C, Lin C. A comparison of methods for multiclass Support Vector Machines. IEEE Transactions on Neural Networks. 2002;13:415–425. 
           <ul class="reflinks">
            <li><a href="#" data-author="Hsu" data-cit="%0AHsuC%2C%20LinC.%20A%20comparison%20of%20methods%20for%20multiclass%20Support%20Vector%20Machines.%20IEEE%20Transactions%20on%20Neural%20Networks.%202002%3B13%3A415%E2%80%93425." data-title="A%20comparison%20of%20methods%20for%20multiclass%20Support%20Vector%20Machines" target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Hsu%5Bauthor%5D+AND+A+comparison+of+methods+for+multiclass+Support+Vector+Machines" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=A+comparison+of+methods+for+multiclass+Support+Vector+Machines&amp;author=Hsu&amp;publication_year=2002" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref39"><span class="order">39. </span><a name="pone.0155036.ref039" id="pone.0155036.ref039" class="link-target"></a>Gr?ar M. Mining text-enriched heterogeneous information networks. PhD thesis, Jožef Stefan International Postgraduate School, Ljubljana, Slovenia; 2015. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref40"><span class="order">40. </span><a name="pone.0155036.ref040" id="pone.0155036.ref040" class="link-target"></a>Smailovi? J. Sentiment analysis in streams of microblogging posts. PhD thesis, Jožef Stefan International Postgraduate School, Ljubljana, Slovenia; 2014. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref41"><span class="order">41. </span><a name="pone.0155036.ref041" id="pone.0155036.ref041" class="link-target"></a>Smailovi? J, Kranjc J, Gr?ar M, Žnidarši? M, Mozeti? I. Monitoring the Twitter sentiment during the Bulgarian elections. In: Proc. IEEE Intl. Conf. on Data Science and Advanced Analytics. IEEE; 2015. p. 1–10. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref42"><span class="order">42. </span><a name="pone.0155036.ref042" id="pone.0155036.ref042" class="link-target"></a> Bergmeir C, Benítez JM. On the Use of Cross-validation for Time Series Predictor Evaluation. Information Sciences. 2012;191:192–213. Available from: <a href="http://dx.doi.org/10.1016/j.ins.2011.12.028">http://dx.doi.org/10.1016/j.ins.2011.12.028</a>. 
           <ul class="reflinks" data-doi="http://dx.doi.org/10.1016/j.ins.2011.12.028">
            <li><a href="https://doi.org/http://dx.doi.org/10.1016/j.ins.2011.12.028" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Bergmeir%5Bauthor%5D+AND+On+the+Use+of+Cross-validation+for+Time+Series+Predictor+Evaluation" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=On+the+Use+of+Cross-validation+for+Time+Series+Predictor+Evaluation&amp;author=Bergmeir&amp;publication_year=2012" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
          <li id="ref43"><span class="order">43. </span><a name="pone.0155036.ref043" id="pone.0155036.ref043" class="link-target"></a>Martineau J, Finin T. Delta TFIDF: An improved feature space for sentiment analysis. In: Proc. 3rd AAAI Intl. Conf. on Weblogs and Social Media (ICWSM); 2009. p. 258–261. 
           <ul class="find-nolinks"></ul></li>
          <li id="ref44"><span class="order">44. </span><a name="pone.0155036.ref044" id="pone.0155036.ref044" class="link-target"></a> Demšar J. Statistical comparison of classifiers over multiple data sets. Journal of Machine Learning Research. 2006;7:1–30. 
           <ul class="reflinks">
            <li><a href="#" data-author="Dem%C5%A1ar" data-cit="%0ADem%C5%A1arJ.%20Statistical%20comparison%20of%20classifiers%20over%20multiple%20data%20sets.%20Journal%20of%20Machine%20Learning%20Research.%202006%3B7%3A1%E2%80%9330." data-title="Statistical%20comparison%20of%20classifiers%20over%20multiple%20data%20sets" target="_new" title="Go to article in CrossRef"> View Article </a></li>
            <li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Dem%C5%A1ar%5Bauthor%5D+AND+Statistical+comparison+of+classifiers+over+multiple+data+sets" target="_new" title="Go to article in PubMed"> PubMed/NCBI </a></li>
            <li><a href="http://scholar.google.com/scholar_lookup?title=Statistical+comparison+of+classifiers+over+multiple+data+sets&amp;author=Dem%C5%A1ar&amp;publication_year=2006" target="_new" title="Go to article in Google Scholar"> Google Scholar </a></li>
           </ul></li>
         </ol>
        </div> 
        <div class="ref-tooltip"> 
         <div class="ref_tooltip-content"> 
         </div> 
        </div> 
       </div> 
      </div> 
     </div> 
    </section> 
    <aside class="article-aside"> 
     <!--[if IE 9]>
<style>
.dload-xml {margin-top: 38px}
</style>
<![endif]--> 
     <div class="dload-menu"> 
      <div class="dload-pdf"> 
       <a href="/plosone/article/file?id=10.1371/journal.pone.0155036&amp;type=printable" id="downloadPdf" target="_blank">Download PDF</a> 
      </div> 
      <div data-js-tooltip-hover="trigger" class="dload-hover">
       &nbsp; 
       <ul class="dload-xml" data-js-tooltip-hover="target"> 
        <li><a href="/plosone/article/citation?id=10.1371/journal.pone.0155036" id="downloadCitation">Citation</a></li> 
        <li><a href="/plosone/article/file?id=10.1371/journal.pone.0155036&amp;type=manuscript" id="downloadXml">XML</a> </li> 
       </ul> 
      </div> 
     </div> 
     <div class="aside-container"> 
      <div class="print-article" id="printArticle" data-js-tooltip-hover="trigger">
        Print 
       <ul class="print-options" data-js-tooltip-hover="target"> 
        <li> <a href="#" onclick="window.print(); return false;" class="preventDefault" id="printBrowser" title="Print
        Article">Print article</a> </li> 
        <li> <a title="Odyssey Press" href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&amp;page=0&amp;journal=7&amp;doi=10.1371%2Fjournal.pone.0155036&amp;volume=&amp;issue=&amp;title=Multilingual%20Twitter%20Sentiment%20Classification%3A%20The%20Role%20of%20Human%20Annotators&amp;author_name=Igor%20Mozeti%C4%8D%2C%20Miha%20Gr%C4%8Dar%2C%20Jasmina%20Smailovi%C4%87&amp;start_page=1&amp;end_page=26">EzReprint </a> </li> 
       </ul> 
      </div> 
      <div class="share-article" id="shareArticle" data-js-tooltip-hover="trigger">
        Share 
       <ul data-js-tooltip-hover="target" class="share-options"> 
        <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036" id="shareReddit" target="_blank" title="Submit to Reddit"><img src="/plosone/resource/img/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li> 
        <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036" id="shareGoogle" target="_blank" title="Share on Google+"><img src="/plosone/resource/img/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li> 
        <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036" id="shareStumble" target="_blank" title="Add to StumbleUpon"><img src="/plosone/resource/img/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li> 
        <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036&amp;t=Multilingual Twitter Sentiment Classification: The Role of Human Annotators" id="shareFacebook" target="_blank" title="Share on Facebook"><img src="/plosone/resource/img/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li> 
        <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036&amp;title=Multilingual Twitter Sentiment Classification: The Role of Human Annotators&amp;summary=Checkout this article I found at PLOS" id="shareLinkedIn" target="_blank" title="Add to LinkedIn"><img src="/plosone/resource/img/icon.linkedin.16.png" width="16" height="16" alt="LinkedIn">LinkedIn</a></li> 
        <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036&amp;title=Multilingual Twitter Sentiment Classification: The Role of Human Annotators" id="shareCiteULike" target="_blank" title="Add to CiteULike"><img src="/plosone/resource/img/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li> 
        <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036" id="shareMendeley" target="_blank" title="Add to Mendeley"><img src="/plosone/resource/img/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li> 
        <li><a href="https://www.pubchase.com/library?add_aid=10.1371/journal.pone.0155036&amp;source=plos" id="sharePubChase" target="_blank" title="Add to PubChase"><img src="/plosone/resource/img/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li> 
        <li><a href="http://twitter.com/intent/tweet?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0155036&amp;text=%23PLOSONE%3A%20Multilingual Twitter Sentiment Classification: The Role of Human Annotators" target="_blank" title="share on Twitter" id="twitter-share-link"><img src="/plosone/resource/img/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li> 
        <li><a href="/plosone/article/email?id=10.1371/journal.pone.0155036" id="shareEmail" title="Email this article"><img src="/plosone/resource/img/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li> 
       </ul> 
      </div>
     </div> ?
     <!-- Crossmark 2.0 widget --> 
     <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script> 
     <a data-target="crossmark"><img width="150" src="http://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_BW_horizontal.svg"></a> 
     <!-- End Crossmark 2.0 widget --> 
     <div class="skyscraper-container"> 
      <div class="title">
       Advertisement
      </div> 
      <!-- DoubleClick Ad Zone --> 
      <div class="advertisement" id="div-gpt-ad-1458247671871-1" style="width:160px; height:600px;"> 
       <script type="text/javascript">
      googletag.cmd.push(function() { googletag.display('div-gpt-ad-1458247671871-1'); });
    </script> 
      </div> 
     </div> 
     <div class="subject-areas-container"> 
      <h3>Subject Areas 
       <div id="subjInfo">
        ?
       </div> 
       <div id="subjInfoText"> 
        <p>For more information about PLOS Subject Areas, click <a href="/plosone/s/help-using-this-site#loc-subject-areas">here</a>.</p> 
        <span class="inline-intro">We want your feedback.</span> Do these Subject Areas make sense for this article? Click the target next to the incorrect Subject Area and let us know. Thanks for your help! 
       </div> </h3> 
      <ul id="subjectList"> 
       <li> <a class="taxo-term" title="Search for articles about Twitter" href="/plosone/search?filterSubjects=Twitter&amp;filterJournals=PLoSONE&amp;q=">Twitter</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Twitter">
         <p class="taxo-explain">Is the Subject Area <strong>"Twitter"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Support vector machines" href="/plosone/search?filterSubjects=Support+vector+machines&amp;filterJournals=PLoSONE&amp;q=">Support vector machines</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Support vector machines">
         <p class="taxo-explain">Is the Subject Area <strong>"Support vector machines"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Machine learning algorithms" href="/plosone/search?filterSubjects=Machine+learning+algorithms&amp;filterJournals=PLoSONE&amp;q=">Machine learning algorithms</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Machine learning algorithms">
         <p class="taxo-explain">Is the Subject Area <strong>"Machine learning algorithms"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Slovenian people" href="/plosone/search?filterSubjects=Slovenian+people&amp;filterJournals=PLoSONE&amp;q=">Slovenian people</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Slovenian people">
         <p class="taxo-explain">Is the Subject Area <strong>"Slovenian people"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Albanian people" href="/plosone/search?filterSubjects=Albanian+people&amp;filterJournals=PLoSONE&amp;q=">Albanian people</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Albanian people">
         <p class="taxo-explain">Is the Subject Area <strong>"Albanian people"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Machine learning" href="/plosone/search?filterSubjects=Machine+learning&amp;filterJournals=PLoSONE&amp;q=">Machine learning</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Machine learning">
         <p class="taxo-explain">Is the Subject Area <strong>"Machine learning"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Bulgarian people" href="/plosone/search?filterSubjects=Bulgarian+people&amp;filterJournals=PLoSONE&amp;q=">Bulgarian people</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Bulgarian people">
         <p class="taxo-explain">Is the Subject Area <strong>"Bulgarian people"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
       <li> <a class="taxo-term" title="Search for articles about Serbian people" href="/plosone/search?filterSubjects=Serbian+people&amp;filterJournals=PLoSONE&amp;q=">Serbian people</a> <span class="taxo-flag">&nbsp;</span> 
        <div class="taxo-tooltip" data-categoryname="Serbian people">
         <p class="taxo-explain">Is the Subject Area <strong>"Serbian people"</strong> applicable to this article? <button id="noFlag" data-action="remove">Yes</button> <button id="flagIt" value="flagno" data-action="add">No</button></p> 
         <p class="taxo-confirm">Thanks for your feedback.</p> 
        </div> </li> 
      </ul> 
     </div> 
     <div id="subjectErrors"></div> 
     <div class="twitter-container"> 
      <h3>Archived Tweets</h3> 
      <ul id="tweetList"> 
      </ul> 
      <div class="load-more">
       Load more 
       <span></span>
      </div> 
      <div class="view-all">
       <a href="http://alm.plos.org/works/doi.org/10.1371/journal.pone.0155036?source_id=twitter">View all tweets</a> 
      </div> 
     </div> 
     <script type="text/template" id="twitterModuleItemTemplate">
  <% _.each(items, function(item) { %>
    <li>
      <div class="tweet-info">
        <a href="http://twitter.com/<%= item.user %>">
          <span class="imgholder">
            <img class="imgLoad" src="<%= item.user_profile_image %>">
          </span>
          <div class="tweetDate"><%= item.created_at %></div>
          <div class="tweetUser">
            <strong><%= item.user_name %></strong>
            <span>@<%= item.user %></span>
          </div>
        </a>
      </div>
      <div class="tweetText">
        <%= item.text %>
      </div>
      <div id="tweetActions">
        <a class="tweet-reply" href="https://twitter.com/intent/tweet?in_reply_to<%= item.id %>&amp;text=@<%= item.user %>">
          <div>&nbsp;</div> Reply
        </a>
        <a class="tweet-retweet" href="https://twitter.com/intent/retweet?tweet_id=<%= item.id %>">
          <div>&nbsp;</div> Retweet
        </a>
        <a class="tweet-favorite" href="https://twitter.com/intent/favorite?tweet_id=<%= item.id %>">
          <div>&nbsp;</div> Favorite
        </a>
      </div>
    </li>
  <% }); %>
</script> 
    </aside> 
   </div> 
  </main> 
  <footer id="pageftr"> 
   <div class="row"> 
    <div class="brand-column"> 
     <img src="/plosone/resource/img/logo-plos-footer.png" alt="PLOS" class="logo-footer"> 
     <p class="nav-special"> &nbsp; 
      <!--
  Webapp build:  3.3.5 at 20170725 by teamcity, commit: 
  Service build: 2.2.3 at 20170626 by teamcity, commit: 
  Enabled dev features: []
  --> </p> 
     <ul class="nav-aux"> 
      <li><a href="https://www.plos.org/privacy-policy" id="ftr-privacy">Privacy Policy</a></li> 
      <li><a href="https://www.plos.org/terms-of-use" id="ftr-terms">Terms of Use</a></li> 
      <li><a href="https://www.plos.org/advertise/" id="ftr-advertise">Advertise</a></li> 
      <li><a href="https://www.plos.org/media-inquiries" id="ftr-media">Media Inquiries</a></li> 
     </ul> 
    </div> 
    <div class="link-column"> 
     <p class="nav-special"><a href="https://www.plos.org/publications/journals/">Publications</a></p> 
     <div class="nav"> 
      <ul> 
       <li><a href="/plosbiology/" id="ftr-bio">PLOS Biology</a></li> 
       <li><a href="/plosmedicine/" id="ftr-med">PLOS Medicine</a></li> 
       <li><a href="/ploscompbiol/" id="ftr-compbio">PLOS Computational Biology</a></li> 
       <li><a href="http://currents.plos.org" id="ftr-cur">PLOS Currents</a></li> 
       <li><a href="/plosgenetics/" id="ftr-gen">PLOS Genetics</a></li> 
       <li><a href="/plospathogens/" id="ftr-path">PLOS Pathogens</a></li> 
       <li><a href="/plosone/" id="ftr-one">PLOS ONE</a></li> 
       <li><a href="/plosntds/" id="ftr-ntds">PLOS Neglected Tropical Diseases</a></li> 
      </ul> 
     </div> 
    </div> 
    <div class="link-column"> 
     <div class="nav nav-special"> 
      <p><a href="https://www.plos.org" id="ftr-home">plos.org</a></p> 
      <p><a href="http://blogs.plos.org" id="ftr-blog">Blogs</a></p> 
      <p><a href="http://collections.plos.org" id="ftr-collections">Collections</a></p> 
      <p><a href="/plosone/feedback" id="ftr-feedback">Send us feedback</a></p> 
      <p><a href="/plosone/s/help-using-this-site" id="ftr-help">Help using this site</a></p> 
      <p><a href="/plosone/lockss-manifest" id="ftr-lockss">LOCKSS</a></p> 
      <p class="footer-non-profit-statement">PLOS is a nonprofit 501(c)(3) corporation, #C2354500, and is based in San Francisco, California, US</p>
     </div> 
    </div> 
   </div> 
  </footer> 
  <script type="text/javascript">
  var ArticleData = {
    doi: '10.1371/journal.pone.0155036',
    title: '<article-title xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Multilingual Twitter Sentiment Classification: The Role of Human Annotators<\/article-title>',
    date: 'May 05, 2016'
  };
</script> 
  <script type="text/javascript" async src="//platform.twitter.com/widgets.js"></script> 
  <!-- This file should be loaded before the renderJs, to avoid conflicts with the FigShare, that implements the MathJax also. --> 
  <!--  mathjax configuration options  --> 
  <!-- more can be found at http://docs.mathjax.org/en/latest/ --> 
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  "HTML-CSS": {
    scale: 100,
    availableFonts: ["STIX","TeX"],
    preferredFont: "STIX",
    webFont: "STIX-Web",
    linebreaks: { automatic: false }
  },
  jax: ["input/MathML", "output/HTML-CSS"]
});
</script> 
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=MML_HTMLorMML"></script> 
  <script src="/plosone/resource/compiled/asset_V4KEKJMAKU34D33A4HSQGT4JUZQJDCQS.js"></script> 
  <div class="reveal-modal-bg"></div>   
 </body>
</html>