<!doctype html>
<html>
 <head> 
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes"> 
  <meta name="theme-color" content="#4F7DC9"> 
  <meta charset="UTF-8"> 
  <title>Google Cloud Speech API : Node.js example</title> 
  <script src="../../bower_components/webcomponentsjs/webcomponents-lite.min.js"></script> 
  <link rel="import" href="../../elements/codelab.html"> 
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono"> 
  <style>
    body {
      font-family: "Roboto",sans-serif;
    }
  </style> 
 </head> 
 <body unresolved class="fullbleed"> 
  <google-codelab title="Google Cloud Speech API : Node.js example" environment="web" feedback-link="https://github.com/googlecodelabs/feedback/issues/new?title=[cloud-speech-nodejs]:&amp;labels[]=content-platform&amp;labels[]=cloud"> 
   <google-codelab-step label="Overview" duration="1"> 
    <p>In this lab you will run a Node.js instance to send streaming and non streaming requests to Speech API to transcribe recorded audio</p> 
    <h2>What you need</h2> 
    <p>To complete this lab, you need:</p> 
    <ul> 
     <li>MacOS or Linux laptop with microphone and admin/install privileges [preferred]</li> 
     <li>A project on Google Cloud Platform</li> 
    </ul> 
    <h2>What you learn</h2> 
    <p>In this lab, you:</p> 
    <ul> 
     <li>Configure environment settings for your Cloud Console Project and authentication</li> 
     <li>Run Node.js sample to send requests to Speech API for speech recognition</li> 
    </ul> 
   </google-codelab-step> 
   <google-codelab-step label="Setup" duration="5"> 
    <h2>Step 1</h2> 
    <p>If necessary, install the following software:</p> 
    <ul> 
     <li><a href="https://git-scm.com/" target="_blank">Download and install git.</a></li> 
     <li><a href="https://cloud.google.com/sdk/docs/" target="_blank">Download Google Cloud SDK</a>.</li> 
     <li><a href="https://nodejs.org/" target="_blank">Download and install Node.js</a>.</li> 
    </ul> 
    <p>If you don't have a Mac or Linux laptop with admin privileges, create a Compute Engine instance on Google Cloud Platform and install the above software there (the gcloud SDK will already be present). In this case, you will not be able to test out the microphone aspect of this lab.</p> 
    <h2>Step 2</h2> 
    <p>Install brew with the following command at the command prompt:</p> 
    <pre>/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
</pre> 
    <h2>Step 3</h2> 
    <p>Install sox (utility to play and record audio and convert audio formats):</p> 
    <pre>brew install sox
</pre> 
    <h2>Step 4</h2> 
    <p>Clone the Node.js samples repository</p> 
    <pre>git clone https://github.com/GoogleCloudPlatform/nodejs-docs-samples.git
cd nodejs-docs-samples</pre> 
    <h2>Step 5</h2> 
    <p>Set cloud console project </p> 
    <pre>export GCLOUD_PROJECT=&lt;YOUR-PROJECT-ID&gt;</pre> 
    <h2>Step 6</h2> 
    <p>Obtain authentication credentials</p> 
    <ul> 
     <li>Go to API Manager -&gt; Credentials</li> 
     <li>Click "New Credentials", and create a service account or <a href="https://console.cloud.google.com/project/_/apiui/credential/serviceaccount" target="_blank">click here</a></li> 
     <li>Download the JSON for this service account, and set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to the file containing the JSON credentials.</li> 
    </ul> 
    <p>Set the GOOGLE_APPLICATION_CREDENTIALS environment variable:</p> 
    <pre>export GOOGLE_APPLICATION_CREDENTIALS= /path/to/service_account_file.json </pre> 
   </google-codelab-step> 
   <google-codelab-step label="Synchronous use case" duration="5"> 
    <h2>Step 1</h2> 
    <pre>cd speech</pre> 
    <h2>Step 2</h2> 
    <p>Install dependencies (listed in package.json)</p> 
    <pre>npm install</pre> 
    <h2>Step 3</h2> 
    <p>List command options and run a simple speech recognition request (synchronous)</p> 
    <pre>node recognize --help</pre> 
    <pre>node recognize sync ./resources/audio.raw</pre> 
    <p>Examine the code for <code>recognize.js</code> in a text editor such as nano:</p> 
    <pre>nano recognize.js</pre> 
    <p>Look for the function <em><code>syncRecognize</code></em> that performs the above. Get familiar with the options used, like encoding and sampleRate.</p> 
   </google-codelab-step> 
   <google-codelab-step label="Try custom hints" duration="3"> 
    <p>In this section, you will modify the code to use custom hints.</p> 
    <p><strong>Step 1</strong></p> 
    <p>Download a new audio file that has 44100 sample rate, and uses a relatively new word, <strong><em>typogram</em></strong>, that's not in the speech database yet.</p> 
    <pre>gsutil cp gs://speechapi-demo/typograms_entities.wav ./resources</pre> 
    <p><strong>Step 2</strong></p> 
    <p>Modify the sample rate to 44100 in <em><code>syncRecognize</code></em> function in <code>recognize.js</code>:</p> 
    <pre>nano recognize.js</pre> 
    <p><strong>Step 3</strong></p> 
    <p>Repeat the command with the new audio file:</p> 
    <pre>node recognize sync ./resources/typograms_entities.wav</pre> 
    <p>Note in the result that instead of spelling out <em>typograms</em>, the result recognizes it as <em>telegrams</em>.</p> 
    <p><strong>Step 4</strong></p> 
    <p>Add a custom hint to your syncRecognize function so it does a better job with the audio file by editing the function as follows:</p> 
    <pre>function syncRecognize (filename, callback) {
  // Detect speech in the audio file, e.g. "./resources/audio.raw"
  speech.recognize(filename, {
    encoding: 'LINEAR16',
    speechContext: {
      "phrases":["typograms"]
     },
    sampleRate: 44100
  }, (err, results) =&gt; {
    if (err) {
      callback(err);
      return;
    }

    console.log('Results:', results);
    callback();
  });
}</pre> 
    <p><strong>Step 5</strong></p> 
    <p>Run the sync command again and now take a look at how the result has changed.</p> 
    <pre>node recognize sync ./resources/typograms_entities.wav</pre> 
   </google-codelab-step> 
   <google-codelab-step label="Asynchronous calls" duration="3"> 
    <p><strong>Step 1</strong></p> 
    <p>Try out the asynchronous version of the recognize function</p> 
    <pre>node recognize async ./resources/audio.raw</pre> 
    <p><strong>Step 2</strong></p> 
    <p>When making an asynchronous (non-blocking) call, other operations that you may add after the function's callback might run first. </p> 
    <p>Lets test this in our code. Simply add a print statement like </p> 
    <pre>console.log(‘your speech is being processed.....‘) </pre> 
    <p>after the callback in both <em>syncRecognize</em> and <em>asyncRecognize</em> functions and run them again. Notice how the statement appears after the results, in case of sync, and before results in case of async.</p> 
   </google-codelab-step> 
   <google-codelab-step label="Streaming calls" duration="3"> 
    <p>Cloud Speech API also allows you to you to stream audio via rpc to do real-time speech to text, for example live news feed, or a speech enabled dictation system.</p> 
    <p>Run the stream and listen version of the command to invoke a real-time streaming request to take input from your microphone, send it to Cloud Speech API and transcribe it:</p> 
    <pre>node recognize stream ./resources/audio.raw

node recognize listen</pre> 
    <p>Look at the <em>streamingRecognize</em> and <em>streamingMicRecognize</em> functions to understand how the above commands work.</p> 
   </google-codelab-step> 
   <google-codelab-step label="Summary" duration="1"> 
    <p>In this lab you ran a Node.js instance to send streaming and non streaming requests to Speech API to transcribe recorded audio</p> 
   </google-codelab-step> 
  </google-codelab> 
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-49880327-14', 'auto');

    (function() {
      var gaCodelab = 'UA-66226300-1';
      if (gaCodelab) {
        ga('create', gaCodelab, 'auto', {name: 'codelab'});
      }

      var gaView;
      var parts = location.search.substring(1).split('&');
      for (var i = 0; i < parts.length; i++) {
        var param = parts[i].split('=');
        if (param[0] === 'viewga') {
          gaView = param[1];
          break;
        }
      }
      if (gaView && gaView !== gaCodelab) {
        ga('create', gaView, 'auto', {name: 'view'});
      }
    })();
  </script>   
 </body>
</html>